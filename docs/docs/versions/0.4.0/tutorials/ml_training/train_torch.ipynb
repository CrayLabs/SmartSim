{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Neural Network loading data from and to SmartSim\n",
    "\n",
    "In this tutorial, we will see how one can set up a workflow in which one or multiple processes produce data (e.g. in a simulation) and one or multiple processes consume the data to train a neural network. The key to achieve this behavior is to use SmartSim to load data from and to the database.\n",
    "\n",
    "This tutorial works on Slurm, but can easily be adapted to any supported Workload Manager (PBS, Cobalt, or LSF), with an only difference: in Slurm, we are allocating the resources *from* this Notebook, whereas with other WLMs, this Notebook has to be started *within* an existing allocation.\n",
    "\n",
    "Note: this tutorial requires the python packages `torch`, `mpi4py` and `horovod`.\n",
    "\n",
    "## 1. First scenario: an ensemble of parallel producers and a single trainer\n",
    "\n",
    "The first use case is similar to a common workflow:\n",
    "- several copies of a simulation (possibly with different initializations) are running, each one consisting of multiple MPI ranks. Each rank produces samples (e.g. data points computed by the simulation) at regular intervals (e.g. each time iteration)\n",
    "- a neural network has to be trained on the data produced by the simulation, and as new data is produced, the neural network needs to add it to its training data set.\n",
    "\n",
    "\n",
    "### 1.1 Workflow components\n",
    "We will use SmartSim to allow the exchange of data between the data-producing processes and the training service. Thus, the first component which we will need to launch is the `Orchestrator`. This is a fairly small example, thus we will run a single-node DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from smartsim import Experiment\n",
    "from smartsim.database import SlurmOrchestrator\n",
    "from smartsim.settings import SrunSettings\n",
    "from smartsim import slurm\n",
    "\n",
    "def launch_cluster_orc(experiment, port, alloc):\n",
    "    \"\"\"Just spin up a database cluster\"\"\"\n",
    "\n",
    "    db = SlurmOrchestrator(port=port,\n",
    "                            db_nodes=1,\n",
    "                            batch=False,\n",
    "                            alloc=alloc,\n",
    "                            interface=\"ib0\")\n",
    "    \n",
    "\n",
    "    # generate directories for output files\n",
    "    # pass in objects to make dirs for\n",
    "    experiment.generate(db, overwrite=True)\n",
    "\n",
    "    # start the database on interactive allocation\n",
    "    experiment.start(db, block=True)\n",
    "\n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data will be very simple: each rank will produce random samples, drawing values from a Gaussian distribution centered at the rank id, and each sample will be labeled with the rank id. The neural network will be trained to infer which rank produced a data sample. Thus, our data set will have a total number of labels equal to the number of ranks in each replicas.\n",
    "\n",
    "The data-producing processes will be started as part of an ensemble of two replicas (this mimics the execution of two copies of the simulation). The actual script producing the data is called `data_uploader.py` and is contained in the `torch` directory. Its content is\n",
    "\n",
    "```python\n",
    "\n",
    "from smartsim.ml import TrainingDataUploader\n",
    "from os import environ\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "mpi_rank = comm.Get_rank()\n",
    "mpi_size = comm.Get_size()\n",
    "\n",
    "batches_per_loop = 10\n",
    "\n",
    "data_uploader = TrainingDataUploader(num_classes=mpi_size,\n",
    "                                     cluster=False, \n",
    "                                     producer_prefixes=\"uploader\",\n",
    "                                     num_ranks=mpi_size)\n",
    "                                     \n",
    "if environ[\"SSKEYOUT\"] == \"uploader_0\":\n",
    "    data_uploader.publish_info()\n",
    "\n",
    "\n",
    "# Start \"simulation\", produce data every two minutes, for thirty minutes\n",
    "for _ in range(15):\n",
    "    new_batch = np.random.normal(loc=float(mpi_rank), scale=5.0, size=(32*batches_per_loop, 224, 224, 3)).astype(float)\n",
    "    new_labels = np.ones(shape=(32*batches_per_loop,)).astype(int) * mpi_rank\n",
    "\n",
    "    data_uploader.put_batch(new_batch, new_labels)\n",
    "    print(f\"{mpi_rank}: New data pushed to DB\")\n",
    "    sleep(120)\n",
    "\n",
    "```\n",
    "\n",
    "as the script is running in python, we can use the `TrainingDataUploader` class, which streamlines uploading of the data samples from the simulation. Here is an explanation of the arguments:\n",
    "- `num_classes` is the number of classes of our training data set\n",
    "- `smartredis_cluster=False` specifies that the DB is not a cluster (it is a single shard)\n",
    "- `producer_prefixes` is used to identify the SmartSim entity names of the processes producing data: this is useful in the case the training service has several incoming entities (as defined in SmartSim, through the environment variable `SSKEYIN`), but it should expect data only from a subset of them. \n",
    "- `num_ranks` is the number of concurrent data loaders withing this application, in this case it is simply the number of MPI ranks, as each process will upload its own data.\n",
    "\n",
    "\n",
    "At each iteration, each rank of \"simulation\" calls `put_batch` to upload a batch of samples and the corresponding labels. The batch will be uploaded on the DB and stored under a key composed by a prefix (defaulting to `samples`), the sub-index, and the iteration number, which increases monotonically every time a batch is uploaded.\n",
    "\n",
    "Now that we know the content of `data_uploader.py`, we can create an entity representing the uploader ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_uploader(experiment, alloc, nodes=1, tasks_per_node=1):\n",
    "   \"\"\"Start an ensemble of two processes producing sample batches at\n",
    "      regular intervals.\n",
    "   \"\"\"\n",
    "   srun = SrunSettings(exe=\"python\",\n",
    "                     exe_args=\"data_uploader.py\",\n",
    "                     env_vars={\"PYTHONUNBUFFERED\": \"1\"},\n",
    "                     alloc=alloc)\n",
    "   srun.set_nodes(nodes)\n",
    "   srun.set_tasks_per_node(tasks_per_node)\n",
    "\n",
    "   uploader = experiment.create_ensemble(\"uploader\", replicas=2, run_settings=srun)\n",
    "\n",
    "   # create directories for the output files and copy\n",
    "   # scripts to execution location inside newly created dir\n",
    "   # only necessary if its not an executable (python is executable here)\n",
    "   uploader.attach_generator_files(to_copy=[\"./torch/data_uploader.py\"])\n",
    "   experiment.generate(uploader, overwrite=True)\n",
    "   return uploader\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last component of our workflow is the trainer. This process will keep downloading samples as they are produced, and use them to train a neural network.\n",
    "\n",
    "Here is the content of the `training_service.py` script, stored in the `torch` directory\n",
    "\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from smartsim.ml.torch import DynamicDataGenerator, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    torch.multiprocessing.set_start_method('spawn')\n",
    "    training_set = DynamicDataGenerator(cluster=False,\n",
    "                                 init_samples=False,\n",
    "                                 verbose=True)\n",
    "    trainloader = DataLoader(training_set, batch_size=None,\n",
    "                             num_workers=2)\n",
    "    model = models.mobilenet_v2().double().to('cuda')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    print(\"Started training\")\n",
    "\n",
    "    for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        epoch_running_loss = 0.0\n",
    "        output_period = 100\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].double().to('cuda'), data[1].to('cuda')\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_running_loss += loss.item()\n",
    "\n",
    "            if i % output_period == (output_period-1):    # print every \"output_period\" mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / output_period))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        \n",
    "        print('[%d, %5d] loss: %.3f' %\n",
    "            (epoch + 1, i + 1, epoch_running_loss / (i+1)))\n",
    "        epoch_running_loss = 0.0\n",
    "                \n",
    "    print('Finished Training')\n",
    "    \n",
    "```\n",
    "\n",
    "The key component is the `DataGenerator` object. Since the DB contains the metadata of the `TrainingDataUploader`, and these are stored under a default key, the `DataGenerator` will retrieve them and use them to know the key of the uploaded batches of samples and labels. We need to specify `init_samples=False`, because initialization of the data set will be performed by the `DataLoader` workers: notice that the `DataLoader` is imported from `smartsim.ml.torch`: it is our custom implementation of a data loader, which will also triger a data update at the beginning of each epoch, to check if there are new samples available.\n",
    "\n",
    "We create a SmartSim entity representing the trainer and we are ready to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_trainer(experiment, alloc):\n",
    "    \"\"\"Start a process running a training service which will\n",
    "       download batches from the DB.\n",
    "    \"\"\"\n",
    "    srun = SrunSettings(exe=\"python\",\n",
    "                        exe_args=\"training_service.py\",\n",
    "                        env_vars={\"PYTHONUNBUFFERED\": \"1\"},\n",
    "                        alloc=alloc)\n",
    "    srun.set_tasks(1)\n",
    "\n",
    "    trainer = experiment.create_model(\"trainer\", srun)\n",
    "\n",
    "    # create directories for the output files and copy\n",
    "    # scripts to execution location inside newly created dir\n",
    "    # only necessary if its not an executable (python is executable here)\n",
    "    trainer.attach_generator_files(to_copy=\"./torch/training_service.py\")\n",
    "    experiment.generate(trainer, overwrite=True)\n",
    "    return trainer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Request an allocation\n",
    "\n",
    "We need one node for the DB, two for the producer ensemble, and one for the trainer, thus we request 4 nodes to be allocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alloc = slurm.get_allocation(nodes=4, time=\"03:00:00\", options={\"constraint\": \"V100\", \"partition\": \"spider\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Run the workflow\n",
    "\n",
    "Now that all components are available, we create the SmartSim experiment representing our workflow. \n",
    "Notice that the line\n",
    "```python\n",
    "uploader_model.enable_key_prefixing()\n",
    "```\n",
    "sets the ``uploader`` process so that tensor keys produced within it will be prefixed with its\n",
    "SmartSim entity name, and the lines\n",
    "```python\n",
    "for uploader in uploader_model.entities:\n",
    "    trainer_model.register_incoming_entity(uploader)\n",
    "```\n",
    "make sure that each uploader replica (within the ensemble) is set as an incoming entity of the `trainer_model`. This will allow the `trainer_model` to know which processes are producing batches it will need to download.\n",
    "\n",
    "We call `exp.start` and the workflow is launched! As the trainer was started with `verbose=True`, we can look at its output in `launch_streaming`: we will see that it keeps downloading batches at the end of each epoch, as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp = Experiment(\"launch_streaming\", launcher=\"slurm\")\n",
    "\n",
    "db_port = 6780\n",
    "\n",
    "# start the database\n",
    "db = launch_cluster_orc(exp, db_port, alloc)\n",
    "uploader_model = create_uploader(exp, alloc, 1, 2)\n",
    "uploader_model.enable_key_prefixing()\n",
    "exp.start(uploader_model, block=False, summary=False)\n",
    "trainer_model = create_trainer(exp, alloc)\n",
    "for uploader in uploader_model.entities:\n",
    "    trainer_model.register_incoming_entity(uploader)\n",
    "\n",
    "exp.start(trainer_model, block=True, summary=False)\n",
    "\n",
    "# shutdown the database because we don't need it anymore\n",
    "exp.stop(db)\n",
    "\n",
    "print(exp.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Stop the workflow (optional) and release the allocation\n",
    "If we did not wait until completion of the previous cell, but we stopped it, we need to stop the SmartSim entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp.stop(db, uploader_model, trainer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, we release the allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.release_allocation(alloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Second scenario: an ensemble of parallel producers and a distributed trainer\n",
    "\n",
    "In the second scenario, we use Horovod to distribute the training and use multiple ranks. Each rank will download only a portion of the dataset, thus speeding up the download and training process.\n",
    "\n",
    "\n",
    "### 2.1 Workflow components\n",
    "The only component we change with respect to the previous example, is the training service, which is now defined in `training_service_hvd.py`:\n",
    "\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "\n",
    "from smartsim.ml.torch import DynamicDataGenerator, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import horovod.torch as hvd\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Initialize Horovod\n",
    "    hvd.init()\n",
    "\n",
    "    hvd_rank = hvd.rank()\n",
    "    hvd_size = hvd.size()\n",
    "\n",
    "    # Pin GPU to be used to process local rank (one GPU per process)\n",
    "    torch.cuda.set_device(hvd.local_rank())\n",
    "\n",
    "    torch.multiprocessing.set_start_method('spawn')\n",
    "    training_set = DynamicDataGenerator(cluster=False,\n",
    "                                 verbose=True,\n",
    "                                 init_samples=False,\n",
    "                                 num_replicas=hvd_size,\n",
    "                                 replica_rank=hvd_rank)\n",
    "\n",
    "    trainloader = DataLoader(training_set, batch_size=None,\n",
    "                             num_workers=2)\n",
    "\n",
    "    model = models.mobilenet_v2().double().to('cuda')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001*hvd_size)\n",
    "    optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n",
    "    hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "    print(f\"Rank {hvd_rank}: Started training\")\n",
    "\n",
    "    for epoch in range(100):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        epoch_running_loss = 0.0\n",
    "        if hvd_rank == 0:\n",
    "            print(f\"Epoch {epoch}\")\n",
    "        output_period = 100\n",
    "\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].double().to('cuda'), data[1].to('cuda')\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_running_loss += loss.item()\n",
    "\n",
    "            if hvd_rank == 0:\n",
    "                if i % output_period == (output_period-1):    # print every \"output_period\" mini-batches\n",
    "                    print('[%d, %5d] loss: %.3f' %\n",
    "                        (epoch + 1, i + 1, running_loss / output_period))\n",
    "                    running_loss = 0.0\n",
    "\n",
    "        if hvd_rank == 0:    \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                (epoch + 1, i + 1, epoch_running_loss / (i+1)))\n",
    "            epoch_running_loss = 0.0\n",
    "                \n",
    "    print('Finished Training')\n",
    "    \n",
    "```\n",
    "\n",
    "Compared to the previous training service, we notice that the only thing changing for the `DataGenerator` is that we need to specify the total number of replicas, which is equal to the total number of Horovod ranks, and the rank of each replica, which corresponds to the Horovod rank. The rest of the training service script is modified according to the standard Horovod distributed training rules.\n",
    "\n",
    "Let's define the SmartSim entity representing the training service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer_hvd(experiment, alloc, nodes=1, tasks_per_node=1):\n",
    "   \"\"\"Start a process running a distributed training service which will\n",
    "      download batches from the DB and use Horovod to distribute\n",
    "      data and compute global weight updates.\n",
    "   \"\"\"\n",
    "   srun = SrunSettings(exe=\"python\",\n",
    "                     exe_args=\"training_service_hvd.py\",\n",
    "                     env_vars={\"PYTHONUNBUFFERED\": \"1\"},\n",
    "                     alloc=alloc)\n",
    "   srun.set_nodes(nodes)\n",
    "   srun.set_tasks_per_node(tasks_per_node)\n",
    "\n",
    "   trainer = experiment.create_model(\"trainer\", srun)\n",
    "\n",
    "   # create directories for the output files and copy\n",
    "   # scripts to execution location inside newly created dir\n",
    "   # only necessary if its not an executable (python is executable here)\n",
    "   trainer.attach_generator_files(to_copy=\"./torch/training_service_hvd.py\")\n",
    "   experiment.generate(trainer, overwrite=True)\n",
    "   return trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Request an allocation\n",
    "\n",
    "As before, we need one node for the DB, two for the producer ensemble, and one for the trainer, thus we request 4 nodes to be allocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alloc = slurm.get_allocation(nodes=4, time=\"03:00:00\", options={\"constraint\": \"V100\", \"partition\": \"spider\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Run the workflow\n",
    "\n",
    "Now that all components are available, we create the SmartSim experiment representing our workflow. The setup is completely identical to the previous example, thus we can start the experiment and look at the output files to see that now each replica has a smaller portion of the dataset, and the training proceeds much faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp = Experiment(\"launch_streaming_hvd\", launcher=\"slurm\")\n",
    "\n",
    "db_port = 6780\n",
    "\n",
    "# start the database\n",
    "db = launch_cluster_orc(exp, db_port, alloc)\n",
    "uploader_model = create_uploader(exp, alloc, 1, 16)\n",
    "uploader_model.enable_key_prefixing()\n",
    "exp.start(uploader_model, block=False, summary=False)\n",
    "trainer_model = create_trainer_hvd(exp, alloc, 1, 8)\n",
    "for uploader in uploader_model.entities:\n",
    "    trainer_model.register_incoming_entity(uploader)\n",
    "\n",
    "exp.start(trainer_model, block=True, summary=False)\n",
    "\n",
    "# shutdown the database because we don't need it anymore\n",
    "exp.stop(db)\n",
    "\n",
    "print(exp.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Stop the workflow (optional) and release the allocation\n",
    "If we did not wait until completion of the previous cell, but we stopped it, we need to stop the SmartSim entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp.stop(db, uploader_model, trainer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, we release the allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.release_allocation(alloc)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
