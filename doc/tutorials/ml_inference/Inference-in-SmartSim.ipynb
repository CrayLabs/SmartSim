{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "487eb264-3c79-434f-842f-a11a8601ae7b",
   "metadata": {},
   "source": [
    "# Online Inference\n",
    "\n",
    "This tutorial shows how to use trained PyTorch, TensorFlow, and ONNX (format) models, written in Python, directly in HPC workloads written in Fortran, C, C++ and Python.\n",
    "\n",
    "The example simulation here is written in Python for brevity, however, the inference API in SmartRedis is the same (besides extra parameters for compiled langauges) across all clients. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3604189-d438-4702-9aba-89161ebc4554",
   "metadata": {},
   "source": [
    "## Installing the ML backends\n",
    "\n",
    "In order to use the `Orchestrator` database as an inference engine, the Machine Learning (ML) backends need to be built and supplied to the database at runtime. \n",
    "\n",
    "To check which backends are built, a simple helper function is available in SmartSim as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd289351-b2a7-45ae-a774-e54a94a80c65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tensorflow', 'torch'}\n"
     ]
    }
   ],
   "source": [
    "## Installing the ML backends\n",
    "from smartsim._core.utils.helpers import installed_redisai_backends\n",
    "print(installed_redisai_backends())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609884f5-37a0-4559-ad0b-cf3ae65463cb",
   "metadata": {},
   "source": [
    "As you can see, only the Torch backend is built. In order to use the TensorFlow and ONNX backends as well, they need to be built.\n",
    "\n",
    "The `smart` command line interface can be used to build the backends using the `smart build` command. The output of `smart build --help` is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8efd39b-7517-4220-9419-4fb82918e182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: smart build [-h] [-v] [--device {cpu,gpu}] [--dragon]\n",
      "                   [--only_python_packages] [--no_pt] [--no_tf] [--onnx]\n",
      "                   [--torch_dir TORCH_DIR]\n",
      "                   [--libtensorflow_dir LIBTENSORFLOW_DIR] [--keydb]\n",
      "                   [--no_torch_with_mkl]\n",
      "\n",
      "Build SmartSim dependencies (Redis, RedisAI, Dragon, ML runtimes)\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -v                    Enable verbose build process\n",
      "  --device {cpu,gpu}    Device to build ML runtimes for\n",
      "  --dragon              Install the dragon runtime\n",
      "  --only_python_packages\n",
      "                        Only evaluate the python packages (i.e. skip building\n",
      "                        backends)\n",
      "  --no_pt               Do not build PyTorch backend\n",
      "  --no_tf               Do not build TensorFlow backend\n",
      "  --onnx                Build ONNX backend (off by default)\n",
      "  --torch_dir TORCH_DIR\n",
      "                        Path to custom <path>/torch/share/cmake/Torch/\n",
      "                        directory (ONLY USE IF NEEDED)\n",
      "  --libtensorflow_dir LIBTENSORFLOW_DIR\n",
      "                        Path to custom libtensorflow directory (ONLY USE IF\n",
      "                        NEEDED)\n",
      "  --keydb               Build KeyDB instead of Redis\n",
      "  --no_torch_with_mkl   Do not build Torch with Intel MKL\n"
     ]
    }
   ],
   "source": [
    "!smart build --help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6c6f7-b4f5-4e8a-82a7-c3d8376e3374",
   "metadata": {},
   "source": [
    "We use `smart clean` first to remove the previous build, and then call `smart build` to build the new backend set. For larger teams, CrayLabs will help setup your system so that the backends do not have to be built by each user.\n",
    "\n",
    "By default, the PyTorch and TensorFlow backends are built. To build all three backends for use on CPU, we issue the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6d157cf-1d2c-49d0-a588-ec99506661fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[SmartSim]\u001b[0m \u001b[1;30mINFO\u001b[0m Successfully removed existing RedisAI installation\n",
      "\u001b[34m[SmartSim]\u001b[0m \u001b[1;30mINFO\u001b[0m Successfully removed ML runtimes\n",
      "\u001b[34m[SmartSim]\u001b[0m \u001b[1;30mINFO\u001b[0m Running SmartSim build process...\n",
      "\u001b[34m[SmartSim]\u001b[0m \u001b[1;30mINFO\u001b[0m Checking requested versions...\n",
      "\u001b[34m[SmartSim]\u001b[0m \u001b[1;30mINFO\u001b[0m Redis build complete!\n",
      "\n",
      "ML Backends Requested\n",
      "╒════════════╤════════╤══════╕\n",
      "│ PyTorch    │ 2.1.0  │ \u001b[32mTrue\u001b[0m │\n",
      "│ TensorFlow │ 2.13.1 │ \u001b[32mTrue\u001b[0m │\n",
      "│ ONNX       │ 1.14.1 │ \u001b[32mTrue\u001b[0m │\n",
      "╘════════════╧════════╧══════╛\n",
      "\n",
      "Building for GPU support: \u001b[31mFalse\u001b[0m\n",
      "\n",
      "\u001b[34m[SmartSim]\u001b[0m \u001b[1;30mINFO\u001b[0m Building RedisAI version 1.2.7 from https://github.com/RedisAI/RedisAI.git/\n",
      "\u001b[34m[SmartSim]\u001b[0m \u001b[1;30mINFO\u001b[0m ML Backends and RedisAI build complete!\n",
      "\u001b[34m[SmartSim]\u001b[0m \u001b[1;30mINFO\u001b[0m Tensorflow, Onnxruntime, Torch backend(s) built\n",
      "\u001b[34m[SmartSim]\u001b[0m \u001b[1;30mINFO\u001b[0m SmartSim build complete!\n"
     ]
    }
   ],
   "source": [
    "!smart clean && smart build --device cpu --onnx\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7df6c3dd-6cc5-46e6-9c58-6ba2333d7045",
   "metadata": {},
   "source": [
    "## Starting the Database for Inference\n",
    "\n",
    "SmartSim performs online inference by using the SmartRedis clients to call into the\n",
    "Machine Learning (ML) runtimes linked into the Orchestrator database. The Orchestrator\n",
    "is the name in SmartSim for a Redis or KeyDB database with a RedisAI module built\n",
    "into it with the ML runtimes.\n",
    "\n",
    "Therefore, to perform inference, you must first create an Orchestrator database and\n",
    "launch it. There are two methods to couple the database to your application in\n",
    "order to add inference capability to your application.\n",
    " - standard (not colocated)\n",
    " - colocated\n",
    " \n",
    "`standard` mode launches an optionally clustered (across many compute hosts) database instance\n",
    "that can be treated as a single storage device for many clients (possibly the many ranks\n",
    "of an MPI program) where there is a single address space for keys across all hosts.\n",
    "\n",
    "`colocated` mode launches a orchestrator instance on each compute host used by a,\n",
    "possibly distributed, application. each instance contains their own address space\n",
    "for keys. In SmartSim, `Model` instances can be launched with a colocated orchetrator\n",
    "through `Model.colocate_db_tcp` or `Model.colocate_db_udp`. Colocated `Model`s are used for\n",
    "highly scalable inference where global aggregations aren't necessary for inference.\n",
    "\n",
    "The code below launches the `Orchestrator` database using the `standard` deployment\n",
    "method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "201b9c43-f3e9-476c-ac21-e45f2c621b00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some helper libraries for the tutorial\n",
    "import io\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# import smartsim and smartredis\n",
    "from smartredis import Client\n",
    "from smartsim import Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1df7ff13-e292-4c68-a99c-d0b5491be079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp = Experiment(\"Inference-Tutorial\", launcher=\"local\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7364dbdf-52bf-4107-be3a-78fb541449f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = exp.create_database(port=6780, interface=\"lo\")\n",
    "exp.start(db)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58615a9e-bb53-4025-90de-c1eee4e315eb",
   "metadata": {},
   "source": [
    "## Using PyTorch\n",
    "\n",
    "The Orchestrator supports both [PyTorch](https://pytorch.org/)\n",
    "models and [TorchScript](https://pytorch.org/docs/stable/jit.html) functions and scripts\n",
    "in PyTorch.\n",
    "\n",
    "Below, the code is shown to create, jit-trace (prepare for inference), set,\n",
    "and call a PyTorch Convolutional Neural Network (CNN) with SmartSim and SmartRedis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dde172d9-4f18-4adc-8e78-fc6f71fb405c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ab54aba-f6d7-4ecb-907e-1efdba9657a9",
   "metadata": {},
   "source": [
    "To set a PyTorch model, we create a function to \"jit-trace\" the model\n",
    "and save it to a buffer in memory.\n",
    "\n",
    "If you aren't familiar with the concept of tracing, take a look at the\n",
    "Torch documentation for [trace](https://pytorch.org/docs/stable/generated/torch.jit.trace.html#torch.jit.trace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5aa5995-d250-46c4-87f0-0115112560ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an instance of our CNN model\n",
    "n = Net()\n",
    "n.eval()\n",
    "\n",
    "# prepare a sample input to trace on (random noise is fine)\n",
    "example_forward_input = torch.rand(1, 1, 28, 28)\n",
    "\n",
    "def create_torch_model(torch_module, example_forward_input):\n",
    "\n",
    "    # perform the trace of the nn.Module.forward() method\n",
    "    module = torch.jit.trace(torch_module, example_forward_input)\n",
    "\n",
    "    # save the traced module to a buffer\n",
    "    model_buffer = io.BytesIO()\n",
    "    torch.jit.save(module, model_buffer)\n",
    "    return model_buffer.getvalue()\n",
    "\n",
    "traced_cnn = create_torch_model(n, example_forward_input)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "209a8db0-249a-4345-b3de-9c309ae5ebe6",
   "metadata": {},
   "source": [
    "Lastly, we use the SmartRedis Python client to\n",
    "\n",
    "1. Connect to the database\n",
    "2. Put a batch of 20 tensors into the database  (``put_tensor``)\n",
    "3. Set the Torch model in the database (``set_model``)\n",
    "4. Run the model on the batch of tensors (``run_model``)\n",
    "5. Retrieve the result (``get_tensor``)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bdabc3d-31cc-47fc-853b-94fffaa2bf10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [[-2.2239347 -2.256488  -2.3910825 -2.2572591 -2.2663934 -2.3775585\n",
      "  -2.257742  -2.3160243 -2.391289  -2.3055189]\n",
      " [-2.2149696 -2.2576108 -2.3899908 -2.2715292 -2.2628417 -2.3693023\n",
      "  -2.260772  -2.3166935 -2.3967428 -2.3028378]\n",
      " [-2.2214003 -2.2581112 -2.3854284 -2.2616909 -2.2745335 -2.3779867\n",
      "  -2.2570336 -2.3125517 -2.391247  -2.302534 ]\n",
      " [-2.214657  -2.2598932 -2.3800194 -2.2612374 -2.2718334 -2.3784144\n",
      "  -2.2596886 -2.318937  -2.3904119 -2.3075597]\n",
      " [-2.2034936 -2.2570574 -2.4026587 -2.2698882 -2.2597382 -2.3796346\n",
      "  -2.2662714 -2.3141642 -2.3986044 -2.2949069]\n",
      " [-2.2162325 -2.2635622 -2.3800213 -2.2569213 -2.264393  -2.3763664\n",
      "  -2.2658355 -2.3211577 -2.3904028 -2.307555 ]\n",
      " [-2.2084794 -2.258525  -2.393487  -2.26341   -2.2674217 -2.3792422\n",
      "  -2.264515  -2.3262923 -2.3823283 -2.300095 ]\n",
      " [-2.2175536 -2.2577217 -2.3975415 -2.2582505 -2.269493  -2.365971\n",
      "  -2.2619228 -2.3258338 -2.3984828 -2.291332 ]\n",
      " [-2.2151139 -2.2522063 -2.3931108 -2.2577128 -2.270789  -2.371976\n",
      "  -2.2567465 -2.32229   -2.395818  -2.308673 ]\n",
      " [-2.2141316 -2.2494154 -2.3948152 -2.2606037 -2.2732735 -2.3758345\n",
      "  -2.2620056 -2.3184063 -2.385798  -2.3094575]\n",
      " [-2.221041  -2.2519057 -2.398841  -2.259931  -2.2686832 -2.3660865\n",
      "  -2.2632158 -2.322879  -2.3970191 -2.2942836]\n",
      " [-2.2142313 -2.2578502 -2.393603  -2.2673647 -2.2553272 -2.37376\n",
      "  -2.2617526 -2.3199627 -2.399065  -2.301728 ]\n",
      " [-2.2082942 -2.2571995 -2.3889875 -2.266007  -2.257706  -2.37675\n",
      "  -2.266374  -2.3223817 -2.3961644 -2.304737 ]\n",
      " [-2.2229445 -2.2658186 -2.399095  -2.2566628 -2.266294  -2.3742397\n",
      "  -2.2578638 -2.3047974 -2.3973055 -2.2988966]\n",
      " [-2.215887  -2.2676513 -2.3889093 -2.246127  -2.266115  -2.3842902\n",
      "  -2.2586591 -2.3106883 -2.396018  -2.3104343]\n",
      " [-2.2099977 -2.2719226 -2.391469  -2.255561  -2.266949  -2.371345\n",
      "  -2.2596216 -2.324484  -2.3890057 -2.3031068]\n",
      " [-2.214121  -2.2561312 -2.391877  -2.261881  -2.2639613 -2.3679278\n",
      "  -2.269122  -2.3139405 -2.4036062 -2.3015296]\n",
      " [-2.22871   -2.256755  -2.3881361 -2.2651346 -2.2651856 -2.3733103\n",
      "  -2.2641761 -2.3182902 -2.3855858 -2.2960906]\n",
      " [-2.2103846 -2.2450664 -2.3848588 -2.2795632 -2.2658024 -2.3679922\n",
      "  -2.2666745 -2.3190453 -2.3987417 -2.3054008]\n",
      " [-2.2175698 -2.2573788 -2.391653  -2.2519581 -2.2637622 -2.3839104\n",
      "  -2.265371  -2.3158426 -2.3929882 -2.3040662]]\n"
     ]
    }
   ],
   "source": [
    "client = Client(address=db.get_address()[0], cluster=False)\n",
    "\n",
    "client.put_tensor(\"input\", torch.rand(20, 1, 28, 28).numpy())\n",
    "\n",
    "# put the PyTorch CNN in the database in GPU memory\n",
    "client.set_model(\"cnn\", traced_cnn, \"TORCH\", device=\"CPU\")\n",
    "\n",
    "# execute the model, supports a variable number of inputs and outputs\n",
    "client.run_model(\"cnn\", inputs=[\"input\"], outputs=[\"output\"])\n",
    "\n",
    "# get the output\n",
    "output = client.get_tensor(\"output\")\n",
    "print(f\"Prediction: {output}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37c6f801-deb5-463c-bfc7-565a79b8bcfb",
   "metadata": {},
   "source": [
    "As we gave the CNN random noise, the predictions reflect that.\n",
    "\n",
    "If running on GPU, be sure to change the argument in the ``set_model`` call\n",
    "above to ``device=\"GPU\"``."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "638cdd2a-c0ad-4a54-a7ce-9eef0096da30",
   "metadata": {},
   "source": [
    "## Using TorchScript\n",
    "\n",
    "In addition to PyTorch models, TorchScript scripts and functions can be set in the\n",
    "Orchestrator database and called from any of the SmartRedis languages. Functions\n",
    "can be set in the database in Python prior to application launch and then used\n",
    "directly in Fortran, C, and C++ simulations.\n",
    "\n",
    "The example below uses the TorchScript Singular Value Decomposition (SVD) function.\n",
    "The function set in side the database and then called with a random input\n",
    "tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "267405bf-3144-4219-a82b-15be10cf5125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_svd(input_tensor):\n",
    "    # svd function from TorchScript API\n",
    "    return input_tensor.svd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e85ced9-e39a-4efb-91db-3919aa4e9489",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U: [[[-0.50057614  0.2622205 ]\n",
      "  [-0.47629714 -0.8792326 ]\n",
      "  [-0.7228863   0.39773142]]\n",
      "\n",
      " [[-0.45728168  0.88121146]\n",
      "  [-0.37974676 -0.31532544]\n",
      "  [-0.80416775 -0.35218775]]\n",
      "\n",
      " [[-0.4667158   0.8836199 ]\n",
      "  [-0.47055572 -0.21237665]\n",
      "  [-0.7488349  -0.4172673 ]]\n",
      "\n",
      " [[-0.32159734  0.92966324]\n",
      "  [-0.6941528  -0.10238242]\n",
      "  [-0.64399314 -0.35389856]]\n",
      "\n",
      " [[-0.6984835   0.4685579 ]\n",
      "  [-0.55331963  0.12572214]\n",
      "  [-0.45382637 -0.8744412 ]]]\n",
      "\n",
      ", S: [[164.58028    49.682358 ]\n",
      " [120.11677    66.62553  ]\n",
      " [130.01929    17.520935 ]\n",
      " [198.615      22.047113 ]\n",
      " [154.67653     2.6773496]]\n",
      "\n",
      ", V: [[[-0.7275351  -0.68607044]\n",
      "  [-0.68607044  0.7275351 ]]\n",
      "\n",
      " [[-0.6071297   0.79460275]\n",
      "  [-0.79460275 -0.6071297 ]]\n",
      "\n",
      " [[-0.604189    0.7968411 ]\n",
      "  [-0.7968411  -0.604189  ]]\n",
      "\n",
      " [[-0.69911253 -0.7150117 ]\n",
      "  [-0.7150117   0.69911253]]\n",
      "\n",
      " [[-0.8665945  -0.499013  ]\n",
      "  [-0.499013    0.8665945 ]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# connect a client to the database\n",
    "client = Client(address=db.get_address()[0], cluster=False)\n",
    "\n",
    "# test the SVD function\n",
    "tensor = np.random.randint(0, 100, size=(5, 3, 2)).astype(np.float32)\n",
    "client.put_tensor(\"input\", tensor)\n",
    "client.set_function(\"svd\", calc_svd)\n",
    "client.run_script(\"svd\", \"calc_svd\", [\"input\"], [\"U\", \"S\", \"V\"])\n",
    "U = client.get_tensor(\"U\")\n",
    "S = client.get_tensor(\"S\")\n",
    "V = client.get_tensor(\"V\")\n",
    "print(f\"U: {U}\\n\\n, S: {S}\\n\\n, V: {V}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55ae0408-7ddb-4a75-912f-7312ee43f79b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "# create a simple Fully connected network in Keras\n",
    "model = keras.Sequential(\n",
    "    layers=[\n",
    "        keras.layers.InputLayer(input_shape=(28, 28), name=\"input\"),\n",
    "        keras.layers.Flatten(input_shape=(28, 28), name=\"flatten\"),\n",
    "        keras.layers.Dense(128, activation=\"relu\", name=\"dense\"),\n",
    "        keras.layers.Dense(10, activation=\"softmax\", name=\"output\"),\n",
    "    ],\n",
    "    name=\"FCN\",\n",
    ")\n",
    "\n",
    "# Compile model with optimizer\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48b1efb6-4f39-4ad6-8a27-58648ec66bde",
   "metadata": {},
   "source": [
    "### Setting TensorFlow and Keras Models\n",
    "\n",
    "After a model is created (trained or not), the graph of the model is\n",
    "frozen and saved to file so the client method `client.set_model_from_file`\n",
    "can load it into the database.\n",
    "\n",
    "SmartSim includes a utility to freeze the graph of a TensorFlow or Keras model in\n",
    "`smartsim.ml.tf`. To use TensorFlow or Keras in SmartSim, specify\n",
    "`TF` as the argument for *backend* in the call to `client.set_model` or\n",
    "`client.set_model_from_file`.\n",
    "\n",
    "Note that TensorFlow and Keras, unlike the other ML libraries supported by\n",
    "SmartSim, requires an `input` and `output` argument in the call to\n",
    "`set_model`. These arguments correspond to the layer names of the\n",
    "created model. The `smartsim.ml.tf.freeze_model` utility\n",
    "returns these values for convenience as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "400e2a53-74b2-4bf1-a0e8-32222fd968f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06595241 0.11921222 0.02889561 0.20963618 0.08950416 0.11298887\n",
      "  0.05179482 0.09778847 0.14826407 0.07596324]]\n"
     ]
    }
   ],
   "source": [
    "from smartsim.ml.tf import freeze_model\n",
    "\n",
    "# SmartSim utility for Freezing the model and saving it to a file.\n",
    "model_path, inputs, outputs = freeze_model(model, os.getcwd(), \"fcn.pb\")\n",
    "\n",
    "# use the same client we used for PyTorch to set the TensorFlow model\n",
    "# this time the method for setting a model from a saved file is shown.\n",
    "# TensorFlow backed requires named inputs and outputs on graph\n",
    "# this differs from PyTorch and ONNX.\n",
    "client.set_model_from_file(\n",
    "    \"keras_fcn\", model_path, \"TF\", device=\"CPU\", inputs=inputs, outputs=outputs\n",
    ")\n",
    "\n",
    "# put random random input tensor into the database\n",
    "input_data = np.random.rand(1, 28, 28).astype(np.float32)\n",
    "client.put_tensor(\"input\", input_data)\n",
    "\n",
    "# run the Fully Connected Network model on the tensor we just put\n",
    "# in and store the result of the inference at the \"output\" key\n",
    "client.run_model(\"keras_fcn\", \"input\", \"output\")\n",
    "\n",
    "# get the result of the inference\n",
    "pred = client.get_tensor(\"output\")\n",
    "print(pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a23d248c-6280-44b0-88a7-2babbaed3f3f",
   "metadata": {},
   "source": [
    "## Using ONNX\n",
    "\n",
    "ONNX is a standard format for representing models. A number of different Machine Learning\n",
    "Libraries are supported by ONNX and can be readily used with SmartSim.\n",
    "\n",
    "Some popular ones are:\n",
    "\n",
    "\n",
    "- [Scikit-learn](https://scikit-learn.org)\n",
    "- [XGBoost](https://xgboost.readthedocs.io)\n",
    "- [CatBoost](https://catboost.ai)\n",
    "- [LightGBM](https://lightgbm.readthedocs.io/en/latest/)\n",
    "- [libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)\n",
    "\n",
    "\n",
    "As well as some that are not listed. There are also many tools to help convert\n",
    "models to ONNX.\n",
    "\n",
    "- [onnxmltools](https://github.com/onnx/onnxmltools)\n",
    "- [skl2onnx](https://github.com/onnx/sklearn-onnx/)\n",
    "- [tensorflow-onnx](https://github.com/onnx/tensorflow-onnx/)\n",
    "\n",
    "\n",
    "And PyTorch has its own converter.\n",
    "\n",
    "Below are some examples of a few models in [Scikit-learn](https://scikit-learn.org)\n",
    "that are converted into ONNX format for use with SmartSim. To use ONNX in SmartSim, specify\n",
    "`ONNX` as the argument for *backend* in the call to `client.set_model` or\n",
    "`client.set_model_from_file`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4bf422e5-bece-4402-a2ef-8bca1e009b5c",
   "metadata": {},
   "source": [
    "### Scikit-Learn K-means Cluster\n",
    "\n",
    "\n",
    "K-means clustering is an unsupervised ML algorithm. It is used to categorize data points\n",
    "into functional groups (\"clusters\"). Scikit Learn has a built in implementation of K-means clustering\n",
    "and it is easily converted to ONNX for use with SmartSim through \n",
    "[skl2onnx.to_onnx](http://onnx.ai/sklearn-onnx/auto_examples/plot_convert_syntax.html)\n",
    "\n",
    "Since the KMeans model returns two outputs, we provide the `client.run_model` call\n",
    "with two `output` key names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "889328a7-2326-476f-a686-f34397f4a210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skl2onnx import to_onnx\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "729486bb-ab34-44cb-a36f-7d26cbf6393a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default@[0 0 0 0 0 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = np.arange(20, dtype=np.float32).reshape(10, 2)\n",
    "tr = KMeans(n_clusters=2)\n",
    "tr.fit(X)\n",
    "\n",
    "# save the trained k-means model in memory with skl2onnx\n",
    "kmeans = to_onnx(tr, X, target_opset=11)\n",
    "model = kmeans.SerializeToString()\n",
    "\n",
    "# random input data\n",
    "sample = np.arange(20, dtype=np.float32).reshape(10, 2)\n",
    "\n",
    "# use the same client from TensorFlow and Pytorch examples.\n",
    "client.put_tensor(\"input\", sample)\n",
    "client.set_model(\"kmeans\", model, \"ONNX\", device=\"CPU\")\n",
    "client.run_model(\"kmeans\", inputs=\"input\", outputs=[\"labels\", \"transform\"])\n",
    "\n",
    "print(client.get_tensor(\"labels\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7da6c43d-b1b2-46ae-89f8-cb7050d9592b",
   "metadata": {},
   "source": [
    "### Scikit-Learn Random Forest\n",
    "\n",
    "The Random Forest example uses the Iris dataset from Scikit Learn to train a\n",
    "RandomForestRegressor. As with the other examples, the skl2onnx function\n",
    "`skl2onnx.to_onnx` is used to convert the model to ONNX format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "657b3e7b-067a-4053-92e6-60379e5a6807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d73f0d5b-e6c2-42b1-8e21-3b6d064d20e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.9999987]]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, _ = train_test_split(X, y, random_state=13)\n",
    "clr = RandomForestRegressor(n_jobs=1, n_estimators=100)\n",
    "clr.fit(X_train, y_train)\n",
    "\n",
    "rf_model = to_onnx(clr, X_test.astype(np.float32), target_opset=11)\n",
    "\n",
    "sample = np.array([[6.4, 2.8, 5.6, 2.2]]).astype(np.float32)\n",
    "model = rf_model.SerializeToString()\n",
    "\n",
    "client.put_tensor(\"input\", sample)\n",
    "client.set_model(\"rf_regressor\", model, \"ONNX\", device=\"CPU\")\n",
    "client.run_model(\"rf_regressor\", inputs=\"input\", outputs=\"output\")\n",
    "print(client.get_tensor(\"output\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea774e12-956c-4bbe-be57-af416123c307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp.stop(db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15662aeb-1b00-4887-9e47-2c596fdbe941",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>  </th><th>Name          </th><th>Entity-Type  </th><th>JobID  </th><th>RunID  </th><th>Time   </th><th>Status                         </th><th>Returncode  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>0 </td><td>orchestrator_0</td><td>DBNode       </td><td>2809   </td><td>0      </td><td>70.9690</td><td>SmartSimStatus.STATUS_CANCELLED</td><td>0           </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<thead>\\n<tr><th>  </th><th>Name          </th><th>Entity-Type  </th><th>JobID  </th><th>RunID  </th><th>Time   </th><th>Status                         </th><th>Returncode  </th></tr>\\n</thead>\\n<tbody>\\n<tr><td>0 </td><td>orchestrator_0</td><td>DBNode       </td><td>2809   </td><td>0      </td><td>70.9690</td><td>SmartSimStatus.STATUS_CANCELLED</td><td>0           </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.summary(style=\"html\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5daa7402-f62b-4710-a269-e078b2ce08ac",
   "metadata": {},
   "source": [
    "# Colocated Deployment\n",
    "\n",
    "A colocated Orchestrator is a special type of Orchestrator that is deployed\n",
    "on the same compute hosts an a Model instance defined by the user. In this\n",
    "deployment, the database is not connected together in a cluster and each shard\n",
    "of the database is addressed individually by the processes running on that compute\n",
    "host. This is particularly important for GPU-intensive workloads which require\n",
    "frequent communication with the database.\n",
    "\n",
    "<center><img src=\"https://www.craylabs.org/docs/_images/colocated_orchestrator-1.png\" alt=\"lattice\" width=\"600\"/></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "030448b9-67f0-4cd4-889e-3fac65ccaeaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create colocated model\n",
    "colo_settings = exp.create_run_settings(\n",
    "    exe=\"python\",\n",
    "    exe_args=\"./colo-db-torch-example.py\"\n",
    ")\n",
    "\n",
    "colo_model = exp.create_model(\"colocated_model\", colo_settings)\n",
    "colo_model.colocate_db_tcp(\n",
    "    port=6780,\n",
    "    db_cpus=1,\n",
    "    debug=False,\n",
    "    ifname=\"lo\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8f30f83-9f93-41f6-8276-c805bc9b6eda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:30:35 HPE-C02YR4ANLVCJ SmartSim[1187:MainThread] INFO \n",
      "\n",
      "=== Launch Summary ===\n",
      "Experiment: Inference-Tutorial\n",
      "Experiment Path: /home/craylabs/tutorials/ml_inference/Inference-Tutorial\n",
      "Launcher: local\n",
      "Models: 1\n",
      "Database Status: inactive\n",
      "\n",
      "=== Models ===\n",
      "colocated_model\n",
      "Executable: /usr/local/anaconda3/envs/ss-py3.10/bin/python\n",
      "Executable Arguments: ./colo-db-torch-example.py\n",
      "Co-located Database: True\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:30:38 HPE-C02YR4ANLVCJ SmartSim[1187:JobManager] WARNING colocated_model(3199): SmartSimStatus.STATUS_FAILED\n",
      "19:30:38 HPE-C02YR4ANLVCJ SmartSim[1187:JobManager] WARNING colocated_model failed. See below for details \n",
      "Job status at failure: SmartSimStatus.STATUS_FAILED \n",
      "Launcher status at failure: Failed \n",
      "Job returncode: 2 \n",
      "Error and output file located at: /home/craylabs/tutorials/ml_inference/Inference-Tutorial/colocated_model\n"
     ]
    }
   ],
   "source": [
    "exp.start(colo_model, summary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87a21608-c1cd-43db-8c11-150ef44d0363",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>  </th><th>Name           </th><th>Entity-Type  </th><th>JobID  </th><th>RunID  </th><th>Time   </th><th>Status                         </th><th>Returncode  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>0 </td><td>orchestrator_0 </td><td>DBNode       </td><td>2809   </td><td>0      </td><td>70.9690</td><td>SmartSimStatus.STATUS_CANCELLED</td><td>0           </td></tr>\n",
       "<tr><td>1 </td><td>colocated_model</td><td>Model        </td><td>3199   </td><td>0      </td><td>3.1599 </td><td>SmartSimStatus.STATUS_FAILED   </td><td>2           </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<thead>\\n<tr><th>  </th><th>Name           </th><th>Entity-Type  </th><th>JobID  </th><th>RunID  </th><th>Time   </th><th>Status                         </th><th>Returncode  </th></tr>\\n</thead>\\n<tbody>\\n<tr><td>0 </td><td>orchestrator_0 </td><td>DBNode       </td><td>2809   </td><td>0      </td><td>70.9690</td><td>SmartSimStatus.STATUS_CANCELLED</td><td>0           </td></tr>\\n<tr><td>1 </td><td>colocated_model</td><td>Model        </td><td>3199   </td><td>0      </td><td>3.1599 </td><td>SmartSimStatus.STATUS_FAILED   </td><td>2           </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.summary(style=\"html\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
