{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42905082",
   "metadata": {},
   "source": [
    "# Setting up a Ray cluster with SmartSim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dc796b",
   "metadata": {},
   "source": [
    "## 1. Start the cluster\n",
    "We set up a SmartSim experiment, which will handle the launch of the Ray cluster.\n",
    "\n",
    "First we import the relevant modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74c17fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smartsim import Experiment\n",
    "from smartsim.ray import RayCluster\n",
    "\n",
    "NUM_WORKERS = 8\n",
    "alloc=None;#slurm.get_allocation(nodes=1+NUM_WORKERS, time=\"12:00:00\", options={\"ntasks\": str(1+NUM_WORKERS), \"partition\": \"spider\", \"C\": \"V100\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a69118",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:00:50 crystal SmartSim[9377] INFO Working in previously created experiment\n"
     ]
    }
   ],
   "source": [
    "exp = Experiment(\"ray-cluster\", launcher='pbs')\n",
    "cluster = RayCluster(name=\"ray-cluster\", run_args={}, path='',\n",
    "                     launcher='pbs', workers=NUM_WORKERS, alloc=alloc, batch=True, ray_num_cpus=38)\n",
    "\n",
    "if cluster.batch:\n",
    "    cluster.head_model.batch_settings._preamble += [\"source ~/.bashrc\", \"conda activate smartsim\"]\n",
    "    if NUM_WORKERS:\n",
    "        cluster.worker_model.batch_settings._preamble += [\"source ~/.bashrc\", \"conda activate smartsim\"]\n",
    "\n",
    "exp.generate(cluster, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b255fa37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:00:53 crystal SmartSim[9377] ERROR An error occurred when launching head \n",
      "Check error and output files for details.\n",
      "Name: head\n",
      "Type: RayHead\n",
      "Executable: /lus/scratch/arigazzi/anaconda3/envs/smartsim/bin/python\n",
      "Executable arguments: ['/lus/snx11242/arigazzi/smartsim-dev/SmartSim/smartsim/ray/rayserverstarter.py', '--num-cpus=38', '--port=6780', '--redis-password=a8c08c1b-44fd-45c4-a7e6-d6756783a222']\n",
      "Run Command: aprun\n",
      "\n",
      "16:00:53 crystal SmartSim[9377] ERROR Job step head failed to launch\n"
     ]
    },
    {
     "ename": "SmartSimError",
     "evalue": "Job step head failed to launch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLauncherError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/lus/snx11242/arigazzi/smartsim-dev/SmartSim/smartsim/control/controller.py\u001b[0m in \u001b[0;36m_launch_step\u001b[0;34m(self, job_step, entity)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0mjob_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLauncherError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lus/snx11242/arigazzi/smartsim-dev/SmartSim/smartsim/launcher/pbs/pbsLauncher.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, step)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mLauncherError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Qsub batch submission failed\\n {out}\\n {err}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLauncherError\u001b[0m: Qsub batch submission failed\n \n pbs_iff: cannot connect to host\npbs_iff: cannot connect to host\nNo Permission.\nqsub: cannot connect to server sdb (errno=15007)\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSmartSimError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c855113fa31d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/lus/snx11242/arigazzi/smartsim-dev/SmartSim/smartsim/experiment.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, block, summary, *args)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_control\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSmartSimError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lus/snx11242/arigazzi/smartsim-dev/SmartSim/smartsim/control/controller.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, block, *args)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanity_check_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morchestrator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_lists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morchestrator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mray_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# start the job manager thread if not already started\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lus/snx11242/arigazzi/smartsim-dev/SmartSim/smartsim/control/controller.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, entities, entity_lists, orchestrator, ray_clusters)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mray_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mray_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch_ray_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;31m# create all steps prior to launch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lus/snx11242/arigazzi/smartsim-dev/SmartSim/smartsim/control/controller.py\u001b[0m in \u001b[0;36m_launch_ray_cluster\u001b[0;34m(self, ray_cluster)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mray_cluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mhead_batch_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_batch_job_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray_cluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_batch_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mray_cluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0mray_cluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ray_head_node_address\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lus/snx11242/arigazzi/smartsim-dev/SmartSim/smartsim/control/controller.py\u001b[0m in \u001b[0;36m_launch_step\u001b[0;34m(self, job_step, entity)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"{entity}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mSmartSimError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Job step {entity.name} failed to launch\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_restart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSmartSimError\u001b[0m: Job step head failed to launch"
     ]
    }
   ],
   "source": [
    "exp.start(cluster, block=False, summary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0af6be4",
   "metadata": {},
   "source": [
    "## 2. Start the ray driver script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf9607",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.start_ray_job('/lus/scratch/arigazzi/smartsim-dev/SmartSim/tutorials/05_starting_ray/templates/ppo_tune.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5afafa-61bd-49d6-8162-1dc3cad4ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.start_ray_job('/lus/scratch/arigazzi/smartsim-dev/SmartSim/tutorials/05_starting_ray/templates/ppo_train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.start_ray_job('/lus/scratch/arigazzi/smartsim-dev/SmartSim/tutorials/05_starting_ray/templates/mnist_pytorch_trainable.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742ce83",
   "metadata": {},
   "source": [
    "## 3. Stop cluster and release allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24151d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if alloc:\n",
    "    slurm.release_allocation(alloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68019ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized\n",
      "<IPython.core.display.HTML object>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32602)\u001b[0m 2021-05-19 15:38:52,005\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32598)\u001b[0m 2021-05-19 15:38:57,975\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32600)\u001b[0m 2021-05-19 15:38:57,981\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32595)\u001b[0m 2021-05-19 15:38:58,191\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32597)\u001b[0m 2021-05-19 15:38:58,188\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32594)\u001b[0m 2021-05-19 15:38:58,229\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32602)\u001b[0m 2021-05-19 15:39:24,282\tINFO trainable.py:101 -- Trainable.setup took 32.277 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=32598)\u001b[0m 2021-05-19 15:39:38,771\tINFO trainable.py:101 -- Trainable.setup took 40.796 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=32600)\u001b[0m 2021-05-19 15:39:38,825\tINFO trainable.py:101 -- Trainable.setup took 40.844 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=32595)\u001b[0m 2021-05-19 15:39:38,971\tINFO trainable.py:101 -- Trainable.setup took 40.781 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=32597)\u001b[0m 2021-05-19 15:39:39,240\tINFO trainable.py:101 -- Trainable.setup took 41.053 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=32594)\u001b[0m 2021-05-19 15:39:39,634\tINFO trainable.py:101 -- Trainable.setup took 41.406 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00000 reported episode_reward_max=86.0,episode_reward_min=9.0,episode_reward_mean=22.982758620689655,episode_len_mean=22.982758620689655,episode_media={},episodes_this_iter=174,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.6501029991226115, 'mean_inference_ms': 17.685853087411164, 'mean_action_processing_ms': 0.6844336755076307, 'mean_env_wait_ms': 0.8950426797003957, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 43255.553, 'sample_throughput': 92.474, 'learn_time_ms': 64934.756, 'learn_throughput': 61.6, 'update_time_ms': 7.278},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.001, 'total_loss': 83.75345146656036, 'policy_loss': -0.05583573368494399, 'vf_loss': 83.80137264728546, 'vf_explained_var': 0.28486428, 'kl': 0.039563861675560474, 'entropy': 0.6551137436181307, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.5140000000000002, 'ram_util_percent': 5.333999999999998} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.001, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00003 reported episode_reward_max=60.0,episode_reward_min=8.0,episode_reward_mean=21.9010989010989,episode_len_mean=21.9010989010989,episode_media={},episodes_this_iter=182,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.4421765838896563, 'mean_inference_ms': 15.611708908246136, 'mean_action_processing_ms': 0.5540046581595939, 'mean_env_wait_ms': 0.7862425435214007, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 38061.338, 'sample_throughput': 105.094, 'learn_time_ms': 61545.555, 'learn_throughput': 64.993, 'update_time_ms': 6.693},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0012727272727272728, 'total_loss': 58.846105098724365, 'policy_loss': -0.061687562440056354, 'vf_loss': 58.899521350860596, 'vf_explained_var': 0.32918245, 'kl': 0.041356766829267144, 'entropy': 0.6526412181556225, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.4260416666666667, 'ram_util_percent': 5.299999999999999} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0012727272727272728, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00005 reported episode_reward_max=61.0,episode_reward_min=8.0,episode_reward_mean=21.641304347826086,episode_len_mean=21.641304347826086,episode_media={},episodes_this_iter=184,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.0953377031806995, 'mean_inference_ms': 15.64209256274649, 'mean_action_processing_ms': 0.6880239405009434, 'mean_env_wait_ms': 1.0014703146438413, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 38315.208, 'sample_throughput': 104.397, 'learn_time_ms': 61517.805, 'learn_throughput': 65.022, 'update_time_ms': 140.547},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0014545454545454547, 'total_loss': 61.03749632835388, 'policy_loss': -0.054428475035820156, 'vf_loss': 61.08410930633545, 'vf_explained_var': 0.2879268, 'kl': 0.03907776332926005, 'entropy': 0.6560273822396994, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.425, 'ram_util_percent': 5.299999999999999} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0014545454545454547, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00001 reported episode_reward_max=74.0,episode_reward_min=9.0,episode_reward_mean=21.770491803278688,episode_len_mean=21.770491803278688,episode_media={},episodes_this_iter=183,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.092395931752925, 'mean_inference_ms': 15.61968765983636, 'mean_action_processing_ms': 0.4095623605787805, 'mean_env_wait_ms': 1.2411664562889086, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 38344.827, 'sample_throughput': 104.317, 'learn_time_ms': 61782.186, 'learn_throughput': 64.744, 'update_time_ms': 8.821},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.001090909090909091, 'total_loss': 81.68029713630676, 'policy_loss': -0.052230939792934805, 'vf_loss': 81.72454130649567, 'vf_explained_var': 0.27245754, 'kl': 0.03992917004507035, 'entropy': 0.6543322298675776, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.4260416666666667, 'ram_util_percent': 5.299999999999999} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.001090909090909091, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00002 reported episode_reward_max=70.0,episode_reward_min=8.0,episode_reward_mean=21.85635359116022,episode_len_mean=21.85635359116022,episode_media={},episodes_this_iter=181,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.8664822153901718, 'mean_inference_ms': 15.665498188207152, 'mean_action_processing_ms': 0.5770581640043583, 'mean_env_wait_ms': 1.041189100522861, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 38248.716, 'sample_throughput': 104.579, 'learn_time_ms': 61926.169, 'learn_throughput': 64.593, 'update_time_ms': 10.518},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0011818181818181819, 'total_loss': 86.65823686122894, 'policy_loss': -0.06186266423901543, 'vf_loss': 86.71211504936218, 'vf_explained_var': 0.27370447, 'kl': 0.03992348175961524, 'entropy': 0.6541482340544462, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.427835051546392, 'ram_util_percent': 5.299999999999999} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0011818181818181819, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=39406, ip=172.30.49.190)\u001b[0m 2021-05-19 15:41:23,961\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=39480, ip=172.30.49.190)\u001b[0m 2021-05-19 15:41:23,967\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=39509, ip=172.30.49.190)\u001b[0m 2021-05-19 15:41:23,963\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=39526, ip=172.30.49.190)\u001b[0m 2021-05-19 15:41:23,970\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=39544, ip=172.30.49.190)\u001b[0m 2021-05-19 15:41:23,965\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=39499, ip=172.30.49.190)\u001b[0m 2021-05-19 15:41:24,122\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00004 reported episode_reward_max=91.0,episode_reward_min=8.0,episode_reward_mean=22.11731843575419,episode_len_mean=22.11731843575419,episode_media={},episodes_this_iter=179,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.0451030319036347, 'mean_inference_ms': 13.869340846331925, 'mean_action_processing_ms': 0.6118810980377736, 'mean_env_wait_ms': 0.9321321311947851, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 45110.224, 'sample_throughput': 88.672, 'learn_time_ms': 65871.576, 'learn_throughput': 60.724, 'update_time_ms': 124.506},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0013636363636363637, 'total_loss': 77.54526734352112, 'policy_loss': -0.04880880439304747, 'vf_loss': 77.58650612831116, 'vf_explained_var': 0.27982253, 'kl': 0.03785534529015422, 'entropy': 0.6561946384608746, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.4233644859813088, 'ram_util_percent': 5.299999999999998} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0013636363636363637, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2K\u001b[36m(pid=32576)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m <IPython.core.display.HTML object>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=39406, ip=172.30.49.190)\u001b[0m 2021-05-19 15:41:41,784\tINFO trainable.py:101 -- Trainable.setup took 17.824 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=39480, ip=172.30.49.190)\u001b[0m 2021-05-19 15:41:41,799\tINFO trainable.py:101 -- Trainable.setup took 17.836 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=39509, ip=172.30.49.190)\u001b[0m 2021-05-19 15:41:41,816\tINFO trainable.py:101 -- Trainable.setup took 17.856 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=39526, ip=172.30.49.190)\u001b[0m 2021-05-19 15:41:41,828\tINFO trainable.py:101 -- Trainable.setup took 17.863 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=39544, ip=172.30.49.190)\u001b[0m 2021-05-19 15:41:41,927\tINFO trainable.py:101 -- Trainable.setup took 17.967 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=39499, ip=172.30.49.190)\u001b[0m 2021-05-19 15:41:42,192\tINFO trainable.py:101 -- Trainable.setup took 18.086 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00000 reported episode_reward_max=188.0,episode_reward_min=9.0,episode_reward_mean=46.85,episode_len_mean=46.85,episode_media={},episodes_this_iter=67,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.4564239079623877, 'mean_inference_ms': 16.402095843358897, 'mean_action_processing_ms': 0.6417709862110046, 'mean_env_wait_ms': 0.8880732257390872, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 38492.344, 'sample_throughput': 103.917, 'learn_time_ms': 64690.736, 'learn_throughput': 61.833, 'update_time_ms': 7.032},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.001, 'total_loss': 446.1968250274658, 'policy_loss': -0.03938280988950282, 'vf_loss': 446.2305965423584, 'vf_explained_var': 0.15547001, 'kl': 0.01872205507243052, 'entropy': 0.5956625882536173, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4000000000000006, 'ram_util_percent': 5.344086021505374} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.001, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00003 reported episode_reward_max=200.0,episode_reward_min=9.0,episode_reward_mean=44.85,episode_len_mean=44.85,episode_media={},episodes_this_iter=76,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.2710108159940499, 'mean_inference_ms': 15.531577080700066, 'mean_action_processing_ms': 0.5939084680017678, 'mean_env_wait_ms': 0.7121578165210138, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 37089.76, 'sample_throughput': 107.846, 'learn_time_ms': 61329.713, 'learn_throughput': 65.221, 'update_time_ms': 7.774},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0012727272727272728, 'total_loss': 375.8993082046509, 'policy_loss': -0.024017524352530017, 'vf_loss': 375.91731452941895, 'vf_explained_var': 0.19015318, 'kl': 0.02003749692812562, 'entropy': 0.5948826093226671, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4000000000000004, 'ram_util_percent': 5.35164835164835} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0012727272727272728, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00005 reported episode_reward_max=200.0,episode_reward_min=9.0,episode_reward_mean=44.92,episode_len_mean=44.92,episode_media={},episodes_this_iter=74,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.9877680596643134, 'mean_inference_ms': 15.4587739535252, 'mean_action_processing_ms': 0.6896858521127197, 'mean_env_wait_ms': 0.9643644769312119, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 37266.874, 'sample_throughput': 107.334, 'learn_time_ms': 61299.363, 'learn_throughput': 65.254, 'update_time_ms': 73.948},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0014545454545454547, 'total_loss': 409.50276947021484, 'policy_loss': -0.039437732615624554, 'vf_loss': 409.53558254241943, 'vf_explained_var': 0.19039579, 'kl': 0.022083265328546986, 'entropy': 0.5964617561548948, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4010989010989015, 'ram_util_percent': 5.347252747252746} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0014545454545454547, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00001 reported episode_reward_max=168.0,episode_reward_min=9.0,episode_reward_mean=45.91,episode_len_mean=45.91,episode_media={},episodes_this_iter=65,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.1434748596650979, 'mean_inference_ms': 15.36125835363765, 'mean_action_processing_ms': 0.30508294036587336, 'mean_env_wait_ms': 1.1892300301016772, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 37327.661, 'sample_throughput': 107.159, 'learn_time_ms': 61689.378, 'learn_throughput': 64.841, 'update_time_ms': 92.651},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.001090909090909091, 'total_loss': 461.0493097305298, 'policy_loss': -0.029938818654045463, 'vf_loss': 461.0730266571045, 'vf_explained_var': 0.18609022, 'kl': 0.020720546192023903, 'entropy': 0.5908530335873365, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.405376344086022, 'ram_util_percent': 5.360215053763439} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.001090909090909091, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00002 reported episode_reward_max=179.0,episode_reward_min=8.0,episode_reward_mean=45.36,episode_len_mean=45.36,episode_media={},episodes_this_iter=72,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.9452986714704739, 'mean_inference_ms': 15.308920215187346, 'mean_action_processing_ms': 0.5126399358594731, 'mean_env_wait_ms': 0.999937219481254, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 37228.603, 'sample_throughput': 107.444, 'learn_time_ms': 62053.699, 'learn_throughput': 64.46, 'update_time_ms': 7.007},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0011818181818181819, 'total_loss': 409.3650779724121, 'policy_loss': -0.02993359966785647, 'vf_loss': 409.3887052536011, 'vf_explained_var': 0.17189422, 'kl': 0.021013032062910497, 'entropy': 0.5957129541784525, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4053191489361707, 'ram_util_percent': 5.357446808510636} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0011818181818181819, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00010 reported episode_reward_max=116.0,episode_reward_min=9.0,episode_reward_mean=23.28654970760234,episode_len_mean=23.28654970760234,episode_media={},episodes_this_iter=171,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.0988357732388596, 'mean_inference_ms': 13.88231115015288, 'mean_action_processing_ms': 0.6566402518604412, 'mean_env_wait_ms': 0.9772659668833695, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 34906.29, 'sample_throughput': 114.593, 'learn_time_ms': 52644.252, 'learn_throughput': 75.982, 'update_time_ms': 7.602},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0019090909090909093, 'total_loss': 149.79577159881592, 'policy_loss': -0.05035888279962819, 'vf_loss': 149.83846616744995, 'vf_explained_var': 0.19800937, 'kl': 0.03832307585980743, 'entropy': 0.654933225363493, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.389534883720931, 'ram_util_percent': 4.899999999999998} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0019090909090909093, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:10,067\tWARNING util.py:161 -- The `callbacks.on_trial_result` operation took 0.512 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:10,069\tWARNING util.py:161 -- The `process_trial_result` operation took 0.515 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:10,069\tWARNING util.py:161 -- Processing trial results took 0.515 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:10,069\tWARNING util.py:161 -- The `process_trial` operation took 0.515 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00007 reported episode_reward_max=60.0,episode_reward_min=9.0,episode_reward_mean=21.06878306878307,episode_len_mean=21.06878306878307,episode_media={},episodes_this_iter=189,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.828155612580164, 'mean_inference_ms': 14.442360556331888, 'mean_action_processing_ms': 0.6357064856156606, 'mean_env_wait_ms': 0.8940045761700655, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 34890.46, 'sample_throughput': 114.645, 'learn_time_ms': 52703.412, 'learn_throughput': 75.896, 'update_time_ms': 7.871},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0016363636363636363, 'total_loss': 56.70728224515915, 'policy_loss': -0.04471227078465745, 'vf_loss': 56.744479298591614, 'vf_explained_var': 0.3108323, 'kl': 0.03757377015426755, 'entropy': 0.6564968153834343, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.3906976744186053, 'ram_util_percent': 4.899999999999998} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0016363636363636363, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00009 reported episode_reward_max=61.0,episode_reward_min=9.0,episode_reward_mean=21.090425531914892,episode_len_mean=21.090425531914892,episode_media={},episodes_this_iter=188,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.3299340264910553, 'mean_inference_ms': 13.882858257059864, 'mean_action_processing_ms': 0.5857810685930891, 'mean_env_wait_ms': 0.9128706803071827, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 35674.331, 'sample_throughput': 112.125, 'learn_time_ms': 52131.681, 'learn_throughput': 76.729, 'update_time_ms': 6.828},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0018181818181818182, 'total_loss': 58.44193172454834, 'policy_loss': -0.05741100263549015, 'vf_loss': 58.49106442928314, 'vf_explained_var': 0.31991106, 'kl': 0.04139802802819759, 'entropy': 0.6527688689529896, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.389534883720931, 'ram_util_percent': 4.899999999999998} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0018181818181818182, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:12,334\tWARNING util.py:161 -- The `callbacks.on_trial_result` operation took 0.779 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:12,335\tWARNING util.py:161 -- The `process_trial_result` operation took 0.781 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:12,335\tWARNING util.py:161 -- Processing trial results took 0.781 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:12,335\tWARNING util.py:161 -- The `process_trial` operation took 0.781 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00011 reported episode_reward_max=72.0,episode_reward_min=9.0,episode_reward_mean=23.197674418604652,episode_len_mean=23.197674418604652,episode_media={},episodes_this_iter=172,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.0360927510186442, 'mean_inference_ms': 14.335171140007951, 'mean_action_processing_ms': 0.7109910115666653, 'mean_env_wait_ms': 0.81544464527809, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 34873.709, 'sample_throughput': 114.7, 'learn_time_ms': 52829.169, 'learn_throughput': 75.716, 'update_time_ms': 7.749},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.002, 'total_loss': 94.76134920120239, 'policy_loss': -0.04660388032789342, 'vf_loss': 94.80022430419922, 'vf_explained_var': 0.24756524, 'kl': 0.0386274685151875, 'entropy': 0.6557049844413996, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.3906976744186053, 'ram_util_percent': 4.899999999999998} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.002, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:13,026\tWARNING util.py:161 -- The `callbacks.on_trial_result` operation took 0.684 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:13,027\tWARNING util.py:161 -- The `process_trial_result` operation took 0.686 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:13,027\tWARNING util.py:161 -- Processing trial results took 0.686 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:13,027\tWARNING util.py:161 -- The `process_trial` operation took 0.686 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00006 reported episode_reward_max=125.0,episode_reward_min=8.0,episode_reward_mean=24.714285714285715,episode_len_mean=24.714285714285715,episode_media={},episodes_this_iter=161,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.9822105554057337, 'mean_inference_ms': 14.296846717232409, 'mean_action_processing_ms': 0.5160799470851087, 'mean_env_wait_ms': 0.7743891812214223, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 34729.229, 'sample_throughput': 115.177, 'learn_time_ms': 52569.878, 'learn_throughput': 76.089, 'update_time_ms': 50.26},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0015454545454545456, 'total_loss': 170.70933389663696, 'policy_loss': -0.052674749385914765, 'vf_loss': 170.75411796569824, 'vf_explained_var': 0.16828957, 'kl': 0.039459910010918975, 'entropy': 0.6548414379358292, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.3896551724137938, 'ram_util_percent': 4.899999999999998} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0015454545454545456, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:13,719\tWARNING util.py:161 -- The `callbacks.on_trial_result` operation took 0.686 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:13,720\tWARNING util.py:161 -- The `process_trial_result` operation took 0.688 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:13,720\tWARNING util.py:161 -- Processing trial results took 0.688 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:13,720\tWARNING util.py:161 -- The `process_trial` operation took 0.689 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00008 reported episode_reward_max=85.0,episode_reward_min=9.0,episode_reward_mean=24.2,episode_len_mean=24.2,episode_media={},episodes_this_iter=165,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.7644982411824482, 'mean_inference_ms': 14.623200097806958, 'mean_action_processing_ms': 0.41561189750015226, 'mean_env_wait_ms': 0.776773469396422, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 34836.152, 'sample_throughput': 114.823, 'learn_time_ms': 52623.561, 'learn_throughput': 76.012, 'update_time_ms': 7.119},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0017272727272727275, 'total_loss': 104.73519110679626, 'policy_loss': -0.04131202757707797, 'vf_loss': 104.76919102668762, 'vf_explained_var': 0.23390914, 'kl': 0.03656290646176785, 'entropy': 0.657233040779829, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.3896551724137938, 'ram_util_percent': 4.899999999999998} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0017272727272727275, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:14,452\tWARNING util.py:161 -- The `callbacks.on_trial_result` operation took 0.725 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:14,454\tWARNING util.py:161 -- The `process_trial_result` operation took 0.727 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:14,454\tWARNING util.py:161 -- Processing trial results took 0.727 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:14,454\tWARNING util.py:161 -- The `process_trial` operation took 0.728 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:15,179\tWARNING util.py:161 -- The `callbacks.on_trial_result` operation took 0.721 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:15,181\tWARNING util.py:161 -- The `process_trial_result` operation took 0.723 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:15,181\tWARNING util.py:161 -- Processing trial results took 0.723 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:43:15,181\tWARNING util.py:161 -- The `process_trial` operation took 0.723 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00004 reported episode_reward_max=169.0,episode_reward_min=9.0,episode_reward_mean=48.19,episode_len_mean=48.19,episode_media={},episodes_this_iter=63,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.1014962094020115, 'mean_inference_ms': 13.701296394183627, 'mean_action_processing_ms': 0.5380312697771665, 'mean_env_wait_ms': 0.8839612576262804, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 38619.603, 'sample_throughput': 103.574, 'learn_time_ms': 74361.051, 'learn_throughput': 53.792, 'update_time_ms': 63.695},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0013636363636363637, 'total_loss': 506.90384101867676, 'policy_loss': -0.032757237800979055, 'vf_loss': 506.93035316467285, 'vf_explained_var': 0.15885347, 'kl': 0.020788979483768344, 'entropy': 0.5909513998776674, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.5606060606060606, 'ram_util_percent': 5.326262626262626} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0013636363636363637, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36057)\u001b[0m 2021-05-19 15:43:45,121\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=36059)\u001b[0m 2021-05-19 15:43:45,123\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=36057)\u001b[0m 2021-05-19 15:44:25,707\tINFO trainable.py:101 -- Trainable.setup took 40.587 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=36059)\u001b[0m 2021-05-19 15:44:25,831\tINFO trainable.py:101 -- Trainable.setup took 40.708 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00010 reported episode_reward_max=200.0,episode_reward_min=9.0,episode_reward_mean=45.31,episode_len_mean=45.31,episode_media={},episodes_this_iter=72,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.8638825331225629, 'mean_inference_ms': 11.729535241701276, 'mean_action_processing_ms': 0.584269493405715, 'mean_env_wait_ms': 0.9035118919218319, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 27624.12, 'sample_throughput': 144.801, 'learn_time_ms': 55483.344, 'learn_throughput': 72.094, 'update_time_ms': 56.183},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0019090909090909093, 'total_loss': 326.1769299507141, 'policy_loss': -0.027764145022956654, 'vf_loss': 326.1992211341858, 'vf_explained_var': 0.20572773, 'kl': 0.01823339131078683, 'entropy': 0.6080934461206198, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.397435897435898, 'ram_util_percent': 4.899999999999998} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0019090909090909093, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "<IPython.core.display.HTML object>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The actor or task with ID ffffffffffffffffe4fee91c91bb6f05b75c3bb901000000 cannot be scheduled right now. It requires {CPU_group_0_db9c63726e369ba8ebf20eed9a99f77f: 1.000000}, {CPU_group_db9c63726e369ba8ebf20eed9a99f77f: 1.000000} for placement, but this node only has remaining {0.000000/18.000000 CPU, 87.135904 GiB/87.135904 GiB memory, 37.343959 GiB/37.343959 GiB object_store_memory, 1.000000/1.000000 CPU_group_0_db9c63726e369ba8ebf20eed9a99f77f, 1.000000/1.000000 CPU_group_1_e230dffb3fd10e5648088dd8af6d9169, 1.000000/1.000000 CPU_group_1_db9c63726e369ba8ebf20eed9a99f77f, 1.000000/1.000000 CPU_group_1_19401f96d5f51f491cfdeacd17a53e34, 1.000000/1.000000 CPU_group_1_3385cc0525dfcbbd4059316246676e35, 0.000000/1.000000 CPU_group_0_5763e5534f3359a5da9b9be1b5b983e2, 1.000000/1.000000 node:10.128.0.18, 0.000000/3.000000 CPU_group_3385cc0525dfcbbd4059316246676e35, 1.000000/1.000000 CPU_group_2_3385cc0525dfcbbd4059316246676e35, 1.000000/1.000000 CPU_group_2_5763e5534f3359a5da9b9be1b5b983e2, 1.000000/1.000000 CPU_group_2_19401f96d5f51f491cfdeacd17a53e34, 1.000000/1.000000 CPU_group_1_04424e424ef3127dac20feda22e6c560, 0.000000/1.000000 CPU_group_0_19401f96d5f51f491cfdeacd17a53e34, 0.000000/3.000000 CPU_group_e230dffb3fd10e5648088dd8af6d9169, 1.000000/1.000000 CPU_group_1_5763e5534f3359a5da9b9be1b5b983e2, 0.000000/1.000000 CPU_group_0_04424e424ef3127dac20feda22e6c560, 3.000000/3.000000 CPU_group_db9c63726e369ba8ebf20eed9a99f77f, 1.000000/1.000000 CPU_group_2_db9c63726e369ba8ebf20eed9a99f77f, 0.000000/3.000000 CPU_group_19401f96d5f51f491cfdeacd17a53e34, 0.000000/3.000000 CPU_group_04424e424ef3127dac20feda22e6c560, 1.000000/1.000000 CPU_group_2_04424e424ef3127dac20feda22e6c560, 0.000000/1.000000 CPU_group_0_e230dffb3fd10e5648088dd8af6d9169, 0.000000/3.000000 CPU_group_5763e5534f3359a5da9b9be1b5b983e2, 0.000000/1.000000 CPU_group_0_3385cc0525dfcbbd4059316246676e35, 1.000000/1.000000 CPU_group_2_e230dffb3fd10e5648088dd8af6d9169}\n",
      ". In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00007 reported episode_reward_max=163.0,episode_reward_min=11.0,episode_reward_mean=44.11,episode_len_mean=44.11,episode_media={},episodes_this_iter=82,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.7046348658742994, 'mean_inference_ms': 13.633418806857035, 'mean_action_processing_ms': 0.593093972633863, 'mean_env_wait_ms': 1.061995075303193, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 32531.963, 'sample_throughput': 122.956, 'learn_time_ms': 63166.931, 'learn_throughput': 63.324, 'update_time_ms': 7.678},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0016363636363636363, 'total_loss': 296.44860076904297, 'policy_loss': -0.026098009548150003, 'vf_loss': 296.4687080383301, 'vf_explained_var': 0.20200726, 'kl': 0.019976341718574986, 'entropy': 0.6104601603001356, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4145833333333335, 'ram_util_percent': 4.933333333333333} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0016363636363636363, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00009 reported episode_reward_max=190.0,episode_reward_min=9.0,episode_reward_mean=45.61,episode_len_mean=45.61,episode_media={},episodes_this_iter=67,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.1828713909384252, 'mean_inference_ms': 13.551678556129103, 'mean_action_processing_ms': 0.5582970548766759, 'mean_env_wait_ms': 0.7382705195927187, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 33896.004, 'sample_throughput': 118.008, 'learn_time_ms': 62949.476, 'learn_throughput': 63.543, 'update_time_ms': 7.913},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0018181818181818182, 'total_loss': 478.7930784225464, 'policy_loss': -0.03297928557731211, 'vf_loss': 478.81946849823, 'vf_explained_var': 0.13942945, 'kl': 0.021941187005722895, 'entropy': 0.5880301501601934, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4151515151515153, 'ram_util_percent': 4.934343434343433} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0018181818181818182, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00011 reported episode_reward_max=182.0,episode_reward_min=11.0,episode_reward_mean=45.64,episode_len_mean=45.64,episode_media={},episodes_this_iter=75,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.0173416982020387, 'mean_inference_ms': 13.756014908305346, 'mean_action_processing_ms': 0.7039103230724897, 'mean_env_wait_ms': 0.8109332903739204, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 33126.746, 'sample_throughput': 120.748, 'learn_time_ms': 63454.315, 'learn_throughput': 63.037, 'update_time_ms': 51.395},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.002, 'total_loss': 319.820369720459, 'policy_loss': -0.02792505215620622, 'vf_loss': 319.8420171737671, 'vf_explained_var': 0.18515068, 'kl': 0.02093524404335767, 'entropy': 0.6024804245680571, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.415, 'ram_util_percent': 4.9319999999999995} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.002, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00006 reported episode_reward_max=200.0,episode_reward_min=10.0,episode_reward_mean=48.69,episode_len_mean=48.69,episode_media={},episodes_this_iter=67,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.9800223761223159, 'mean_inference_ms': 13.721118044548552, 'mean_action_processing_ms': 0.48279729956933026, 'mean_env_wait_ms': 0.7938138645809448, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 33111.536, 'sample_throughput': 120.804, 'learn_time_ms': 63549.182, 'learn_throughput': 62.943, 'update_time_ms': 28.697},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0015454545454545456, 'total_loss': 435.52845668792725, 'policy_loss': -0.023329457064392045, 'vf_loss': 435.5459814071655, 'vf_explained_var': 0.17419547, 'kl': 0.019336602912517264, 'entropy': 0.5932292677462101, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4141414141414144, 'ram_util_percent': 4.935353535353534} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0015454545454545456, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00008 reported episode_reward_max=200.0,episode_reward_min=9.0,episode_reward_mean=46.29,episode_len_mean=46.29,episode_media={},episodes_this_iter=73,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.7537176549514777, 'mean_inference_ms': 14.208120299942934, 'mean_action_processing_ms': 0.3736338419764774, 'mean_env_wait_ms': 0.747428307846206, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 33017.628, 'sample_throughput': 121.147, 'learn_time_ms': 63482.875, 'learn_throughput': 63.009, 'update_time_ms': 7.703},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0017272727272727275, 'total_loss': 395.6604766845703, 'policy_loss': -0.026198714622296393, 'vf_loss': 395.6812410354614, 'vf_explained_var': 0.15844178, 'kl': 0.01814256186480634, 'entropy': 0.6079575698822737, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.415, 'ram_util_percent': 4.934} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0017272727272727275, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=42350, ip=172.30.49.190)\u001b[0m 2021-05-19 15:45:09,721\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=42399, ip=172.30.49.190)\u001b[0m 2021-05-19 15:45:21,430\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00000 reported episode_reward_max=200.0,episode_reward_min=9.0,episode_reward_mean=79.05,episode_len_mean=79.05,episode_media={},episodes_this_iter=27,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.4075163431411692, 'mean_inference_ms': 16.272974541682682, 'mean_action_processing_ms': 0.680826604522172, 'mean_env_wait_ms': 0.955387804373008, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 42682.971, 'sample_throughput': 93.714, 'learn_time_ms': 76302.393, 'learn_throughput': 52.423, 'update_time_ms': 7.059},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.001, 'total_loss': 755.8994483947754, 'policy_loss': -0.0190161170612555, 'vf_loss': 755.9158668518066, 'vf_explained_var': 0.11948523, 'kl': 0.008698977311723866, 'entropy': 0.5603696927428246, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.738655462184874, 'ram_util_percent': 5.379831932773109} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.001, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=42394, ip=172.30.49.190)\u001b[0m 2021-05-19 15:45:23,027\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00001 reported episode_reward_max=200.0,episode_reward_min=9.0,episode_reward_mean=79.34,episode_len_mean=79.34,episode_media={},episodes_this_iter=28,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.1641206688959675, 'mean_inference_ms': 16.152426389769346, 'mean_action_processing_ms': 0.4699457413155163, 'mean_env_wait_ms': 1.1426707845975534, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 44833.109, 'sample_throughput': 89.22, 'learn_time_ms': 74172.134, 'learn_throughput': 53.929, 'update_time_ms': 64.285},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.001090909090909091, 'total_loss': 648.5875043869019, 'policy_loss': -0.01494635570270475, 'vf_loss': 648.5975389480591, 'vf_explained_var': 0.15898564, 'kl': 0.010908270196523517, 'entropy': 0.5605469923466444, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.778861788617886, 'ram_util_percent': 5.380487804878049} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.001090909090909091, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "<IPython.core.display.HTML object>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:45:39,989\tWARNING util.py:161 -- The `start_trial` operation took 0.751 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00002 reported episode_reward_max=200.0,episode_reward_min=9.0,episode_reward_mean=78.49,episode_len_mean=78.49,episode_media={},episodes_this_iter=26,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.04819643372076, 'mean_inference_ms': 16.075676549188593, 'mean_action_processing_ms': 0.5969556930671912, 'mean_env_wait_ms': 0.96988415087358, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 44836.267, 'sample_throughput': 89.213, 'learn_time_ms': 74837.82, 'learn_throughput': 53.449, 'update_time_ms': 122.031},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.0011818181818181819, 'total_loss': 611.6427593231201, 'policy_loss': -0.01514126022811979, 'vf_loss': 611.6536903381348, 'vf_explained_var': 0.1824764, 'kl': 0.00933830812573433, 'entropy': 0.5792771894484758, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.7759999999999994, 'ram_util_percent': 5.3808} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0011818181818181819, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:45:41,204\tWARNING util.py:161 -- The `start_trial` operation took 0.835 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00004 reported episode_reward_max=200.0,episode_reward_min=10.0,episode_reward_mean=81.28,episode_len_mean=81.28,episode_media={},episodes_this_iter=28,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.1134831427615723, 'mean_inference_ms': 14.549173849558496, 'mean_action_processing_ms': 0.6015314150508622, 'mean_env_wait_ms': 0.9005559283685522, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 44537.498, 'sample_throughput': 89.812, 'learn_time_ms': 80883.591, 'learn_throughput': 49.454, 'update_time_ms': 46.11},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.0013636363636363637, 'total_loss': 592.7615032196045, 'policy_loss': -0.0075556281371973455, 'vf_loss': 592.7660570144653, 'vf_explained_var': 0.22264326, 'kl': 0.006669162379694171, 'entropy': 0.5994750391691923, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.6823529411764704, 'ram_util_percent': 5.405042016806721} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0013636363636363637, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "<IPython.core.display.HTML object>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=42399, ip=172.30.49.190)\u001b[0m 2021-05-19 15:45:59,974\tINFO trainable.py:101 -- Trainable.setup took 38.544 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=42350, ip=172.30.49.190)\u001b[0m 2021-05-19 15:46:00,099\tINFO trainable.py:101 -- Trainable.setup took 50.379 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=42394, ip=172.30.49.190)\u001b[0m 2021-05-19 15:46:00,113\tINFO trainable.py:101 -- Trainable.setup took 37.087 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=38170)\u001b[0m 2021-05-19 15:46:03,517\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=38182)\u001b[0m 2021-05-19 15:46:03,960\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=38208)\u001b[0m 2021-05-19 15:46:04,720\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=38220)\u001b[0m 2021-05-19 15:46:14,182\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=38170)\u001b[0m 2021-05-19 15:46:43,814\tINFO trainable.py:101 -- Trainable.setup took 40.297 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=38182)\u001b[0m 2021-05-19 15:46:44,952\tINFO trainable.py:101 -- Trainable.setup took 40.993 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00013 reported episode_reward_max=60.0,episode_reward_min=8.0,episode_reward_mean=22.508474576271187,episode_len_mean=22.508474576271187,episode_media={},episodes_this_iter=177,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.9632635642047588, 'mean_inference_ms': 11.183544822291148, 'mean_action_processing_ms': 0.4634645151826821, 'mean_env_wait_ms': 0.9142962922635987, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 27982.865, 'sample_throughput': 142.945, 'learn_time_ms': 119828.295, 'learn_throughput': 33.381, 'update_time_ms': 15.719},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.002181818181818182, 'total_loss': 73.68086576461792, 'policy_loss': -0.053564514906611294, 'vf_loss': 73.7263754606247, 'vf_explained_var': 0.28593293, 'kl': 0.04027918295469135, 'entropy': 0.6530792359262705, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.6813559322033902, 'ram_util_percent': 5.339830508474575} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.002181818181818182, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00007 reported episode_reward_max=200.0,episode_reward_min=11.0,episode_reward_mean=63.0,episode_len_mean=63.0,episode_media={},episodes_this_iter=51,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.8005804406477708, 'mean_inference_ms': 14.915922397135269, 'mean_action_processing_ms': 0.6945614723592045, 'mean_env_wait_ms': 0.9946654762699483, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 38579.842, 'sample_throughput': 103.681, 'learn_time_ms': 64997.579, 'learn_throughput': 61.541, 'update_time_ms': 7.518},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0016363636363636363, 'total_loss': 426.8452615737915, 'policy_loss': -0.024745465634623542, 'vf_loss': 426.86483097076416, 'vf_explained_var': 0.23828317, 'kl': 0.01727581094019115, 'entropy': 0.5705048739910126, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.4326923076923077, 'ram_util_percent': 4.832692307692307} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0016363636363636363, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=38208)\u001b[0m 2021-05-19 15:46:58,264\tINFO trainable.py:101 -- Trainable.setup took 53.545 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=38220)\u001b[0m 2021-05-19 15:46:58,323\tINFO trainable.py:101 -- Trainable.setup took 44.142 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00011 reported episode_reward_max=200.0,episode_reward_min=10.0,episode_reward_mean=75.2,episode_len_mean=75.2,episode_media={},episodes_this_iter=32,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.1364696852554206, 'mean_inference_ms': 14.509816785615248, 'mean_action_processing_ms': 0.6338021818498327, 'mean_env_wait_ms': 0.8409370986219166, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 39490.775, 'sample_throughput': 101.289, 'learn_time_ms': 65491.506, 'learn_throughput': 61.077, 'update_time_ms': 36.561},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.002, 'total_loss': 537.199465751648, 'policy_loss': -0.017334639182081446, 'vf_loss': 537.2115268707275, 'vf_explained_var': 0.1692768, 'kl': 0.011727103468729183, 'entropy': 0.5605054441839457, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.4311320754716983, 'ram_util_percent': 4.826415094339621} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.002, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00009 reported episode_reward_max=200.0,episode_reward_min=9.0,episode_reward_mean=80.08,episode_len_mean=80.08,episode_media={},episodes_this_iter=26,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.1530308500205393, 'mean_inference_ms': 14.227890470437483, 'mean_action_processing_ms': 0.5388350457952304, 'mean_env_wait_ms': 0.8188853835783958, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 40288.842, 'sample_throughput': 99.283, 'learn_time_ms': 64900.347, 'learn_throughput': 61.633, 'update_time_ms': 8.352},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.0018181818181818182, 'total_loss': 573.8926067352295, 'policy_loss': -0.016413894685683772, 'vf_loss': 573.9048261642456, 'vf_explained_var': 0.2007482, 'kl': 0.009305363302701153, 'entropy': 0.5604791101068258, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.430188679245283, 'ram_util_percent': 4.8311320754716975} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0018181818181818182, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00012 reported episode_reward_max=82.0,episode_reward_min=9.0,episode_reward_mean=24.312883435582823,episode_len_mean=24.312883435582823,episode_media={},episodes_this_iter=163,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.813433937761185, 'mean_inference_ms': 11.703515491164021, 'mean_action_processing_ms': 0.3898207816764543, 'mean_env_wait_ms': 0.5326351581868604, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 35237.068, 'sample_throughput': 113.517, 'learn_time_ms': 124351.636, 'learn_throughput': 32.167, 'update_time_ms': 7.322},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.002090909090909091, 'total_loss': 104.5951714515686, 'policy_loss': -0.04675136361038312, 'vf_loss': 104.6344199180603, 'vf_explained_var': 0.23985618, 'kl': 0.03751678514527157, 'entropy': 0.6563236713409424, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.665625, 'ram_util_percent': 5.350781250000001} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.002090909090909091, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44587, ip=172.30.49.190)\u001b[0m 2021-05-19 15:47:30,458\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=44589, ip=172.30.49.190)\u001b[0m 2021-05-19 15:47:30,454\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=44590, ip=172.30.49.190)\u001b[0m 2021-05-19 15:47:30,886\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=44587, ip=172.30.49.190)\u001b[0m 2021-05-19 15:47:45,827\tINFO trainable.py:101 -- Trainable.setup took 15.370 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=44589, ip=172.30.49.190)\u001b[0m 2021-05-19 15:47:45,851\tINFO trainable.py:101 -- Trainable.setup took 15.397 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00016 reported episode_reward_max=79.0,episode_reward_min=8.0,episode_reward_mean=21.983425414364643,episode_len_mean=21.983425414364643,episode_media={},episodes_this_iter=181,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.8380613003123464, 'mean_inference_ms': 11.46914958947275, 'mean_action_processing_ms': 0.5164421369535072, 'mean_env_wait_ms': 0.5734599183234741, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 27881.379, 'sample_throughput': 143.465, 'learn_time_ms': 78216.459, 'learn_throughput': 51.14, 'update_time_ms': 165.533},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.002454545454545455, 'total_loss': 76.1794056892395, 'policy_loss': -0.05160876706941053, 'vf_loss': 76.22342014312744, 'vf_explained_var': 0.2851473, 'kl': 0.03797473944723606, 'entropy': 0.656042518094182, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.4316326530612247, 'ram_util_percent': 4.790816326530611} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.002454545454545455, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44590, ip=172.30.49.190)\u001b[0m 2021-05-19 15:47:46,610\tINFO trainable.py:101 -- Trainable.setup took 15.724 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00014 reported episode_reward_max=143.0,episode_reward_min=9.0,episode_reward_mean=22.206703910614525,episode_len_mean=22.206703910614525,episode_media={},episodes_this_iter=179,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.8353868955201016, 'mean_inference_ms': 11.240557688855786, 'mean_action_processing_ms': 0.4690879469850356, 'mean_env_wait_ms': 0.8192364086307798, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 27686.031, 'sample_throughput': 144.477, 'learn_time_ms': 78851.351, 'learn_throughput': 50.728, 'update_time_ms': 139.09},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0022727272727272726, 'total_loss': 86.1795494556427, 'policy_loss': -0.039461526306695305, 'vf_loss': 86.2123191356659, 'vf_explained_var': 0.24073923, 'kl': 0.033458296151366085, 'entropy': 0.6612515319138765, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.43265306122449, 'ram_util_percent': 4.792857142857142} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0022727272727272726, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00015 reported episode_reward_max=75.0,episode_reward_min=8.0,episode_reward_mean=21.7103825136612,episode_len_mean=21.7103825136612,episode_media={},episodes_this_iter=183,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.1273017495958733, 'mean_inference_ms': 11.04618078478971, 'mean_action_processing_ms': 0.4386652793499561, 'mean_env_wait_ms': 0.7958577007203973, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 27759.723, 'sample_throughput': 144.094, 'learn_time_ms': 79076.484, 'learn_throughput': 50.584, 'update_time_ms': 9.127},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0023636363636363638, 'total_loss': 80.50306236743927, 'policy_loss': -0.052805814222665504, 'vf_loss': 80.54808902740479, 'vf_explained_var': 0.25935197, 'kl': 0.03889514185721055, 'entropy': 0.6549413651227951, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.4346938775510207, 'ram_util_percent': 4.78673469387755} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0023636363636363638, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00017 reported episode_reward_max=85.0,episode_reward_min=9.0,episode_reward_mean=21.138297872340427,episode_len_mean=21.138297872340427,episode_media={},episodes_this_iter=188,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.1488713187570672, 'mean_inference_ms': 16.125988531580656, 'mean_action_processing_ms': 0.4473914673896592, 'mean_env_wait_ms': 0.8183459629098305, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 39125.523, 'sample_throughput': 102.235, 'learn_time_ms': 69161.332, 'learn_throughput': 57.836, 'update_time_ms': 7.68},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0025454545454545456, 'total_loss': 79.43197691440582, 'policy_loss': -0.05291351320920512, 'vf_loss': 79.47789525985718, 'vf_explained_var': 0.2809031, 'kl': 0.03497849917039275, 'entropy': 0.6588609907776117, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.4712871287128717, 'ram_util_percent': 5.399999999999998} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0025454545454545456, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00018 reported episode_reward_max=63.0,episode_reward_min=8.0,episode_reward_mean=21.69945355191257,episode_len_mean=21.69945355191257,episode_media={},episodes_this_iter=183,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.4492049514785885, 'mean_inference_ms': 15.70530208311657, 'mean_action_processing_ms': 0.25412375964025047, 'mean_env_wait_ms': 1.048317665464313, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 38934.788, 'sample_throughput': 102.736, 'learn_time_ms': 68607.989, 'learn_throughput': 58.302, 'update_time_ms': 7.513},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0026363636363636363, 'total_loss': 65.04666340351105, 'policy_loss': -0.06001238661701791, 'vf_loss': 65.09912383556366, 'vf_explained_var': 0.31107265, 'kl': 0.03776254545664415, 'entropy': 0.6565238814800978, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.4613861386138618, 'ram_util_percent': 5.4019801980198} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0026363636363636363, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00013 reported episode_reward_max=150.0,episode_reward_min=10.0,episode_reward_mean=44.46,episode_len_mean=44.46,episode_media={},episodes_this_iter=77,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.9921607405444974, 'mean_inference_ms': 12.826082689108743, 'mean_action_processing_ms': 0.5034073239690462, 'mean_env_wait_ms': 1.003357110083513, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 32685.5, 'sample_throughput': 122.378, 'learn_time_ms': 93531.309, 'learn_throughput': 42.766, 'update_time_ms': 11.272},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.002181818181818182, 'total_loss': 223.547221660614, 'policy_loss': -0.028214905178174376, 'vf_loss': 223.56875228881836, 'vf_explained_var': 0.26087195, 'kl': 0.02229116123635322, 'entropy': 0.5940805450081825, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.403030303030303, 'ram_util_percent': 5.4030303030303015} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.002181818181818182, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00020 reported episode_reward_max=74.0,episode_reward_min=9.0,episode_reward_mean=24.181818181818183,episode_len_mean=24.181818181818183,episode_media={},episodes_this_iter=165,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.218040915986838, 'mean_inference_ms': 15.534275267321425, 'mean_action_processing_ms': 0.4702244631325177, 'mean_env_wait_ms': 0.7793373683410498, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 38264.094, 'sample_throughput': 104.537, 'learn_time_ms': 67500.361, 'learn_throughput': 59.259, 'update_time_ms': 10.192},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0028181818181818186, 'total_loss': 96.39040327072144, 'policy_loss': -0.04639812355162576, 'vf_loss': 96.43008959293365, 'vf_explained_var': 0.23637563, 'kl': 0.03356692276429385, 'entropy': 0.6605852134525776, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.4040404040404044, 'ram_util_percent': 5.405050505050504} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0028181818181818186, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00019 reported episode_reward_max=59.0,episode_reward_min=8.0,episode_reward_mean=20.215384615384615,episode_len_mean=20.215384615384615,episode_media={},episodes_this_iter=195,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.1447575092957247, 'mean_inference_ms': 15.801449915388147, 'mean_action_processing_ms': 0.6017026466318107, 'mean_env_wait_ms': 0.8308249144730785, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 38162.908, 'sample_throughput': 104.814, 'learn_time_ms': 67697.435, 'learn_throughput': 59.086, 'update_time_ms': 8.11},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0027272727272727275, 'total_loss': 49.47584939002991, 'policy_loss': -0.05246835434809327, 'vf_loss': 49.520262122154236, 'vf_explained_var': 0.34925917, 'kl': 0.04027844138909131, 'entropy': 0.6535354815423489, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.4040404040404044, 'ram_util_percent': 5.3999999999999995} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0027272727272727275, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00012 reported episode_reward_max=132.0,episode_reward_min=9.0,episode_reward_mean=45.01,episode_len_mean=45.01,episode_media={},episodes_this_iter=77,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.8628409692421772, 'mean_inference_ms': 12.979746285763563, 'mean_action_processing_ms': 0.3830467249787152, 'mean_env_wait_ms': 0.5971041355975751, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 34894.09, 'sample_throughput': 114.633, 'learn_time_ms': 97544.375, 'learn_throughput': 41.007, 'update_time_ms': 7.1},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.002090909090909091, 'total_loss': 300.878538608551, 'policy_loss': -0.0359166381531395, 'vf_loss': 300.9083905220032, 'vf_explained_var': 0.21165983, 'kl': 0.020198966754833236, 'entropy': 0.5900556780397892, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4000000000000004, 'ram_util_percent': 5.4079999999999995} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.002090909090909091, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00016 reported episode_reward_max=126.0,episode_reward_min=10.0,episode_reward_mean=44.99,episode_len_mean=44.99,episode_media={},episodes_this_iter=81,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.8347557530976879, 'mean_inference_ms': 12.762829531243973, 'mean_action_processing_ms': 0.5211261252155789, 'mean_env_wait_ms': 0.7029137524445307, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 31285.783, 'sample_throughput': 127.854, 'learn_time_ms': 66320.858, 'learn_throughput': 60.313, 'update_time_ms': 86.247},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.002454545454545455, 'total_loss': 238.89999341964722, 'policy_loss': -0.025607558025512844, 'vf_loss': 238.9194393157959, 'vf_explained_var': 0.25209117, 'kl': 0.020554798189550638, 'entropy': 0.6007638331502676, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4000000000000006, 'ram_util_percent': 4.899999999999998} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.002454545454545455, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00014 reported episode_reward_max=200.0,episode_reward_min=9.0,episode_reward_mean=47.49,episode_len_mean=47.49,episode_media={},episodes_this_iter=64,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.8418873476981682, 'mean_inference_ms': 12.293019634861325, 'mean_action_processing_ms': 0.5375213586153046, 'mean_env_wait_ms': 0.8342729855624691, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 31120.595, 'sample_throughput': 128.532, 'learn_time_ms': 66652.035, 'learn_throughput': 60.013, 'update_time_ms': 75.235},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0022727272727272726, 'total_loss': 447.2034749984741, 'policy_loss': -0.025924970206688158, 'vf_loss': 447.22365283966064, 'vf_explained_var': 0.1594699, 'kl': 0.01915450330125168, 'entropy': 0.6014819350093603, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4000000000000006, 'ram_util_percent': 4.899999999999998} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0022727272727272726, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00015 reported episode_reward_max=120.0,episode_reward_min=10.0,episode_reward_mean=44.57,episode_len_mean=44.57,episode_media={},episodes_this_iter=76,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.126142074753212, 'mean_inference_ms': 12.368060476775602, 'mean_action_processing_ms': 0.4479032128707654, 'mean_env_wait_ms': 0.7684213228165959, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 31125.915, 'sample_throughput': 128.51, 'learn_time_ms': 66847.768, 'learn_throughput': 59.837, 'update_time_ms': 43.944},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0023636363636363638, 'total_loss': 257.93257236480713, 'policy_loss': -0.0277033397287596, 'vf_loss': 257.95423555374146, 'vf_explained_var': 0.20243913, 'kl': 0.020117425330681726, 'entropy': 0.6116597875952721, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4000000000000006, 'ram_util_percent': 4.899999999999998} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0023636363636363638, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00023 reported episode_reward_max=80.0,episode_reward_min=8.0,episode_reward_mean=22.53142857142857,episode_len_mean=22.53142857142857,episode_media={},episodes_this_iter=175,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.0103586500152462, 'mean_inference_ms': 14.992494746535204, 'mean_action_processing_ms': 0.5862433069343099, 'mean_env_wait_ms': 0.7001018054379615, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 35944.533, 'sample_throughput': 111.283, 'learn_time_ms': 54561.154, 'learn_throughput': 73.312, 'update_time_ms': 6.613},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.003090909090909091, 'total_loss': 90.440300822258, 'policy_loss': -0.04557178480899893, 'vf_loss': 90.47839081287384, 'vf_explained_var': 0.27355713, 'kl': 0.037398569053038955, 'entropy': 0.6564173270016909, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.4011363636363638, 'ram_util_percent': 4.8999999999999995} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003090909090909091, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00022 reported episode_reward_max=63.0,episode_reward_min=8.0,episode_reward_mean=20.852631578947367,episode_len_mean=20.852631578947367,episode_media={},episodes_this_iter=190,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.9585343667955966, 'mean_inference_ms': 15.006936369371454, 'mean_action_processing_ms': 0.5710798190454591, 'mean_env_wait_ms': 0.6597640892580058, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 36091.783, 'sample_throughput': 110.829, 'learn_time_ms': 54561.408, 'learn_throughput': 73.312, 'update_time_ms': 6.688},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.003, 'total_loss': 61.21295237541199, 'policy_loss': -0.05059257405810058, 'vf_loss': 61.25597155094147, 'vf_explained_var': 0.3156705, 'kl': 0.037867355335038155, 'entropy': 0.6564975827932358, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.4011363636363638, 'ram_util_percent': 4.8999999999999995} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00021 reported episode_reward_max=105.0,episode_reward_min=9.0,episode_reward_mean=22.245714285714286,episode_len_mean=22.245714285714286,episode_media={},episodes_this_iter=175,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.9528706638360419, 'mean_inference_ms': 14.855654090036545, 'mean_action_processing_ms': 0.5509460468555047, 'mean_env_wait_ms': 0.923788285556711, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 35780.313, 'sample_throughput': 111.793, 'learn_time_ms': 54513.149, 'learn_throughput': 73.377, 'update_time_ms': 7.139},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0029090909090909093, 'total_loss': 104.23352336883545, 'policy_loss': -0.053940544894430786, 'vf_loss': 104.27919411659241, 'vf_explained_var': 0.21309082, 'kl': 0.04136172391008586, 'entropy': 0.6527742091566324, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.4011363636363638, 'ram_util_percent': 4.8999999999999995} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0029090909090909093, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The actor or task with ID ffffffffffffffff7896310dcc02927f4d851ca101000000 cannot be scheduled right now. It requires {CPU_group_0_db9c63726e369ba8ebf20eed9a99f77f: 1.000000}, {CPU_group_db9c63726e369ba8ebf20eed9a99f77f: 1.000000} for placement, but this node only has remaining {0.000000/18.000000 CPU, 87.135904 GiB/87.135904 GiB memory, 37.343959 GiB/37.343959 GiB object_store_memory, 1.000000/1.000000 CPU_group_0_db9c63726e369ba8ebf20eed9a99f77f, 1.000000/1.000000 CPU_group_1_e230dffb3fd10e5648088dd8af6d9169, 1.000000/1.000000 CPU_group_1_db9c63726e369ba8ebf20eed9a99f77f, 1.000000/1.000000 CPU_group_1_19401f96d5f51f491cfdeacd17a53e34, 1.000000/1.000000 CPU_group_1_3385cc0525dfcbbd4059316246676e35, 0.000000/1.000000 CPU_group_0_5763e5534f3359a5da9b9be1b5b983e2, 1.000000/1.000000 node:10.128.0.18, 0.000000/3.000000 CPU_group_3385cc0525dfcbbd4059316246676e35, 1.000000/1.000000 CPU_group_2_3385cc0525dfcbbd4059316246676e35, 1.000000/1.000000 CPU_group_2_5763e5534f3359a5da9b9be1b5b983e2, 1.000000/1.000000 CPU_group_2_19401f96d5f51f491cfdeacd17a53e34, 1.000000/1.000000 CPU_group_1_04424e424ef3127dac20feda22e6c560, 0.000000/1.000000 CPU_group_0_19401f96d5f51f491cfdeacd17a53e34, 0.000000/3.000000 CPU_group_e230dffb3fd10e5648088dd8af6d9169, 1.000000/1.000000 CPU_group_1_5763e5534f3359a5da9b9be1b5b983e2, 0.000000/1.000000 CPU_group_0_04424e424ef3127dac20feda22e6c560, 3.000000/3.000000 CPU_group_db9c63726e369ba8ebf20eed9a99f77f, 1.000000/1.000000 CPU_group_2_db9c63726e369ba8ebf20eed9a99f77f, 0.000000/3.000000 CPU_group_19401f96d5f51f491cfdeacd17a53e34, 0.000000/3.000000 CPU_group_04424e424ef3127dac20feda22e6c560, 1.000000/1.000000 CPU_group_2_04424e424ef3127dac20feda22e6c560, 0.000000/1.000000 CPU_group_0_e230dffb3fd10e5648088dd8af6d9169, 0.000000/3.000000 CPU_group_5763e5534f3359a5da9b9be1b5b983e2, 0.000000/1.000000 CPU_group_0_3385cc0525dfcbbd4059316246676e35, 1.000000/1.000000 CPU_group_2_e230dffb3fd10e5648088dd8af6d9169}\n",
      ". In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "\u001b[2m\u001b[36m(pid=46550, ip=172.30.49.190)\u001b[0m 2021-05-19 15:50:05,562\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00017 reported episode_reward_max=150.0,episode_reward_min=9.0,episode_reward_mean=45.65,episode_len_mean=45.65,episode_media={},episodes_this_iter=72,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.2177673488872793, 'mean_inference_ms': 15.32141209961086, 'mean_action_processing_ms': 0.5161836121807452, 'mean_env_wait_ms': 0.7822266649474323, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 36993.802, 'sample_throughput': 108.126, 'learn_time_ms': 69470.783, 'learn_throughput': 57.578, 'update_time_ms': 12.705},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0025454545454545456, 'total_loss': 285.4837260246277, 'policy_loss': -0.04914408281183569, 'vf_loss': 285.5256633758545, 'vf_explained_var': 0.2311878, 'kl': 0.02403555141063407, 'entropy': 0.6000388935208321, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4000000000000001, 'ram_util_percent': 5.482291666666666} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0025454545454545456, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00018 reported episode_reward_max=119.0,episode_reward_min=10.0,episode_reward_mean=41.85,episode_len_mean=41.85,episode_media={},episodes_this_iter=92,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.2734013791334298, 'mean_inference_ms': 14.901860469861925, 'mean_action_processing_ms': 0.4509978649623344, 'mean_env_wait_ms': 1.068141912338292, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 36981.947, 'sample_throughput': 108.161, 'learn_time_ms': 68857.679, 'learn_throughput': 58.091, 'update_time_ms': 7.912},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0026363636363636363, 'total_loss': 180.72106885910034, 'policy_loss': -0.0341367868531961, 'vf_loss': 180.7476077079773, 'vf_explained_var': 0.26662675, 'kl': 0.02532757236622274, 'entropy': 0.5990725196897984, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4000000000000001, 'ram_util_percent': 5.488659793814433} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0026363636363636363, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=46550, ip=172.30.49.190)\u001b[0m 2021-05-19 15:50:20,639\tINFO trainable.py:101 -- Trainable.setup took 15.087 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00013 reported episode_reward_max=200.0,episode_reward_min=13.0,episode_reward_mean=68.02,episode_len_mean=68.02,episode_media={},episodes_this_iter=45,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.0556124640578621, 'mean_inference_ms': 13.550194890419617, 'mean_action_processing_ms': 0.531017184540437, 'mean_env_wait_ms': 0.9920800080262129, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 33760.098, 'sample_throughput': 118.483, 'learn_time_ms': 85285.316, 'learn_throughput': 46.901, 'update_time_ms': 59.406},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.002181818181818182, 'total_loss': 425.2617349624634, 'policy_loss': -0.01900720107369125, 'vf_loss': 425.27446460723877, 'vf_explained_var': 0.32435513, 'kl': 0.013965468795504421, 'entropy': 0.569357743486762, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.4000000000000001, 'ram_util_percent': 5.483333333333334} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.002181818181818182, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00020 reported episode_reward_max=132.0,episode_reward_min=11.0,episode_reward_mean=37.490566037735846,episode_len_mean=37.490566037735846,episode_media={},episodes_this_iter=106,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.0012431943509301, 'mean_inference_ms': 15.456093095592687, 'mean_action_processing_ms': 0.46643227899412365, 'mean_env_wait_ms': 0.8216972039138756, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 37205.683, 'sample_throughput': 107.51, 'learn_time_ms': 71327.014, 'learn_throughput': 56.08, 'update_time_ms': 6.503},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0028181818181818186, 'total_loss': 222.42901134490967, 'policy_loss': -0.04210337554104626, 'vf_loss': 222.4625654220581, 'vf_explained_var': 0.25367767, 'kl': 0.02850045618833974, 'entropy': 0.6101967766880989, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4505050505050507, 'ram_util_percent': 5.48989898989899} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0028181818181818186, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00019 reported episode_reward_max=143.0,episode_reward_min=10.0,episode_reward_mean=42.99,episode_len_mean=42.99,episode_media={},episodes_this_iter=84,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.0784957068960856, 'mean_inference_ms': 15.499188394231203, 'mean_action_processing_ms': 0.5980235294520102, 'mean_env_wait_ms': 0.8614910541885296, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 37000.032, 'sample_throughput': 108.108, 'learn_time_ms': 71435.384, 'learn_throughput': 55.995, 'update_time_ms': 5.308},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0027272727272727275, 'total_loss': 240.4338779449463, 'policy_loss': -0.027661367668770254, 'vf_loss': 240.45540809631348, 'vf_explained_var': 0.2670818, 'kl': 0.020410632190760225, 'entropy': 0.6027704086154699, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.454, 'ram_util_percent': 5.474} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0027272727272727275, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00012 reported episode_reward_max=200.0,episode_reward_min=10.0,episode_reward_mean=74.71,episode_len_mean=74.71,episode_media={},episodes_this_iter=32,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.8460285509455762, 'mean_inference_ms': 13.391117991770871, 'mean_action_processing_ms': 0.4223844451350328, 'mean_env_wait_ms': 0.6731296024159641, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 34457.145, 'sample_throughput': 116.086, 'learn_time_ms': 95472.545, 'learn_throughput': 41.897, 'update_time_ms': 143.824},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.002090909090909091, 'total_loss': 590.6911172866821, 'policy_loss': -0.00921423442196101, 'vf_loss': 590.6955986022949, 'vf_explained_var': 0.1582931, 'kl': 0.010542980235186405, 'entropy': 0.5906539503484964, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.4971698113207546, 'ram_util_percent': 5.522641509433962} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.002090909090909091, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "<IPython.core.display.HTML object>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:50:58,086\tWARNING util.py:161 -- The `start_trial` operation took 0.832 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00016 reported episode_reward_max=200.0,episode_reward_min=10.0,episode_reward_mean=71.43,episode_len_mean=71.43,episode_media={},episodes_this_iter=37,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.8738996674257745, 'mean_inference_ms': 14.26839374928858, 'mean_action_processing_ms': 0.5562291147165326, 'mean_env_wait_ms': 0.7470185745003424, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 38363.524, 'sample_throughput': 104.266, 'learn_time_ms': 64260.31, 'learn_throughput': 62.247, 'update_time_ms': 59.817},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.002454545454545455, 'total_loss': 480.5640296936035, 'policy_loss': -0.01588725796318613, 'vf_loss': 480.5752582550049, 'vf_explained_var': 0.2330685, 'kl': 0.010358986124629155, 'entropy': 0.5840784441679716, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.4244897959183676, 'ram_util_percent': 4.925510204081632} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.002454545454545455, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00022 reported episode_reward_max=190.0,episode_reward_min=9.0,episode_reward_mean=46.76,episode_len_mean=46.76,episode_media={},episodes_this_iter=72,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.1610239583047655, 'mean_inference_ms': 17.38655817068637, 'mean_action_processing_ms': 0.6556867741642176, 'mean_env_wait_ms': 0.8477399277467009, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 44586.954, 'sample_throughput': 89.712, 'learn_time_ms': 56946.272, 'learn_throughput': 70.242, 'update_time_ms': 4.712},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.003, 'total_loss': 341.4821834564209, 'policy_loss': -0.019558761385269463, 'vf_loss': 341.4957857131958, 'vf_explained_var': 0.1856257, 'kl': 0.019837457191897556, 'entropy': 0.5864334553480148, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4234693877551023, 'ram_util_percent': 4.920408163265305} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00015 reported episode_reward_max=200.0,episode_reward_min=11.0,episode_reward_mean=70.5,episode_len_mean=70.5,episode_media={},episodes_this_iter=42,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.0792601171587963, 'mean_inference_ms': 14.248115811981807, 'mean_action_processing_ms': 0.4707944514188898, 'mean_env_wait_ms': 0.8415859225557207, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 38337.796, 'sample_throughput': 104.336, 'learn_time_ms': 64481.62, 'learn_throughput': 62.033, 'update_time_ms': 31.798},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.0023636363636363638, 'total_loss': 516.7899217605591, 'policy_loss': -0.023387634370010346, 'vf_loss': 516.80797290802, 'vf_explained_var': 0.24316347, 'kl': 0.011866917309816927, 'entropy': 0.5804141536355019, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.4247422680412374, 'ram_util_percent': 4.9247422680412365} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0023636363636363638, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00023 reported episode_reward_max=167.0,episode_reward_min=9.0,episode_reward_mean=43.88,episode_len_mean=43.88,episode_media={},episodes_this_iter=84,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.1722837941171107, 'mean_inference_ms': 18.118728125554156, 'mean_action_processing_ms': 0.6099764864835201, 'mean_env_wait_ms': 1.0224681796087018, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 44552.112, 'sample_throughput': 89.783, 'learn_time_ms': 57045.44, 'learn_throughput': 70.12, 'update_time_ms': 4.635},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.003090909090909091, 'total_loss': 346.95421838760376, 'policy_loss': -0.037541382742347196, 'vf_loss': 346.98533964157104, 'vf_explained_var': 0.18315539, 'kl': 0.02139044858631678, 'entropy': 0.5928313788026571, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4224489795918371, 'ram_util_percent': 4.926530612244897} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003090909090909091, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00021 reported episode_reward_max=173.0,episode_reward_min=10.0,episode_reward_mean=46.43,episode_len_mean=46.43,episode_media={},episodes_this_iter=70,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.1729761468587343, 'mean_inference_ms': 17.211020956557192, 'mean_action_processing_ms': 0.6984897661275096, 'mean_env_wait_ms': 0.9845703377508295, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 44485.209, 'sample_throughput': 89.918, 'learn_time_ms': 56781.072, 'learn_throughput': 70.446, 'update_time_ms': 71.634},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0029090909090909093, 'total_loss': 379.66149520874023, 'policy_loss': -0.028334889211691916, 'vf_loss': 379.6849145889282, 'vf_explained_var': 0.18854766, 'kl': 0.016395450205891393, 'entropy': 0.5919813215732574, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4247422680412374, 'ram_util_percent': 4.920618556701029} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0029090909090909093, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=42101)\u001b[0m 2021-05-19 15:51:14,360\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=42155)\u001b[0m 2021-05-19 15:51:16,951\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "The actor or task with ID ffffffffffffffff6efb8d5ff6bed703b34696b201000000 cannot be scheduled right now. It requires {CPU_group_0_04424e424ef3127dac20feda22e6c560: 1.000000}, {CPU_group_04424e424ef3127dac20feda22e6c560: 1.000000} for placement, but this node only has remaining {0.000000/18.000000 CPU, 87.135904 GiB/87.135904 GiB memory, 37.343959 GiB/37.343959 GiB object_store_memory, 0.000000/1.000000 CPU_group_0_db9c63726e369ba8ebf20eed9a99f77f, 1.000000/1.000000 CPU_group_1_e230dffb3fd10e5648088dd8af6d9169, 1.000000/1.000000 CPU_group_1_db9c63726e369ba8ebf20eed9a99f77f, 1.000000/1.000000 CPU_group_1_19401f96d5f51f491cfdeacd17a53e34, 1.000000/1.000000 CPU_group_1_3385cc0525dfcbbd4059316246676e35, 0.000000/1.000000 CPU_group_0_5763e5534f3359a5da9b9be1b5b983e2, 1.000000/1.000000 node:10.128.0.18, 0.000000/3.000000 CPU_group_3385cc0525dfcbbd4059316246676e35, 1.000000/1.000000 CPU_group_2_3385cc0525dfcbbd4059316246676e35, 1.000000/1.000000 CPU_group_2_5763e5534f3359a5da9b9be1b5b983e2, 1.000000/1.000000 CPU_group_2_19401f96d5f51f491cfdeacd17a53e34, 1.000000/1.000000 CPU_group_1_04424e424ef3127dac20feda22e6c560, 0.000000/1.000000 CPU_group_0_19401f96d5f51f491cfdeacd17a53e34, 3.000000/3.000000 CPU_group_e230dffb3fd10e5648088dd8af6d9169, 1.000000/1.000000 CPU_group_1_5763e5534f3359a5da9b9be1b5b983e2, 1.000000/1.000000 CPU_group_0_04424e424ef3127dac20feda22e6c560, 0.000000/3.000000 CPU_group_db9c63726e369ba8ebf20eed9a99f77f, 1.000000/1.000000 CPU_group_2_db9c63726e369ba8ebf20eed9a99f77f, 0.000000/3.000000 CPU_group_19401f96d5f51f491cfdeacd17a53e34, 3.000000/3.000000 CPU_group_04424e424ef3127dac20feda22e6c560, 1.000000/1.000000 CPU_group_2_04424e424ef3127dac20feda22e6c560, 1.000000/1.000000 CPU_group_0_e230dffb3fd10e5648088dd8af6d9169, 0.000000/3.000000 CPU_group_5763e5534f3359a5da9b9be1b5b983e2, 0.000000/1.000000 CPU_group_0_3385cc0525dfcbbd4059316246676e35, 1.000000/1.000000 CPU_group_2_e230dffb3fd10e5648088dd8af6d9169}\n",
      ". In total there are 0 pending tasks and 2 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "\u001b[2m\u001b[36m(pid=48126, ip=172.30.49.190)\u001b[0m 2021-05-19 15:51:52,284\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=48129, ip=172.30.49.190)\u001b[0m 2021-05-19 15:51:52,304\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=42155)\u001b[0m 2021-05-19 15:52:05,795\tINFO trainable.py:101 -- Trainable.setup took 48.845 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=42101)\u001b[0m 2021-05-19 15:52:05,973\tINFO trainable.py:101 -- Trainable.setup took 51.614 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=48129, ip=172.30.49.190)\u001b[0m 2021-05-19 15:52:08,111\tINFO trainable.py:101 -- Trainable.setup took 15.815 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=48126, ip=172.30.49.190)\u001b[0m 2021-05-19 15:52:08,129\tINFO trainable.py:101 -- Trainable.setup took 15.846 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00024 reported episode_reward_max=88.0,episode_reward_min=9.0,episode_reward_mean=22.93103448275862,episode_len_mean=22.93103448275862,episode_media={},episodes_this_iter=174,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.7941089267298268, 'mean_inference_ms': 8.466176791343587, 'mean_action_processing_ms': 0.3267546334223631, 'mean_env_wait_ms': 0.6908704526107212, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 22038.653, 'sample_throughput': 181.499, 'learn_time_ms': 89071.052, 'learn_throughput': 44.908, 'update_time_ms': 6.932},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.003181818181818182, 'total_loss': 111.57347440719604, 'policy_loss': -0.04601599264424294, 'vf_loss': 111.6129789352417, 'vf_explained_var': 0.2129432, 'kl': 0.03257223538821563, 'entropy': 0.661258677020669, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.4212121212121214, 'ram_util_percent': 4.844444444444443} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003181818181818182, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00017 reported episode_reward_max=193.0,episode_reward_min=10.0,episode_reward_mean=72.44,episode_len_mean=72.44,episode_media={},episodes_this_iter=38,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.353187222705007, 'mean_inference_ms': 15.972791185144166, 'mean_action_processing_ms': 0.5836772753776294, 'mean_env_wait_ms': 0.8351771604091618, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 42610.611, 'sample_throughput': 93.873, 'learn_time_ms': 80857.047, 'learn_throughput': 49.47, 'update_time_ms': 10.994},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.0025454545454545456, 'total_loss': 499.74351024627686, 'policy_loss': -0.026685143704526126, 'vf_loss': 499.76412868499756, 'vf_explained_var': 0.27813005, 'kl': 0.013465006777551025, 'entropy': 0.5881016980856657, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.5303278688524586, 'ram_util_percent': 5.483606557377049} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0025454545454545456, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00018 reported episode_reward_max=200.0,episode_reward_min=10.0,episode_reward_mean=63.92,episode_len_mean=63.92,episode_media={},episodes_this_iter=42,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.29668371613444, 'mean_inference_ms': 15.941068089284126, 'mean_action_processing_ms': 0.5203450296672346, 'mean_env_wait_ms': 1.0000200499795358, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 42620.006, 'sample_throughput': 93.853, 'learn_time_ms': 80392.75, 'learn_throughput': 49.756, 'update_time_ms': 21.164},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.0026363636363636363, 'total_loss': 380.0423917770386, 'policy_loss': -0.016764473461080343, 'vf_loss': 380.05347537994385, 'vf_explained_var': 0.33301815, 'kl': 0.012642430316191167, 'entropy': 0.5755793061107397, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.5303278688524586, 'ram_util_percent': 5.48688524590164} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0026363636363636363, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00022 reported episode_reward_max=200.0,episode_reward_min=13.0,episode_reward_mean=74.44,episode_len_mean=74.44,episode_media={},episodes_this_iter=39,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.2640449843495163, 'mean_inference_ms': 18.59147486907273, 'mean_action_processing_ms': 0.7307635103475119, 'mean_env_wait_ms': 0.8836315924011182, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 45639.392, 'sample_throughput': 87.644, 'learn_time_ms': 60246.845, 'learn_throughput': 66.394, 'update_time_ms': 5.376},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.003, 'total_loss': 477.9809799194336, 'policy_loss': -0.010643659625202417, 'vf_loss': 477.9878225326538, 'vf_explained_var': 0.20215732, 'kl': 0.012665145652135834, 'entropy': 0.5685984473675489, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.420588235294118, 'ram_util_percent': 4.8401960784313705} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "<IPython.core.display.HTML object>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:53:05,103\tWARNING util.py:161 -- The `start_trial` operation took 0.738 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00023 reported episode_reward_max=200.0,episode_reward_min=9.0,episode_reward_mean=71.38,episode_len_mean=71.38,episode_media={},episodes_this_iter=35,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.2360563928631507, 'mean_inference_ms': 18.805885149999845, 'mean_action_processing_ms': 0.6094279034790532, 'mean_env_wait_ms': 1.127278288602751, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 45499.183, 'sample_throughput': 87.914, 'learn_time_ms': 60316.586, 'learn_throughput': 66.317, 'update_time_ms': 5.332},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.003090909090909091, 'total_loss': 538.5059070587158, 'policy_loss': -0.024122198665281758, 'vf_loss': 538.5241937637329, 'vf_explained_var': 0.27474448, 'kl': 0.01293537013407331, 'entropy': 0.5558496378362179, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.4220000000000004, 'ram_util_percent': 4.842999999999998} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003090909090909091, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00021 reported episode_reward_max=200.0,episode_reward_min=12.0,episode_reward_mean=76.9,episode_len_mean=76.9,episode_media={},episodes_this_iter=34,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.2477863899918165, 'mean_inference_ms': 18.498194951237437, 'mean_action_processing_ms': 0.6957882758904023, 'mean_env_wait_ms': 1.0427881736869506, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 45523.074, 'sample_throughput': 87.868, 'learn_time_ms': 60012.729, 'learn_throughput': 66.653, 'update_time_ms': 77.464},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0029090909090909093, 'total_loss': 607.2853374481201, 'policy_loss': -0.02065764638246037, 'vf_loss': 607.3022794723511, 'vf_explained_var': 0.2307469, 'kl': 0.01235607179114595, 'entropy': 0.5557048749178648, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.4207920792079212, 'ram_util_percent': 4.844554455445542} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0029090909090909093, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:53:07,520\tWARNING util.py:161 -- The `start_trial` operation took 0.713 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:53:08,760\tWARNING util.py:161 -- The `start_trial` operation took 1.190 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00020 reported episode_reward_max=200.0,episode_reward_min=11.0,episode_reward_mean=60.48,episode_len_mean=60.48,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.1717889917478794, 'mean_inference_ms': 16.480343771427474, 'mean_action_processing_ms': 0.5196613648401729, 'mean_env_wait_ms': 0.9751698599576767, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 44812.683, 'sample_throughput': 89.26, 'learn_time_ms': 84499.491, 'learn_throughput': 47.338, 'update_time_ms': 5.12},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.0028181818181818186, 'total_loss': 492.4771318435669, 'policy_loss': -0.023834797670133412, 'vf_loss': 492.4948720932007, 'vf_explained_var': 0.21329004, 'kl': 0.013562087115133181, 'entropy': 0.5725928191095591, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.5007812499999997, 'ram_util_percent': 5.51484375} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0028181818181818186, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00019 reported episode_reward_max=200.0,episode_reward_min=10.0,episode_reward_mean=64.29,episode_len_mean=64.29,episode_media={},episodes_this_iter=42,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.1095772199475418, 'mean_inference_ms': 16.978832525085227, 'mean_action_processing_ms': 0.6147566751528523, 'mean_env_wait_ms': 0.8575917791120647, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 44681.175, 'sample_throughput': 89.523, 'learn_time_ms': 84257.027, 'learn_throughput': 47.474, 'update_time_ms': 4.333},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.0027272727272727275, 'total_loss': 391.76768589019775, 'policy_loss': -0.024791323114186525, 'vf_loss': 391.78649044036865, 'vf_explained_var': 0.35030168, 'kl': 0.013295734373969026, 'entropy': 0.5849772449582815, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.5015625, 'ram_util_percent': 5.515625} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0027272727272727275, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:53:28,475\tWARNING util.py:161 -- The `start_trial` operation took 0.519 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=49724, ip=172.30.49.190)\u001b[0m 2021-05-19 15:53:40,530\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=49728, ip=172.30.49.190)\u001b[0m 2021-05-19 15:53:40,528\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=49732, ip=172.30.49.190)\u001b[0m 2021-05-19 15:53:40,533\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00027 reported episode_reward_max=88.0,episode_reward_min=9.0,episode_reward_mean=22.46067415730337,episode_len_mean=22.46067415730337,episode_media={},episodes_this_iter=178,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.0571738479426447, 'mean_inference_ms': 11.456385940891183, 'mean_action_processing_ms': 0.32481040717575627, 'mean_env_wait_ms': 0.5735205020517585, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 27808.322, 'sample_throughput': 143.842, 'learn_time_ms': 79644.939, 'learn_throughput': 50.223, 'update_time_ms': 10.857},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.003454545454545455, 'total_loss': 68.31681209802628, 'policy_loss': -0.03905672318069264, 'vf_loss': 68.3486048579216, 'vf_explained_var': 0.29225984, 'kl': 0.03631708910688758, 'entropy': 0.6573957093060017, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.530927835051546, 'ram_util_percent': 4.793814432989691} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003454545454545455, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=49724, ip=172.30.49.190)\u001b[0m 2021-05-19 15:53:55,992\tINFO trainable.py:101 -- Trainable.setup took 15.465 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=49728, ip=172.30.49.190)\u001b[0m 2021-05-19 15:53:56,007\tINFO trainable.py:101 -- Trainable.setup took 15.479 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=49732, ip=172.30.49.190)\u001b[0m 2021-05-19 15:53:56,029\tINFO trainable.py:101 -- Trainable.setup took 15.502 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:53:56,752\tWARNING util.py:161 -- The `callbacks.on_trial_result` operation took 0.970 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:53:56,754\tWARNING util.py:161 -- The `process_trial_result` operation took 0.973 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:53:56,754\tWARNING util.py:161 -- Processing trial results took 0.973 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m 2021-05-19 15:53:56,754\tWARNING util.py:161 -- The `process_trial` operation took 0.973 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44271)\u001b[0m 2021-05-19 15:53:59,166\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00024 reported episode_reward_max=200.0,episode_reward_min=9.0,episode_reward_mean=45.74,episode_len_mean=45.74,episode_media={},episodes_this_iter=73,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.795956258336856, 'mean_inference_ms': 9.260254576711493, 'mean_action_processing_ms': 0.39913369925272646, 'mean_env_wait_ms': 0.5937762466110088, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 24143.137, 'sample_throughput': 165.679, 'learn_time_ms': 84762.591, 'learn_throughput': 47.191, 'update_time_ms': 64.325},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.003181818181818182, 'total_loss': 267.96294689178467, 'policy_loss': -0.030848020658595487, 'vf_loss': 267.98737621307373, 'vf_explained_var': 0.2308121, 'kl': 0.021403095743153244, 'entropy': 0.6162757314741611, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.529473684210527, 'ram_util_percent': 4.7936842105263135} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003181818181818182, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00028 reported episode_reward_max=77.0,episode_reward_min=9.0,episode_reward_mean=21.342245989304814,episode_len_mean=21.342245989304814,episode_media={},episodes_this_iter=187,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.0210482024339071, 'mean_inference_ms': 11.30031614141469, 'mean_action_processing_ms': 0.37054965457368244, 'mean_env_wait_ms': 0.6579073930837557, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 27897.311, 'sample_throughput': 143.383, 'learn_time_ms': 79658.268, 'learn_throughput': 50.214, 'update_time_ms': 136.534},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0035454545454545456, 'total_loss': 68.393603682518, 'policy_loss': -0.053716327383881435, 'vf_loss': 68.43982136249542, 'vf_explained_var': 0.29103565, 'kl': 0.03749127470655367, 'entropy': 0.6568989455699921, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.5333333333333332, 'ram_util_percent': 4.795833333333333} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0035454545454545456, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44323)\u001b[0m 2021-05-19 15:54:01,369\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=44882)\u001b[0m 2021-05-19 15:54:11,663\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "The actor or task with ID ffffffffffffffffb9bdc127ee32267d0cf6cbe701000000 cannot be scheduled right now. It requires {CPU_group_db9c63726e369ba8ebf20eed9a99f77f: 1.000000}, {CPU_group_0_db9c63726e369ba8ebf20eed9a99f77f: 1.000000} for placement, but this node only has remaining {0.000000/18.000000 CPU, 87.135904 GiB/87.135904 GiB memory, 37.343959 GiB/37.343959 GiB object_store_memory, 1.000000/1.000000 CPU_group_0_db9c63726e369ba8ebf20eed9a99f77f, 1.000000/1.000000 CPU_group_1_e230dffb3fd10e5648088dd8af6d9169, 1.000000/1.000000 CPU_group_1_db9c63726e369ba8ebf20eed9a99f77f, 1.000000/1.000000 CPU_group_1_19401f96d5f51f491cfdeacd17a53e34, 1.000000/1.000000 CPU_group_1_3385cc0525dfcbbd4059316246676e35, 0.000000/1.000000 CPU_group_0_5763e5534f3359a5da9b9be1b5b983e2, 1.000000/1.000000 node:10.128.0.18, 0.000000/3.000000 CPU_group_3385cc0525dfcbbd4059316246676e35, 1.000000/1.000000 CPU_group_2_3385cc0525dfcbbd4059316246676e35, 1.000000/1.000000 CPU_group_2_5763e5534f3359a5da9b9be1b5b983e2, 1.000000/1.000000 CPU_group_2_19401f96d5f51f491cfdeacd17a53e34, 1.000000/1.000000 CPU_group_1_04424e424ef3127dac20feda22e6c560, 0.000000/1.000000 CPU_group_0_19401f96d5f51f491cfdeacd17a53e34, 0.000000/3.000000 CPU_group_e230dffb3fd10e5648088dd8af6d9169, 1.000000/1.000000 CPU_group_1_5763e5534f3359a5da9b9be1b5b983e2, 0.000000/1.000000 CPU_group_0_04424e424ef3127dac20feda22e6c560, 3.000000/3.000000 CPU_group_db9c63726e369ba8ebf20eed9a99f77f, 1.000000/1.000000 CPU_group_2_db9c63726e369ba8ebf20eed9a99f77f, 0.000000/3.000000 CPU_group_19401f96d5f51f491cfdeacd17a53e34, 0.000000/3.000000 CPU_group_04424e424ef3127dac20feda22e6c560, 1.000000/1.000000 CPU_group_2_04424e424ef3127dac20feda22e6c560, 0.000000/1.000000 CPU_group_0_e230dffb3fd10e5648088dd8af6d9169, 0.000000/3.000000 CPU_group_5763e5534f3359a5da9b9be1b5b983e2, 0.000000/1.000000 CPU_group_0_3385cc0525dfcbbd4059316246676e35, 1.000000/1.000000 CPU_group_2_e230dffb3fd10e5648088dd8af6d9169}\n",
      ". In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "\u001b[2m\u001b[36m(pid=44271)\u001b[0m 2021-05-19 15:54:27,804\tINFO trainable.py:101 -- Trainable.setup took 28.639 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=44323)\u001b[0m 2021-05-19 15:54:30,487\tINFO trainable.py:101 -- Trainable.setup took 29.118 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=50589, ip=172.30.49.190)\u001b[0m 2021-05-19 15:54:48,396\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=44882)\u001b[0m 2021-05-19 15:54:52,879\tINFO trainable.py:101 -- Trainable.setup took 41.217 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00026 reported episode_reward_max=75.0,episode_reward_min=9.0,episode_reward_mean=23.682634730538922,episode_len_mean=23.682634730538922,episode_media={},episodes_this_iter=167,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.9631844285388717, 'mean_inference_ms': 12.222771919749643, 'mean_action_processing_ms': 0.255405375072121, 'mean_env_wait_ms': 0.5674098740457586, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 29154.236, 'sample_throughput': 137.201, 'learn_time_ms': 146327.871, 'learn_throughput': 27.336, 'update_time_ms': 11.536},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.003363636363636364, 'total_loss': 98.81721806526184, 'policy_loss': -0.049235164624406025, 'vf_loss': 98.85883617401123, 'vf_explained_var': 0.26950118, 'kl': 0.03808935411507264, 'entropy': 0.6566026788204908, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.503787878787879, 'ram_util_percent': 5.518939393939394} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003363636363636364, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00025 reported episode_reward_max=72.0,episode_reward_min=9.0,episode_reward_mean=21.615384615384617,episode_len_mean=21.615384615384617,episode_media={},episodes_this_iter=182,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.1859389613537057, 'mean_inference_ms': 11.77174698261571, 'mean_action_processing_ms': 0.3942190995558289, 'mean_env_wait_ms': 0.6333091128317706, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 28969.839, 'sample_throughput': 138.075, 'learn_time_ms': 146332.115, 'learn_throughput': 27.335, 'update_time_ms': 179.578},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.003272727272727273, 'total_loss': 62.103775918483734, 'policy_loss': -0.0369725339114666, 'vf_loss': 62.13401240110397, 'vf_explained_var': 0.26355857, 'kl': 0.03368750185472891, 'entropy': 0.6609965711832047, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.50381679389313, 'ram_util_percent': 5.516030534351144} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003272727272727273, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=50589, ip=172.30.49.190)\u001b[0m 2021-05-19 15:55:02,636\tINFO trainable.py:101 -- Trainable.setup took 14.240 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00027 reported episode_reward_max=187.0,episode_reward_min=9.0,episode_reward_mean=43.1,episode_len_mean=43.1,episode_media={},episodes_this_iter=78,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.4721618304765354, 'mean_inference_ms': 15.20237193710383, 'mean_action_processing_ms': 0.47480811845146675, 'mean_env_wait_ms': 0.8228191655404109, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 39994.39, 'sample_throughput': 100.014, 'learn_time_ms': 70517.689, 'learn_throughput': 56.723, 'update_time_ms': 8.867},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.003454545454545455, 'total_loss': 360.2304949760437, 'policy_loss': -0.031912141901557334, 'vf_loss': 360.2557306289673, 'vf_explained_var': 0.22010592, 'kl': 0.02222599263768643, 'entropy': 0.5903452634811401, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4214285714285715, 'ram_util_percent': 4.917346938775509} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003454545454545455, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00030 reported episode_reward_max=58.0,episode_reward_min=9.0,episode_reward_mean=20.689473684210526,episode_len_mean=20.689473684210526,episode_media={},episodes_this_iter=190,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.5471508741322342, 'mean_inference_ms': 21.30035299527256, 'mean_action_processing_ms': 1.016779216079152, 'mean_env_wait_ms': 1.7492954551447741, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 53076.432, 'sample_throughput': 75.363, 'learn_time_ms': 61580.08, 'learn_throughput': 64.956, 'update_time_ms': 7.72},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0037272727272727275, 'total_loss': 61.32275938987732, 'policy_loss': -0.04992277477867901, 'vf_loss': 61.365413188934326, 'vf_explained_var': 0.32437074, 'kl': 0.036338812147732824, 'entropy': 0.6576588675379753, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.421428571428572, 'ram_util_percent': 4.915306122448979} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0037272727272727275, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00032 reported episode_reward_max=87.0,episode_reward_min=9.0,episode_reward_mean=22.22905027932961,episode_len_mean=22.22905027932961,episode_media={},episodes_this_iter=179,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.9860711035852623, 'mean_inference_ms': 22.179999930515827, 'mean_action_processing_ms': 0.7600668999447316, 'mean_env_wait_ms': 1.1292498966121496, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 53012.999, 'sample_throughput': 75.453, 'learn_time_ms': 61751.535, 'learn_throughput': 64.776, 'update_time_ms': 6.449},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.003909090909090909, 'total_loss': 87.3034999370575, 'policy_loss': -0.044568521247128956, 'vf_loss': 87.34022963047028, 'vf_explained_var': 0.26253957, 'kl': 0.03919656277867034, 'entropy': 0.6550904382020235, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.4222222222222227, 'ram_util_percent': 4.915151515151514} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003909090909090909, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00031 reported episode_reward_max=80.0,episode_reward_min=8.0,episode_reward_mean=21.825136612021858,episode_len_mean=21.825136612021858,episode_media={},episodes_this_iter=183,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.7871708689881285, 'mean_inference_ms': 21.292849138875102, 'mean_action_processing_ms': 0.7174857511779859, 'mean_env_wait_ms': 2.0108721827608287, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 53176.895, 'sample_throughput': 75.221, 'learn_time_ms': 61860.784, 'learn_throughput': 64.661, 'update_time_ms': 6.853},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0038181818181818187, 'total_loss': 77.4728741645813, 'policy_loss': -0.05467279034201056, 'vf_loss': 77.51951098442078, 'vf_explained_var': 0.28027397, 'kl': 0.040172098902985454, 'entropy': 0.6549572478979826, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.4242424242424245, 'ram_util_percent': 4.9171717171717155} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0038181818181818187, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00017 reported episode_reward_max=200.0,episode_reward_min=10.0,episode_reward_mean=98.68,episode_len_mean=98.68,episode_media={},episodes_this_iter=25,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.3663779056160235, 'mean_inference_ms': 17.24410479090682, 'mean_action_processing_ms': 0.6138668765022183, 'mean_env_wait_ms': 0.9331397045917336, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=16000,timers={'sample_time_ms': 49159.046, 'sample_throughput': 81.369, 'learn_time_ms': 88298.898, 'learn_throughput': 45.301, 'update_time_ms': 10.111},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.0025454545454545456, 'total_loss': 429.226845741272, 'policy_loss': -0.017462012969190255, 'vf_loss': 429.23924350738525, 'vf_explained_var': 0.44972545, 'kl': 0.011257341364398599, 'entropy': 0.5660674963146448, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 16000, 'num_agent_steps_sampled': 16000, 'num_steps_trained': 16000},perf={'cpu_util_percent': 1.5014925373134331, 'ram_util_percent': 5.525373134328359} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0025454545454545456, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00028 reported episode_reward_max=200.0,episode_reward_min=10.0,episode_reward_mean=46.03,episode_len_mean=46.03,episode_media={},episodes_this_iter=68,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.2484369951480847, 'mean_inference_ms': 14.905121587772447, 'mean_action_processing_ms': 0.4941992486758531, 'mean_env_wait_ms': 0.7521477134917408, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 39881.037, 'sample_throughput': 100.298, 'learn_time_ms': 70885.708, 'learn_throughput': 56.429, 'update_time_ms': 73.13},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0035454545454545456, 'total_loss': 405.90173625946045, 'policy_loss': -0.02675847153295763, 'vf_loss': 405.92190647125244, 'vf_explained_var': 0.18040767, 'kl': 0.021960774203762412, 'entropy': 0.5936112720519304, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4196078431372556, 'ram_util_percent': 4.913725490196077} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0035454545454545456, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=46945)\u001b[0m 2021-05-19 15:56:35,910\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=52183, ip=172.30.49.190)\u001b[0m 2021-05-19 15:56:40,269\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=52183, ip=172.30.49.190)\u001b[0m 2021-05-19 15:56:54,102\tINFO trainable.py:101 -- Trainable.setup took 13.833 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00035 reported episode_reward_max=94.0,episode_reward_min=9.0,episode_reward_mean=23.333333333333332,episode_len_mean=23.333333333333332,episode_media={},episodes_this_iter=171,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.6447819503644646, 'mean_inference_ms': 9.221999926196544, 'mean_action_processing_ms': 0.5028998518261439, 'mean_env_wait_ms': 0.4982796021525895, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 22579.031, 'sample_throughput': 177.156, 'learn_time_ms': 90267.002, 'learn_throughput': 44.313, 'update_time_ms': 11.799},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.004181818181818182, 'total_loss': 100.82530117034912, 'policy_loss': -0.049077152769314125, 'vf_loss': 100.8667242527008, 'vf_explained_var': 0.23873895, 'kl': 0.03827173536410555, 'entropy': 0.655359648168087, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.416326530612245, 'ram_util_percent': 4.9204081632653045} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.004181818181818182, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=46945)\u001b[0m 2021-05-19 15:57:14,132\tINFO trainable.py:101 -- Trainable.setup took 38.222 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00029 reported episode_reward_max=62.0,episode_reward_min=9.0,episode_reward_mean=21.03157894736842,episode_len_mean=21.03157894736842,episode_media={},episodes_this_iter=190,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.619285963872151, 'mean_inference_ms': 18.06243403806576, 'mean_action_processing_ms': 0.554287521732346, 'mean_env_wait_ms': 1.0460789449029497, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 44407.538, 'sample_throughput': 90.075, 'learn_time_ms': 122005.047, 'learn_throughput': 32.786, 'update_time_ms': 9.597},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.003636363636363637, 'total_loss': 68.67967522144318, 'policy_loss': -0.05107551920809783, 'vf_loss': 68.72348308563232, 'vf_explained_var': 0.27636135, 'kl': 0.036348226363770664, 'entropy': 0.6581989210098982, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.71484375, 'ram_util_percent': 5.6453125} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003636363636363637, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00033 reported episode_reward_max=108.0,episode_reward_min=8.0,episode_reward_mean=23.38235294117647,episode_len_mean=23.38235294117647,episode_media={},episodes_this_iter=170,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.076501978135114, 'mean_inference_ms': 17.893618907542137, 'mean_action_processing_ms': 0.6565274618099293, 'mean_env_wait_ms': 1.307888566058166, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 42669.44, 'sample_throughput': 93.744, 'learn_time_ms': 122010.124, 'learn_throughput': 32.784, 'update_time_ms': 7.831},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.004, 'total_loss': 115.33252727985382, 'policy_loss': -0.05089786849566735, 'vf_loss': 115.37621831893921, 'vf_explained_var': 0.22616036, 'kl': 0.03603036922868341, 'entropy': 0.6586611960083246, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.70859375, 'ram_util_percent': 5.6453125} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.004, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00026 reported episode_reward_max=186.0,episode_reward_min=9.0,episode_reward_mean=44.02,episode_len_mean=44.02,episode_media={},episodes_this_iter=82,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.058964283649244, 'mean_inference_ms': 12.762824084125214, 'mean_action_processing_ms': 0.31055085886174827, 'mean_env_wait_ms': 0.6856580093647243, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 31192.104, 'sample_throughput': 128.238, 'learn_time_ms': 133863.606, 'learn_throughput': 29.881, 'update_time_ms': 9.33},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.003363636363636364, 'total_loss': 282.3757109642029, 'policy_loss': -0.02677567677164916, 'vf_loss': 282.39606761932373, 'vf_explained_var': 0.20225835, 'kl': 0.021392432536231354, 'entropy': 0.6020689364522696, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.6886178861788619, 'ram_util_percent': 5.6138211382113825} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003363636363636364, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00025 reported episode_reward_max=152.0,episode_reward_min=9.0,episode_reward_mean=41.46,episode_len_mean=41.46,episode_media={},episodes_this_iter=94,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.054975972626767, 'mean_inference_ms': 12.763204061269766, 'mean_action_processing_ms': 0.44138992080611317, 'mean_env_wait_ms': 0.6804608417957083, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 30843.529, 'sample_throughput': 129.687, 'learn_time_ms': 133754.479, 'learn_throughput': 29.906, 'update_time_ms': 93.17},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.003272727272727273, 'total_loss': 221.10450506210327, 'policy_loss': -0.024353002139832824, 'vf_loss': 221.1211452484131, 'vf_explained_var': 0.22794631, 'kl': 0.025716670439578593, 'entropy': 0.6130218356847763, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.6829268292682926, 'ram_util_percent': 5.605691056910568} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003272727272727273, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00034 reported episode_reward_max=88.0,episode_reward_min=8.0,episode_reward_mean=21.944444444444443,episode_len_mean=21.944444444444443,episode_media={},episodes_this_iter=180,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.0765791369593916, 'mean_inference_ms': 13.608131372152158, 'mean_action_processing_ms': 0.5619606163376103, 'mean_env_wait_ms': 0.8886628310071005, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 42876.962, 'sample_throughput': 93.29, 'learn_time_ms': 121653.865, 'learn_throughput': 32.88, 'update_time_ms': 7.808},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.004090909090909091, 'total_loss': 70.0447907447815, 'policy_loss': -0.037395997846033424, 'vf_loss': 70.07546269893646, 'vf_explained_var': 0.2996711, 'kl': 0.03362311254022643, 'entropy': 0.6600688248872757, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.6684210526315792, 'ram_util_percent': 5.603759398496242} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.004090909090909091, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00027 reported episode_reward_max=200.0,episode_reward_min=9.0,episode_reward_mean=72.37,episode_len_mean=72.37,episode_media={},episodes_this_iter=29,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.5143601846541332, 'mean_inference_ms': 16.75071713611028, 'mean_action_processing_ms': 0.536711075306838, 'mean_env_wait_ms': 0.9590168196465058, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 43037.808, 'sample_throughput': 92.942, 'learn_time_ms': 68895.733, 'learn_throughput': 58.059, 'update_time_ms': 8.244},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.003454545454545455, 'total_loss': 536.0865163803101, 'policy_loss': -0.014710704621393234, 'vf_loss': 536.0961971282959, 'vf_explained_var': 0.23950198, 'kl': 0.011181785914232023, 'entropy': 0.5802692845463753, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.4202020202020207, 'ram_util_percent': 4.920202020202019} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003454545454545455, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00030 reported episode_reward_max=143.0,episode_reward_min=9.0,episode_reward_mean=43.5,episode_len_mean=43.5,episode_media={},episodes_this_iter=83,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.3873938260198895, 'mean_inference_ms': 20.831843268830102, 'mean_action_processing_ms': 1.0668413952512759, 'mean_env_wait_ms': 1.6471867282649715, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 51111.911, 'sample_throughput': 78.26, 'learn_time_ms': 63534.337, 'learn_throughput': 62.958, 'update_time_ms': 5.123},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0037272727272727275, 'total_loss': 213.91780471801758, 'policy_loss': -0.03721096841036342, 'vf_loss': 213.94691276550293, 'vf_explained_var': 0.2795485, 'kl': 0.027036867395509034, 'entropy': 0.5927107129245996, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4191919191919196, 'ram_util_percent': 4.919191919191918} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0037272727272727275, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00032 reported episode_reward_max=157.0,episode_reward_min=9.0,episode_reward_mean=47.14,episode_len_mean=47.14,episode_media={},episodes_this_iter=78,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.8525421232202535, 'mean_inference_ms': 21.506104374508432, 'mean_action_processing_ms': 0.6266405851710205, 'mean_env_wait_ms': 1.2471178927264466, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 51222.353, 'sample_throughput': 78.091, 'learn_time_ms': 64032.206, 'learn_throughput': 62.469, 'update_time_ms': 4.703},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.003909090909090909, 'total_loss': 338.2655634880066, 'policy_loss': -0.02275286801159382, 'vf_loss': 338.28256464004517, 'vf_explained_var': 0.22160837, 'kl': 0.019140446151141077, 'entropy': 0.5928974095731974, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4188118811881192, 'ram_util_percent': 4.917821782178216} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003909090909090909, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00031 reported episode_reward_max=200.0,episode_reward_min=10.0,episode_reward_mean=44.41,episode_len_mean=44.41,episode_media={},episodes_this_iter=68,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.6285332257908038, 'mean_inference_ms': 21.010767717756124, 'mean_action_processing_ms': 0.7246172175089092, 'mean_env_wait_ms': 1.9597830116273602, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 51212.917, 'sample_throughput': 78.105, 'learn_time_ms': 63892.036, 'learn_throughput': 62.606, 'update_time_ms': 5.108},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0038181818181818187, 'total_loss': 419.3629961013794, 'policy_loss': -0.024895243695937097, 'vf_loss': 419.38171100616455, 'vf_explained_var': 0.18273216, 'kl': 0.02062134415609762, 'entropy': 0.5892345029860735, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4188118811881194, 'ram_util_percent': 4.918811881188117} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0038181818181818187, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=53737, ip=172.30.49.190)\u001b[0m 2021-05-19 15:58:25,799\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=53739, ip=172.30.49.190)\u001b[0m 2021-05-19 15:58:25,805\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=53737, ip=172.30.49.190)\u001b[0m 2021-05-19 15:58:41,733\tINFO trainable.py:101 -- Trainable.setup took 15.935 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=53739, ip=172.30.49.190)\u001b[0m 2021-05-19 15:58:41,721\tINFO trainable.py:101 -- Trainable.setup took 15.916 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00037 reported episode_reward_max=93.0,episode_reward_min=9.0,episode_reward_mean=22.725714285714286,episode_len_mean=22.725714285714286,episode_media={},episodes_this_iter=175,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.748811364897298, 'mean_inference_ms': 10.81839937158474, 'mean_action_processing_ms': 0.3127144585927598, 'mean_env_wait_ms': 0.4644452052342097, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 25902.238, 'sample_throughput': 154.427, 'learn_time_ms': 88348.739, 'learn_throughput': 45.275, 'update_time_ms': 7.504},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.004363636363636364, 'total_loss': 89.68724203109741, 'policy_loss': -0.04823113637394272, 'vf_loss': 89.72783136367798, 'vf_explained_var': 0.2495628, 'kl': 0.038214046449866146, 'entropy': 0.6561352126300335, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.4280000000000002, 'ram_util_percent': 4.854999999999999} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.004363636363636364, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00035 reported episode_reward_max=154.0,episode_reward_min=9.0,episode_reward_mean=44.91,episode_len_mean=44.91,episode_media={},episodes_this_iter=78,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.646458380871138, 'mean_inference_ms': 9.680649465849106, 'mean_action_processing_ms': 0.4952809612381463, 'mean_env_wait_ms': 0.5001565315766736, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 23695.892, 'sample_throughput': 168.806, 'learn_time_ms': 89521.59, 'learn_throughput': 44.682, 'update_time_ms': 9.42},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.004181818181818182, 'total_loss': 298.0963182449341, 'policy_loss': -0.035613251966424286, 'vf_loss': 298.1249780654907, 'vf_explained_var': 0.24780162, 'kl': 0.023185063939308748, 'entropy': 0.5902097504585981, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4242424242424245, 'ram_util_percent': 4.854545454545454} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.004181818181818182, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00029 reported episode_reward_max=191.0,episode_reward_min=9.0,episode_reward_mean=45.64,episode_len_mean=45.64,episode_media={},episodes_this_iter=70,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.216142330245002, 'mean_inference_ms': 16.658999398561473, 'mean_action_processing_ms': 0.516925878537756, 'mean_env_wait_ms': 1.095093450521403, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 39057.327, 'sample_throughput': 102.414, 'learn_time_ms': 98970.161, 'learn_throughput': 40.416, 'update_time_ms': 8.549},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.003636363636363637, 'total_loss': 368.25832080841064, 'policy_loss': -0.022989694407442585, 'vf_loss': 368.27550506591797, 'vf_explained_var': 0.19779554, 'kl': 0.01935581759607885, 'entropy': 0.611551882699132, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.436274509803922, 'ram_util_percent': 5.600000000000001} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003636363636363637, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00033 reported episode_reward_max=200.0,episode_reward_min=9.0,episode_reward_mean=46.93,episode_len_mean=46.93,episode_media={},episodes_this_iter=65,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.9947410047109034, 'mean_inference_ms': 16.678217464863398, 'mean_action_processing_ms': 0.6618091039874815, 'mean_env_wait_ms': 1.1416420371359188, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 38312.521, 'sample_throughput': 104.405, 'learn_time_ms': 98955.338, 'learn_throughput': 40.422, 'update_time_ms': 7.551},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.004, 'total_loss': 462.0213813781738, 'policy_loss': -0.02603011135943234, 'vf_loss': 462.0411567687988, 'vf_explained_var': 0.17912385, 'kl': 0.020861659868387505, 'entropy': 0.5934895891696215, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.4274509803921571, 'ram_util_percent': 5.600000000000001} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.004, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00036 reported episode_reward_max=71.0,episode_reward_min=8.0,episode_reward_mean=21.176470588235293,episode_len_mean=21.176470588235293,episode_media={},episodes_this_iter=187,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.0492723479834687, 'mean_inference_ms': 15.12219898927296, 'mean_action_processing_ms': 0.4828267844140443, 'mean_env_wait_ms': 0.6675813215337635, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=4000,timers={'sample_time_ms': 45263.37, 'sample_throughput': 88.372, 'learn_time_ms': 84738.698, 'learn_throughput': 47.204, 'update_time_ms': 2.537},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 0.0042727272727272735, 'total_loss': 58.24355274438858, 'policy_loss': -0.046269878454040736, 'vf_loss': 58.28229737281799, 'vf_explained_var': 0.29204154, 'kl': 0.03762706008274108, 'entropy': 0.6561413779854774, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 1.5129310344827585, 'ram_util_percent': 5.6000000000000005} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0042727272727272735, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00030 reported episode_reward_max=200.0,episode_reward_min=9.0,episode_reward_mean=67.67,episode_len_mean=67.67,episode_media={},episodes_this_iter=41,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.2949891266785274, 'mean_inference_ms': 20.584643020055722, 'mean_action_processing_ms': 1.0033027352894839, 'mean_env_wait_ms': 1.565644851847814, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 49053.48, 'sample_throughput': 81.544, 'learn_time_ms': 66808.64, 'learn_throughput': 59.872, 'update_time_ms': 5.968},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.0037272727272727275, 'total_loss': 373.4915885925293, 'policy_loss': -0.019714975962415338, 'vf_loss': 373.5053825378418, 'vf_explained_var': 0.34701684, 'kl': 0.01315755897667259, 'entropy': 0.5774112772196531, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.4221153846153847, 'ram_util_percent': 4.856730769230769} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.0037272727272727275, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "<IPython.core.display.HTML object>\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00032 reported episode_reward_max=200.0,episode_reward_min=12.0,episode_reward_mean=77.15,episode_len_mean=77.15,episode_media={},episodes_this_iter=26,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.749035652599776, 'mean_inference_ms': 21.128797328633006, 'mean_action_processing_ms': 0.7170631947046615, 'mean_env_wait_ms': 1.1509694330270697, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 48908.634, 'sample_throughput': 81.785, 'learn_time_ms': 67116.576, 'learn_throughput': 59.598, 'update_time_ms': 4.011},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.003909090909090909, 'total_loss': 738.3832874298096, 'policy_loss': -0.010090224677696824, 'vf_loss': 738.3875427246094, 'vf_explained_var': 0.269285, 'kl': 0.019435386231634766, 'entropy': 0.5580171952024102, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.4249999999999998, 'ram_util_percent': 4.858653846153845} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003909090909090909, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00026 reported episode_reward_max=200.0,episode_reward_min=10.0,episode_reward_mean=70.97,episode_len_mean=70.97,episode_media={},episodes_this_iter=35,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.0687369026594842, 'mean_inference_ms': 13.091023792824972, 'mean_action_processing_ms': 0.3560358051329004, 'mean_env_wait_ms': 0.73538260575278, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 32589.753, 'sample_throughput': 122.738, 'learn_time_ms': 120647.692, 'learn_throughput': 33.154, 'update_time_ms': 8.502},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.003363636363636364, 'total_loss': 462.3718605041504, 'policy_loss': -0.014000585448229685, 'vf_loss': 462.3795509338379, 'vf_explained_var': 0.27624837, 'kl': 0.014011700506671332, 'entropy': 0.5828752350062132, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.5018018018018022, 'ram_util_percent': 5.627027027027027} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003363636363636364, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00025 reported episode_reward_max=200.0,episode_reward_min=11.0,episode_reward_mean=56.6,episode_len_mean=56.6,episode_media={},episodes_this_iter=52,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.0677923799016305, 'mean_inference_ms': 13.146849484283761, 'mean_action_processing_ms': 0.4324632373939275, 'mean_env_wait_ms': 0.7085179473623969, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=12000,timers={'sample_time_ms': 32310.338, 'sample_throughput': 123.799, 'learn_time_ms': 120721.403, 'learn_throughput': 33.134, 'update_time_ms': 108.666},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.003272727272727273, 'total_loss': 371.6432132720947, 'policy_loss': -0.03600062761688605, 'vf_loss': 371.67265129089355, 'vf_explained_var': 0.25542966, 'kl': 0.014598404319258407, 'entropy': 0.5779610145837069, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 1.5036363636363639, 'ram_util_percent': 5.62181818181818} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.003272727272727273, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}. This trial completed.\n",
      "\u001b[2m\u001b[36m(pid=32576)\u001b[0m Trial PPO_CartPole-v0_359a9_00034 reported episode_reward_max=195.0,episode_reward_min=9.0,episode_reward_mean=46.9,episode_len_mean=46.9,episode_media={},episodes_this_iter=67,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.9818243134831881, 'mean_inference_ms': 14.092998354039207, 'mean_action_processing_ms': 0.6838568961211239, 'mean_env_wait_ms': 0.8131958893710316, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,agent_timesteps_total=8000,timers={'sample_time_ms': 38959.87, 'sample_throughput': 102.67, 'learn_time_ms': 108467.07, 'learn_throughput': 36.878, 'update_time_ms': 5.947},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.004090909090909091, 'total_loss': 426.66083431243896, 'policy_loss': -0.02042515471111983, 'vf_loss': 426.6747570037842, 'vf_explained_var': 0.17694315, 'kl': 0.02170847007073462, 'entropy': 0.6044233702123165, 'entropy_coeff': 0.0}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 1.5072727272727275, 'ram_util_percent': 5.619999999999999} with parameters={'framework': 'torch', 'env': 'CartPole-v0', 'num_gpus': 0, 'lr': 0.004090909090909091, 'log_level': 'ERROR', 'num_cpus_per_worker': 1, 'num_cpus_for_driver': 1}.\n",
      "<IPython.core.display.HTML object>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=49698)\u001b[0m 2021-05-19 16:00:01,892\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=55388, ip=172.30.49.190)\u001b[0m 2021-05-19 16:00:20,464\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=55395, ip=172.30.49.190)\u001b[0m 2021-05-19 16:00:20,462\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=49751)\u001b[0m 2021-05-19 16:00:22,574\tINFO trainer.py:694 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fbeed90d88ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"initialized\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m tune.run(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;34m\"PPO\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"episode_reward_max\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint, _remote)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_remote\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         return ray.get(\n\u001b[0m\u001b[1;32m    301\u001b[0m             ray.remote(num_cpus=0)(run).remote(\n\u001b[1;32m    302\u001b[0m                 \u001b[0mrun_or_experiment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclient_mode_should_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/site-packages/ray/util/client/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, vals, timeout)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmilliseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \"\"\"\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/site-packages/ray/util/client/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, vals, timeout)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                         \u001b[0mop_timeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAX_BLOCKING_OPERATION_TIME_S\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mGetTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/site-packages/ray/util/client/worker.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self, ref, timeout)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray_client_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mdecode_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/site-packages/ray/util/client/dataclient.py\u001b[0m in \u001b[0;36mGetObject\u001b[0;34m(self, request, context)\u001b[0m\n\u001b[1;32m    128\u001b[0m                   context=None) -> ray_client_pb2.GetResponse:\n\u001b[1;32m    129\u001b[0m         \u001b[0mdatareq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray_client_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatareq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/site-packages/ray/util/client/dataclient.py\u001b[0m in \u001b[0;36m_blocking_send\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             self.cv.wait_for(\n\u001b[0m\u001b[1;32m    106\u001b[0m                 lambda: req_id in self.ready_data or self._in_shutdown)\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_shutdown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait_for\u001b[0;34m(self, predicate, timeout)\u001b[0m\n\u001b[1;32m    335\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mwaittime\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaittime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "import ray.util\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from ray.tune.progress_reporter import JupyterNotebookReporter\n",
    "\n",
    "ray.util.connect(cluster.head_model.address +\":10001\")\n",
    "reporter = JupyterNotebookReporter(overwrite=True)\n",
    "\n",
    "print(\"initialized\")\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\"episode_reward_max\": 200},\n",
    "    config={\n",
    "        \"framework\": \"torch\",\n",
    "        \"env\": \"CartPole-v0\",\n",
    "        \"num_gpus\": 0,\n",
    "        \"lr\": tune.grid_search(np.linspace (0.001, 0.01, 100).tolist()),\n",
    "        \"log_level\": \"ERROR\",\n",
    "        \"num_cpus_per_worker\": 1,\n",
    "        \"num_cpus_for_driver\": 1,\n",
    "    },\n",
    "    local_dir=\"/lus/scratch/arigazzi/ray_local/\",\n",
    "    verbose=2,\n",
    "    fail_fast=True,\n",
    "    progress_reporter = reporter,\n",
    "    log_to_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100a7123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:00:30 crystal SmartSim[3737] INFO Stopping model workers with job name workers-CBHIFCQZY0IL\n",
      "16:00:30 crystal SmartSim[3737] INFO Stopping model head with job name head-CBHIF67HSKFD\n"
     ]
    }
   ],
   "source": [
    "exp.stop(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020563e8-e6ab-4a0a-9e66-91fd9417a2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartsim",
   "language": "python",
   "name": "smartsim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
