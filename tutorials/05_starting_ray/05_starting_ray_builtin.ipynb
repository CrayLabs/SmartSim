{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setting up a Ray cluster with SmartSim"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Start the cluster\n",
    "We set up a SmartSim experiment, which will handle the launch of the Ray cluster.\n",
    "\n",
    "First we import the relevant modules and set up variables. `NUM_WORKERS` is the number of worker nodes: in total, we will spin a Ray cluster of `NUM_WORKERS+1` nodes (one node is the head node)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "\n",
    "from ray.tune.progress_reporter import JupyterNotebookReporter\n",
    "import ray\n",
    "from ray import tune\n",
    "import ray.util\n",
    "\n",
    "from smartsim import Experiment\n",
    "from smartsim.ext.ray import RayCluster\n",
    "\n",
    "NUM_WORKERS = 3\n",
    "CPUS_PER_WORKER = 18\n",
    "alloc=None\n",
    "launcher='slurm'"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we define a SmartSim experiment which will spin the Ray cluster. The output files will be located in the `ray-cluster` directory (relative to the path from where we are executing this notebook). We are limiting the number each ray node can use to `CPUS_PER_WORKER`: if we wanted to let it use all the cpus, it would suffice not to pass `ray_args`.\n",
    "Notice that the cluster will be password-protected (the password, generated internally, will be shared with worker nodes)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "exp = Experiment(\"ray-cluster\", launcher=launcher)\n",
    "cluster = RayCluster(name=\"ray-cluster\", run_args={}, ray_args={\"num-cpus\": CPUS_PER_WORKER},\n",
    "                     launcher=launcher, workers=NUM_WORKERS, alloc=alloc, batch=True)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the cluster has to be run as a batch, we might want to pass some preamble lines to the batch files, to setup modules and environments. If we are running this in an internal allocation, the environment will be automatically propagated."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "if cluster.batch:\n",
    "    cluster.head_model.batch_settings.add_preamble( [\"source ~/.bashrc\", \"conda activate smartsim\"])\n",
    "    if NUM_WORKERS:\n",
    "        cluster.worker_model.batch_settings.add_preamble ( [\"source ~/.bashrc\", \"conda activate smartsim\"])"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now generate the needed directories. If an experiment with the same name already exists, this call will fail, to avoid overwriting existing results. If we want to overwrite, we can simply pass `overwrite=True` to `exp.generate()`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "exp.generate(cluster, overwrite=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "08:34:17 osprey.us.cray.com SmartSim[17173] INFO Working in previously created experiment\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are ready to start the cluster!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "exp.start(cluster, block=False, summary=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\u001b[36;1m=== LAUNCH SUMMARY ===\u001b[0m\n",
      "\u001b[32;1mExperiment: ray-cluster\u001b[0m\n",
      "\u001b[32mExperiment Path: /lus/sonexion/arigazzi/smartsim-dev/SmartSim/tutorials/05_starting_ray/ray-cluster\u001b[0m\n",
      "\u001b[32mLaunching with: slurm\u001b[0m\n",
      "\u001b[32m# of Ensembles: 0\u001b[0m\n",
      "\u001b[32m# of Models: 0\u001b[0m\n",
      "\u001b[32mDatabase: no\u001b[0m\n",
      "\n",
      "\u001b[36;1m=== RAY CLUSTERS ===\u001b[0m\n",
      "\u001b[32;1mray-cluster\u001b[0m\n",
      "\u001b[32m# of workers: 3\u001b[0m\n",
      "\u001b[32mLaunching as batch: True\u001b[0m\n",
      "\u001b[32mBatch Settings: \n",
      "None\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "08:34:43 osprey.us.cray.com SmartSim[17173] INFO Ray cluster launched on nodes: ['prod-0010']\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Start the ray driver script\n",
    "\n",
    "Now we can just connect to our running server."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "ray.util.connect(cluster.head_model.address+\":10001\")\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'num_clients': 1,\n",
       " 'python_version': '3.7.10',\n",
       " 'ray_version': '1.3.0',\n",
       " 'ray_commit': '9f45548488c4fa288f3cecb556801f97958eae8b',\n",
       " 'protocol_version': '2020-03-12'}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we check that all resources are set properly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print('''This cluster consists of\n",
    "    {} nodes in total\n",
    "    {} CPU resources in total\n",
    "'''.format(len(ray.nodes()), ray.cluster_resources()['CPU']))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This cluster consists of\n",
      "    4 nodes in total\n",
      "    72.0 CPU resources in total\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\"episode_reward_max\": 200},\n",
    "    config={\n",
    "        \"framework\": \"torch\",\n",
    "        \"env\": \"CartPole-v0\",\n",
    "        \"num_gpus\": 0,\n",
    "        \"lr\": tune.grid_search(np.linspace (0.001, 0.01, 50).tolist()),\n",
    "        \"log_level\": \"ERROR\",\n",
    "    },\n",
    "    local_dir=\"/lus/scratch/arigazzi/ray_local/\",\n",
    "    verbose=0,\n",
    "    fail_fast=True,\n",
    "    log_to_file=True,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=68106)\u001b[0m 2021-07-29 08:35:45,729\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=68100)\u001b[0m 2021-07-29 08:35:46,938\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=68104)\u001b[0m 2021-07-29 08:35:47,383\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=68099)\u001b[0m 2021-07-29 08:35:47,416\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=68095)\u001b[0m 2021-07-29 08:35:47,482\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=68098)\u001b[0m 2021-07-29 08:35:47,555\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=68106)\u001b[0m 2021-07-29 08:36:17,375\tINFO trainable.py:104 -- Trainable.setup took 31.647 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=68100)\u001b[0m 2021-07-29 08:36:19,553\tINFO trainable.py:104 -- Trainable.setup took 32.622 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=68104)\u001b[0m 2021-07-29 08:36:19,668\tINFO trainable.py:104 -- Trainable.setup took 32.286 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=68099)\u001b[0m 2021-07-29 08:36:19,646\tINFO trainable.py:104 -- Trainable.setup took 32.232 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=68098)\u001b[0m 2021-07-29 08:36:20,058\tINFO trainable.py:104 -- Trainable.setup took 32.504 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=68095)\u001b[0m 2021-07-29 08:36:20,081\tINFO trainable.py:104 -- Trainable.setup took 32.599 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=59464, ip=10.10.1.16)\u001b[0m 2021-07-29 08:38:10,147\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=30838, ip=10.10.1.17)\u001b[0m 2021-07-29 08:38:11,458\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=4938, ip=10.10.1.15)\u001b[0m 2021-07-29 08:38:11,662\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=4903, ip=10.10.1.15)\u001b[0m 2021-07-29 08:38:12,314\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=30808, ip=10.10.1.17)\u001b[0m 2021-07-29 08:38:12,934\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=59441, ip=10.10.1.16)\u001b[0m 2021-07-29 08:38:12,967\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=4865, ip=10.10.1.15)\u001b[0m 2021-07-29 08:38:12,981\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=30815, ip=10.10.1.17)\u001b[0m 2021-07-29 08:38:13,133\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=30858, ip=10.10.1.17)\u001b[0m 2021-07-29 08:38:13,169\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=30866, ip=10.10.1.17)\u001b[0m 2021-07-29 08:38:13,266\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=30725, ip=10.10.1.17)\u001b[0m 2021-07-29 08:38:13,394\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=4930, ip=10.10.1.15)\u001b[0m 2021-07-29 08:38:13,565\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=59453, ip=10.10.1.16)\u001b[0m 2021-07-29 08:38:13,966\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=59414, ip=10.10.1.16)\u001b[0m 2021-07-29 08:38:14,032\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=4796, ip=10.10.1.15)\u001b[0m 2021-07-29 08:38:14,414\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=59396, ip=10.10.1.16)\u001b[0m 2021-07-29 08:38:14,552\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=59330, ip=10.10.1.16)\u001b[0m 2021-07-29 08:38:14,644\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=4896, ip=10.10.1.15)\u001b[0m 2021-07-29 08:38:14,676\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=59464, ip=10.10.1.16)\u001b[0m 2021-07-29 08:38:20,528\tINFO trainable.py:104 -- Trainable.setup took 10.383 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=30838, ip=10.10.1.17)\u001b[0m 2021-07-29 08:38:23,911\tINFO trainable.py:104 -- Trainable.setup took 12.454 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=4938, ip=10.10.1.15)\u001b[0m 2021-07-29 08:38:24,998\tINFO trainable.py:104 -- Trainable.setup took 13.337 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=4903, ip=10.10.1.15)\u001b[0m 2021-07-29 08:38:26,937\tINFO trainable.py:104 -- Trainable.setup took 14.625 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=59441, ip=10.10.1.16)\u001b[0m 2021-07-29 08:38:27,326\tINFO trainable.py:104 -- Trainable.setup took 14.360 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=4865, ip=10.10.1.15)\u001b[0m 2021-07-29 08:38:27,741\tINFO trainable.py:104 -- Trainable.setup took 14.760 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=30808, ip=10.10.1.17)\u001b[0m 2021-07-29 08:38:28,123\tINFO trainable.py:104 -- Trainable.setup took 15.190 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=30858, ip=10.10.1.17)\u001b[0m 2021-07-29 08:38:28,342\tINFO trainable.py:104 -- Trainable.setup took 15.173 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=4930, ip=10.10.1.15)\u001b[0m 2021-07-29 08:38:28,490\tINFO trainable.py:104 -- Trainable.setup took 14.921 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=30815, ip=10.10.1.17)\u001b[0m 2021-07-29 08:38:28,909\tINFO trainable.py:104 -- Trainable.setup took 15.777 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=30866, ip=10.10.1.17)\u001b[0m 2021-07-29 08:38:28,918\tINFO trainable.py:104 -- Trainable.setup took 15.653 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=59453, ip=10.10.1.16)\u001b[0m 2021-07-29 08:38:29,098\tINFO trainable.py:104 -- Trainable.setup took 15.132 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=30725, ip=10.10.1.17)\u001b[0m 2021-07-29 08:38:29,181\tINFO trainable.py:104 -- Trainable.setup took 15.788 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=4796, ip=10.10.1.15)\u001b[0m 2021-07-29 08:38:29,238\tINFO trainable.py:104 -- Trainable.setup took 14.825 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=59414, ip=10.10.1.16)\u001b[0m 2021-07-29 08:38:30,122\tINFO trainable.py:104 -- Trainable.setup took 16.091 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=4896, ip=10.10.1.15)\u001b[0m 2021-07-29 08:38:30,462\tINFO trainable.py:104 -- Trainable.setup took 15.786 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=59396, ip=10.10.1.16)\u001b[0m 2021-07-29 08:38:30,471\tINFO trainable.py:104 -- Trainable.setup took 15.920 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=59330, ip=10.10.1.16)\u001b[0m 2021-07-29 08:38:31,005\tINFO trainable.py:104 -- Trainable.setup took 16.362 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=68081)\u001b[0m 2021-07-29 08:40:07,105\tWARNING util.py:162 -- The `process_trial_result` operation took 0.524 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=68081)\u001b[0m 2021-07-29 08:40:07,141\tWARNING util.py:162 -- Processing trial results took 0.560 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(pid=68081)\u001b[0m 2021-07-29 08:40:07,141\tWARNING util.py:162 -- The `process_trial` operation took 0.561 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=68081)\u001b[0m 2021-07-29 08:40:09,460\tWARNING util.py:162 -- The `callbacks.on_trial_result` operation took 0.518 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=68081)\u001b[0m 2021-07-29 08:40:09,476\tWARNING util.py:162 -- The `process_trial_result` operation took 0.546 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=68081)\u001b[0m 2021-07-29 08:40:09,476\tWARNING util.py:162 -- Processing trial results took 0.547 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(pid=68081)\u001b[0m 2021-07-29 08:40:09,476\tWARNING util.py:162 -- The `process_trial` operation took 0.550 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=72206)\u001b[0m 2021-07-29 08:40:43,217\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=72206)\u001b[0m 2021-07-29 08:41:20,462\tINFO trainable.py:104 -- Trainable.setup took 37.245 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "The actor or task with ID ffffffffffffffff95f324476c0b711ea9a45e3101000000 cannot be scheduled right now. It requires {CPU_group_0_2be5cf4c13f12e26fcf85fdbd9b12111: 1.000000}, {CPU_group_2be5cf4c13f12e26fcf85fdbd9b12111: 1.000000} for placement, but this node only has remaining {0.000000/18.000000 CPU, 85.325590 GiB/85.325590 GiB memory, 36.568110 GiB/36.568110 GiB object_store_memory, 3.000000/3.000000 CPU_group_4f68ace57b286e764fc814ba03c5e9cb, 1.000000/1.000000 CPU_group_1_2be5cf4c13f12e26fcf85fdbd9b12111, 1.000000/1.000000 CPU_group_2_2be5cf4c13f12e26fcf85fdbd9b12111, 1.000000/1.000000 CPU_group_1_1295364c8674b04be075bde4ff5ec696, 0.000000/3.000000 CPU_group_aa482940fe440e2e78aa21f17b27e6a1, 1.000000/1.000000 node:10.10.1.15, 1.000000/1.000000 CPU_group_2_1295364c8674b04be075bde4ff5ec696, 1.000000/1.000000 CPU_group_1_4f68ace57b286e764fc814ba03c5e9cb, 0.000000/1.000000 CPU_group_0_fd06d1df950411d2ac52f9d895ccd0f5, 0.000000/3.000000 CPU_group_aff63ab1b9502eab249ecc69f004523c, 3.000000/3.000000 CPU_group_2be5cf4c13f12e26fcf85fdbd9b12111, 1.000000/1.000000 CPU_group_1_aff63ab1b9502eab249ecc69f004523c, 1.000000/1.000000 CPU_group_1_aa482940fe440e2e78aa21f17b27e6a1, 0.000000/1.000000 CPU_group_0_aa482940fe440e2e78aa21f17b27e6a1, 1.000000/1.000000 CPU_group_2_aff63ab1b9502eab249ecc69f004523c, 0.000000/3.000000 CPU_group_fd06d1df950411d2ac52f9d895ccd0f5, 1.000000/1.000000 CPU_group_1_fd06d1df950411d2ac52f9d895ccd0f5, 1.000000/1.000000 CPU_group_0_2be5cf4c13f12e26fcf85fdbd9b12111, 0.000000/3.000000 CPU_group_1295364c8674b04be075bde4ff5ec696, 1.000000/1.000000 CPU_group_0_4f68ace57b286e764fc814ba03c5e9cb, 0.000000/1.000000 CPU_group_0_1295364c8674b04be075bde4ff5ec696, 1.000000/1.000000 CPU_group_2_4f68ace57b286e764fc814ba03c5e9cb, 1.000000/1.000000 CPU_group_2_aa482940fe440e2e78aa21f17b27e6a1, 1.000000/1.000000 CPU_group_2_fd06d1df950411d2ac52f9d895ccd0f5, 0.000000/1.000000 CPU_group_0_aff63ab1b9502eab249ecc69f004523c}\n",
      ". In total there are 0 pending tasks and 2 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "The actor or task with ID fffffffffffffffff12b79e0168ec0d25123280801000000 cannot be scheduled right now. It requires {CPU_group_0_21bf9cf993811a2a2174aadea9724842: 1.000000}, {CPU_group_21bf9cf993811a2a2174aadea9724842: 1.000000} for placement, but this node only has remaining {0.000000/18.000000 CPU, 85.390494 GiB/85.390494 GiB memory, 36.595926 GiB/36.595926 GiB object_store_memory, 1.000000/1.000000 CPU_group_2_e87c1f6b3ff4cc7826fc6010b0e89e5c, 1.000000/1.000000 CPU_group_1_38cda0202e59dc6af2faf2aee0e70d66, 1.000000/1.000000 CPU_group_2_38cda0202e59dc6af2faf2aee0e70d66, 0.000000/3.000000 CPU_group_1edf78cda26535435567346a5e7e9012, 1.000000/1.000000 CPU_group_0_1c1623d47dc52ab4d03bbf01a69f155d, 0.000000/1.000000 CPU_group_0_e87c1f6b3ff4cc7826fc6010b0e89e5c, 1.000000/1.000000 CPU_group_1_007247c1028f776129066838f2c30dda, 1.000000/1.000000 CPU_group_1_1edf78cda26535435567346a5e7e9012, 0.000000/3.000000 CPU_group_38cda0202e59dc6af2faf2aee0e70d66, 0.000000/3.000000 CPU_group_e87c1f6b3ff4cc7826fc6010b0e89e5c, 3.000000/3.000000 CPU_group_1c1623d47dc52ab4d03bbf01a69f155d, 1.000000/1.000000 CPU_group_0_21bf9cf993811a2a2174aadea9724842, 1.000000/1.000000 node:10.10.1.17, 1.000000/1.000000 CPU_group_2_1c1623d47dc52ab4d03bbf01a69f155d, 0.000000/1.000000 CPU_group_0_1edf78cda26535435567346a5e7e9012, 0.000000/1.000000 CPU_group_0_007247c1028f776129066838f2c30dda, 3.000000/3.000000 CPU_group_21bf9cf993811a2a2174aadea9724842, 1.000000/1.000000 CPU_group_2_1edf78cda26535435567346a5e7e9012, 0.000000/1.000000 CPU_group_0_38cda0202e59dc6af2faf2aee0e70d66, 1.000000/1.000000 CPU_group_2_007247c1028f776129066838f2c30dda, 1.000000/1.000000 CPU_group_1_21bf9cf993811a2a2174aadea9724842, 1.000000/1.000000 CPU_group_1_1c1623d47dc52ab4d03bbf01a69f155d, 1.000000/1.000000 CPU_group_1_e87c1f6b3ff4cc7826fc6010b0e89e5c, 1.000000/1.000000 CPU_group_2_21bf9cf993811a2a2174aadea9724842, 0.000000/3.000000 CPU_group_007247c1028f776129066838f2c30dda}\n",
      ". In total there are 0 pending tasks and 2 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "The actor or task with ID ffffffffffffffffe022878603b482adb0806a6d01000000 cannot be scheduled right now. It requires {CPU_group_0_36c81298271a12d43e622d3f8674ae5b: 1.000000}, {CPU_group_36c81298271a12d43e622d3f8674ae5b: 1.000000} for placement, but this node only has remaining {0.000000/18.000000 CPU, 85.553723 GiB/85.553723 GiB memory, 36.665881 GiB/36.665881 GiB object_store_memory, 1.000000/1.000000 CPU_group_2_34b2a68e3bb1d7341cacad3c262d308b, 0.000000/3.000000 CPU_group_7c3399e340d4effa58e7169e1cea5a9e, 1.000000/1.000000 CPU_group_2_31d21345c879abbc0eac72dd3c285a0b, 0.000000/3.000000 CPU_group_34b2a68e3bb1d7341cacad3c262d308b, 0.000000/1.000000 CPU_group_0_7c3399e340d4effa58e7169e1cea5a9e, 1.000000/1.000000 CPU_group_1_36c81298271a12d43e622d3f8674ae5b, 1.000000/1.000000 node:10.10.1.16, 1.000000/1.000000 CPU_group_1_34b2a68e3bb1d7341cacad3c262d308b, 1.000000/1.000000 CPU_group_2_7c3399e340d4effa58e7169e1cea5a9e, 0.000000/3.000000 CPU_group_11150ae516918a93832c04c97c0de393, 1.000000/1.000000 CPU_group_1_11150ae516918a93832c04c97c0de393, 0.000000/3.000000 CPU_group_4f3d192e6b4c6328574fefb762d0b6d1, 0.000000/1.000000 CPU_group_0_4f3d192e6b4c6328574fefb762d0b6d1, 0.000000/3.000000 CPU_group_31d21345c879abbc0eac72dd3c285a0b, 1.000000/1.000000 CPU_group_0_36c81298271a12d43e622d3f8674ae5b, 1.000000/1.000000 CPU_group_2_36c81298271a12d43e622d3f8674ae5b, 0.000000/1.000000 CPU_group_0_31d21345c879abbc0eac72dd3c285a0b, 3.000000/3.000000 CPU_group_36c81298271a12d43e622d3f8674ae5b, 1.000000/1.000000 CPU_group_1_4f3d192e6b4c6328574fefb762d0b6d1, 1.000000/1.000000 CPU_group_2_4f3d192e6b4c6328574fefb762d0b6d1, 0.000000/1.000000 CPU_group_0_11150ae516918a93832c04c97c0de393, 1.000000/1.000000 CPU_group_1_31d21345c879abbc0eac72dd3c285a0b, 1.000000/1.000000 CPU_group_1_7c3399e340d4effa58e7169e1cea5a9e, 1.000000/1.000000 CPU_group_2_11150ae516918a93832c04c97c0de393, 0.000000/1.000000 CPU_group_0_34b2a68e3bb1d7341cacad3c262d308b}\n",
      ". In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "\u001b[2m\u001b[36m(pid=8506, ip=10.10.1.15)\u001b[0m 2021-07-29 08:42:23,007\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=8513, ip=10.10.1.15)\u001b[0m 2021-07-29 08:42:23,250\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=34365, ip=10.10.1.17)\u001b[0m 2021-07-29 08:42:26,278\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=34355, ip=10.10.1.17)\u001b[0m 2021-07-29 08:42:26,634\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=62940, ip=10.10.1.16)\u001b[0m 2021-07-29 08:42:28,035\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=68081)\u001b[0m 2021-07-29 08:42:30,179\tWARNING util.py:162 -- The `start_trial` operation took 0.521 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=8513, ip=10.10.1.15)\u001b[0m 2021-07-29 08:42:36,708\tINFO trainable.py:104 -- Trainable.setup took 13.458 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=8506, ip=10.10.1.15)\u001b[0m 2021-07-29 08:42:36,750\tINFO trainable.py:104 -- Trainable.setup took 13.746 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=34355, ip=10.10.1.17)\u001b[0m 2021-07-29 08:42:39,702\tINFO trainable.py:104 -- Trainable.setup took 13.069 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=34365, ip=10.10.1.17)\u001b[0m 2021-07-29 08:42:39,745\tINFO trainable.py:104 -- Trainable.setup took 13.467 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=62940, ip=10.10.1.16)\u001b[0m 2021-07-29 08:42:41,020\tINFO trainable.py:104 -- Trainable.setup took 12.987 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=68081)\u001b[0m 2021-07-29 08:42:46,035\tWARNING util.py:162 -- The `start_trial` operation took 0.520 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=1512)\u001b[0m 2021-07-29 08:43:02,321\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=1515)\u001b[0m 2021-07-29 08:43:02,753\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=1536)\u001b[0m 2021-07-29 08:43:02,720\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=1520)\u001b[0m 2021-07-29 08:43:02,897\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=1515)\u001b[0m 2021-07-29 08:43:38,412\tINFO trainable.py:104 -- Trainable.setup took 35.659 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=1512)\u001b[0m 2021-07-29 08:43:38,991\tINFO trainable.py:104 -- Trainable.setup took 36.670 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=1536)\u001b[0m 2021-07-29 08:43:39,695\tINFO trainable.py:104 -- Trainable.setup took 36.978 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "The actor or task with ID ffffffffffffffff241cd22b4a52a38bd952792b01000000 cannot be scheduled right now. It requires {CPU_group_aa482940fe440e2e78aa21f17b27e6a1: 1.000000}, {CPU_group_0_aa482940fe440e2e78aa21f17b27e6a1: 1.000000} for placement, but this node only has remaining {0.000000/18.000000 CPU, 85.325590 GiB/85.325590 GiB memory, 36.568110 GiB/36.568110 GiB object_store_memory, 0.000000/3.000000 CPU_group_4f68ace57b286e764fc814ba03c5e9cb, 1.000000/1.000000 CPU_group_1_2be5cf4c13f12e26fcf85fdbd9b12111, 1.000000/1.000000 CPU_group_2_2be5cf4c13f12e26fcf85fdbd9b12111, 1.000000/1.000000 CPU_group_1_1295364c8674b04be075bde4ff5ec696, 3.000000/3.000000 CPU_group_aa482940fe440e2e78aa21f17b27e6a1, 1.000000/1.000000 node:10.10.1.15, 1.000000/1.000000 CPU_group_2_1295364c8674b04be075bde4ff5ec696, 1.000000/1.000000 CPU_group_1_4f68ace57b286e764fc814ba03c5e9cb, 1.000000/1.000000 CPU_group_0_fd06d1df950411d2ac52f9d895ccd0f5, 3.000000/3.000000 CPU_group_aff63ab1b9502eab249ecc69f004523c, 0.000000/3.000000 CPU_group_2be5cf4c13f12e26fcf85fdbd9b12111, 1.000000/1.000000 CPU_group_1_aff63ab1b9502eab249ecc69f004523c, 1.000000/1.000000 CPU_group_1_aa482940fe440e2e78aa21f17b27e6a1, 1.000000/1.000000 CPU_group_0_aa482940fe440e2e78aa21f17b27e6a1, 1.000000/1.000000 CPU_group_2_aff63ab1b9502eab249ecc69f004523c, 3.000000/3.000000 CPU_group_fd06d1df950411d2ac52f9d895ccd0f5, 1.000000/1.000000 CPU_group_1_fd06d1df950411d2ac52f9d895ccd0f5, 0.000000/1.000000 CPU_group_0_2be5cf4c13f12e26fcf85fdbd9b12111, 3.000000/3.000000 CPU_group_1295364c8674b04be075bde4ff5ec696, 0.000000/1.000000 CPU_group_0_4f68ace57b286e764fc814ba03c5e9cb, 1.000000/1.000000 CPU_group_0_1295364c8674b04be075bde4ff5ec696, 1.000000/1.000000 CPU_group_2_4f68ace57b286e764fc814ba03c5e9cb, 1.000000/1.000000 CPU_group_2_aa482940fe440e2e78aa21f17b27e6a1, 1.000000/1.000000 CPU_group_2_fd06d1df950411d2ac52f9d895ccd0f5, 1.000000/1.000000 CPU_group_0_aff63ab1b9502eab249ecc69f004523c}\n",
      ". In total there are 0 pending tasks and 4 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "The actor or task with ID ffffffffffffffffaec20aa39c062c35147d1dbb01000000 cannot be scheduled right now. It requires {CPU_group_007247c1028f776129066838f2c30dda: 1.000000}, {CPU_group_0_007247c1028f776129066838f2c30dda: 1.000000} for placement, but this node only has remaining {0.000000/18.000000 CPU, 85.390494 GiB/85.390494 GiB memory, 36.595926 GiB/36.595926 GiB object_store_memory, 1.000000/1.000000 CPU_group_2_e87c1f6b3ff4cc7826fc6010b0e89e5c, 1.000000/1.000000 CPU_group_1_38cda0202e59dc6af2faf2aee0e70d66, 1.000000/1.000000 CPU_group_2_38cda0202e59dc6af2faf2aee0e70d66, 3.000000/3.000000 CPU_group_1edf78cda26535435567346a5e7e9012, 0.000000/1.000000 CPU_group_0_1c1623d47dc52ab4d03bbf01a69f155d, 1.000000/1.000000 CPU_group_0_e87c1f6b3ff4cc7826fc6010b0e89e5c, 1.000000/1.000000 CPU_group_1_007247c1028f776129066838f2c30dda, 1.000000/1.000000 CPU_group_1_1edf78cda26535435567346a5e7e9012, 3.000000/3.000000 CPU_group_38cda0202e59dc6af2faf2aee0e70d66, 3.000000/3.000000 CPU_group_e87c1f6b3ff4cc7826fc6010b0e89e5c, 0.000000/3.000000 CPU_group_1c1623d47dc52ab4d03bbf01a69f155d, 0.000000/1.000000 CPU_group_0_21bf9cf993811a2a2174aadea9724842, 1.000000/1.000000 node:10.10.1.17, 1.000000/1.000000 CPU_group_2_1c1623d47dc52ab4d03bbf01a69f155d, 1.000000/1.000000 CPU_group_0_1edf78cda26535435567346a5e7e9012, 1.000000/1.000000 CPU_group_0_007247c1028f776129066838f2c30dda, 0.000000/3.000000 CPU_group_21bf9cf993811a2a2174aadea9724842, 1.000000/1.000000 CPU_group_2_1edf78cda26535435567346a5e7e9012, 1.000000/1.000000 CPU_group_0_38cda0202e59dc6af2faf2aee0e70d66, 1.000000/1.000000 CPU_group_2_007247c1028f776129066838f2c30dda, 1.000000/1.000000 CPU_group_1_21bf9cf993811a2a2174aadea9724842, 1.000000/1.000000 CPU_group_1_1c1623d47dc52ab4d03bbf01a69f155d, 1.000000/1.000000 CPU_group_1_e87c1f6b3ff4cc7826fc6010b0e89e5c, 1.000000/1.000000 CPU_group_2_21bf9cf993811a2a2174aadea9724842, 3.000000/3.000000 CPU_group_007247c1028f776129066838f2c30dda}\n",
      ". In total there are 0 pending tasks and 4 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "The actor or task with ID ffffffffffffffff19826334fe71b40b3d7b061501000000 cannot be scheduled right now. It requires {CPU_group_0_31d21345c879abbc0eac72dd3c285a0b: 1.000000}, {CPU_group_31d21345c879abbc0eac72dd3c285a0b: 1.000000} for placement, but this node only has remaining {0.000000/18.000000 CPU, 85.553723 GiB/85.553723 GiB memory, 36.665881 GiB/36.665881 GiB object_store_memory, 1.000000/1.000000 CPU_group_2_34b2a68e3bb1d7341cacad3c262d308b, 3.000000/3.000000 CPU_group_7c3399e340d4effa58e7169e1cea5a9e, 1.000000/1.000000 CPU_group_2_31d21345c879abbc0eac72dd3c285a0b, 0.000000/3.000000 CPU_group_34b2a68e3bb1d7341cacad3c262d308b, 1.000000/1.000000 CPU_group_0_7c3399e340d4effa58e7169e1cea5a9e, 1.000000/1.000000 CPU_group_1_36c81298271a12d43e622d3f8674ae5b, 1.000000/1.000000 node:10.10.1.16, 1.000000/1.000000 CPU_group_1_34b2a68e3bb1d7341cacad3c262d308b, 1.000000/1.000000 CPU_group_2_7c3399e340d4effa58e7169e1cea5a9e, 0.000000/3.000000 CPU_group_11150ae516918a93832c04c97c0de393, 1.000000/1.000000 CPU_group_1_11150ae516918a93832c04c97c0de393, 0.000000/3.000000 CPU_group_4f3d192e6b4c6328574fefb762d0b6d1, 0.000000/1.000000 CPU_group_0_4f3d192e6b4c6328574fefb762d0b6d1, 3.000000/3.000000 CPU_group_31d21345c879abbc0eac72dd3c285a0b, 0.000000/1.000000 CPU_group_0_36c81298271a12d43e622d3f8674ae5b, 1.000000/1.000000 CPU_group_2_36c81298271a12d43e622d3f8674ae5b, 1.000000/1.000000 CPU_group_0_31d21345c879abbc0eac72dd3c285a0b, 0.000000/3.000000 CPU_group_36c81298271a12d43e622d3f8674ae5b, 1.000000/1.000000 CPU_group_1_4f3d192e6b4c6328574fefb762d0b6d1, 1.000000/1.000000 CPU_group_2_4f3d192e6b4c6328574fefb762d0b6d1, 0.000000/1.000000 CPU_group_0_11150ae516918a93832c04c97c0de393, 1.000000/1.000000 CPU_group_1_31d21345c879abbc0eac72dd3c285a0b, 1.000000/1.000000 CPU_group_1_7c3399e340d4effa58e7169e1cea5a9e, 1.000000/1.000000 CPU_group_2_11150ae516918a93832c04c97c0de393, 0.000000/1.000000 CPU_group_0_34b2a68e3bb1d7341cacad3c262d308b}\n",
      ". In total there are 0 pending tasks and 2 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "\u001b[2m\u001b[36m(pid=1520)\u001b[0m 2021-07-29 08:43:52,942\tINFO trainable.py:104 -- Trainable.setup took 50.046 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=64714, ip=10.10.1.16)\u001b[0m 2021-07-29 08:44:06,738\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=10287, ip=10.10.1.15)\u001b[0m 2021-07-29 08:44:06,762\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=64716, ip=10.10.1.16)\u001b[0m 2021-07-29 08:44:07,067\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=10277, ip=10.10.1.15)\u001b[0m 2021-07-29 08:44:07,522\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=64735, ip=10.10.1.16)\u001b[0m 2021-07-29 08:44:07,611\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=10293, ip=10.10.1.15)\u001b[0m 2021-07-29 08:44:07,613\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=10282, ip=10.10.1.15)\u001b[0m 2021-07-29 08:44:07,625\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=64722, ip=10.10.1.16)\u001b[0m 2021-07-29 08:44:07,674\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=36112, ip=10.10.1.17)\u001b[0m 2021-07-29 08:44:08,351\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=36094, ip=10.10.1.17)\u001b[0m 2021-07-29 08:44:08,553\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=36121, ip=10.10.1.17)\u001b[0m 2021-07-29 08:44:08,589\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=36108, ip=10.10.1.17)\u001b[0m 2021-07-29 08:44:08,745\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=64942, ip=10.10.1.16)\u001b[0m 2021-07-29 08:44:17,199\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=10287, ip=10.10.1.15)\u001b[0m 2021-07-29 08:44:21,561\tINFO trainable.py:104 -- Trainable.setup took 14.800 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=10282, ip=10.10.1.15)\u001b[0m 2021-07-29 08:44:22,440\tINFO trainable.py:104 -- Trainable.setup took 14.817 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=10277, ip=10.10.1.15)\u001b[0m 2021-07-29 08:44:22,703\tINFO trainable.py:104 -- Trainable.setup took 15.182 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=36108, ip=10.10.1.17)\u001b[0m 2021-07-29 08:44:22,803\tINFO trainable.py:104 -- Trainable.setup took 14.058 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=36121, ip=10.10.1.17)\u001b[0m 2021-07-29 08:44:22,896\tINFO trainable.py:104 -- Trainable.setup took 14.311 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=36112, ip=10.10.1.17)\u001b[0m 2021-07-29 08:44:22,932\tINFO trainable.py:104 -- Trainable.setup took 14.585 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=10293, ip=10.10.1.15)\u001b[0m 2021-07-29 08:44:22,937\tINFO trainable.py:104 -- Trainable.setup took 15.325 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=36094, ip=10.10.1.17)\u001b[0m 2021-07-29 08:44:23,253\tINFO trainable.py:104 -- Trainable.setup took 14.701 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=64716, ip=10.10.1.16)\u001b[0m 2021-07-29 08:44:37,996\tINFO trainable.py:104 -- Trainable.setup took 30.930 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=64735, ip=10.10.1.16)\u001b[0m 2021-07-29 08:44:39,301\tINFO trainable.py:104 -- Trainable.setup took 31.691 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=64714, ip=10.10.1.16)\u001b[0m 2021-07-29 08:44:39,462\tINFO trainable.py:104 -- Trainable.setup took 32.725 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=64722, ip=10.10.1.16)\u001b[0m 2021-07-29 08:44:53,677\tINFO trainable.py:104 -- Trainable.setup took 46.003 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=64942, ip=10.10.1.16)\u001b[0m 2021-07-29 08:44:53,890\tINFO trainable.py:104 -- Trainable.setup took 36.693 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=3978)\u001b[0m 2021-07-29 08:45:33,837\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=68081)\u001b[0m 2021-07-29 08:46:00,840\tWARNING util.py:162 -- The `start_trial` operation took 0.622 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=68081)\u001b[0m 2021-07-29 08:46:02,531\tWARNING util.py:162 -- The `callbacks.on_trial_result` operation took 0.578 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=68081)\u001b[0m 2021-07-29 08:46:02,542\tWARNING util.py:162 -- The `process_trial_result` operation took 0.590 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=68081)\u001b[0m 2021-07-29 08:46:02,542\tWARNING util.py:162 -- Processing trial results took 0.590 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(pid=68081)\u001b[0m 2021-07-29 08:46:02,542\tWARNING util.py:162 -- The `process_trial` operation took 0.591 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=3978)\u001b[0m 2021-07-29 08:46:07,888\tINFO trainable.py:104 -- Trainable.setup took 34.052 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=12461, ip=10.10.1.15)\u001b[0m 2021-07-29 08:46:42,542\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=12461, ip=10.10.1.15)\u001b[0m 2021-07-29 08:46:56,541\tINFO trainable.py:104 -- Trainable.setup took 14.001 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "The actor or task with ID ffffffffffffffffe0ede7de8e308092698bbf0901000000 cannot be scheduled right now. It requires {CPU_group_0_1c1623d47dc52ab4d03bbf01a69f155d: 1.000000}, {CPU_group_1c1623d47dc52ab4d03bbf01a69f155d: 1.000000} for placement, but this node only has remaining {0.000000/18.000000 CPU, 85.390494 GiB/85.390494 GiB memory, 36.595926 GiB/36.595926 GiB object_store_memory, 1.000000/1.000000 CPU_group_2_e87c1f6b3ff4cc7826fc6010b0e89e5c, 1.000000/1.000000 CPU_group_1_38cda0202e59dc6af2faf2aee0e70d66, 1.000000/1.000000 CPU_group_2_38cda0202e59dc6af2faf2aee0e70d66, 0.000000/3.000000 CPU_group_1edf78cda26535435567346a5e7e9012, 1.000000/1.000000 CPU_group_0_1c1623d47dc52ab4d03bbf01a69f155d, 0.000000/1.000000 CPU_group_0_e87c1f6b3ff4cc7826fc6010b0e89e5c, 1.000000/1.000000 CPU_group_1_007247c1028f776129066838f2c30dda, 1.000000/1.000000 CPU_group_1_1edf78cda26535435567346a5e7e9012, 0.000000/3.000000 CPU_group_38cda0202e59dc6af2faf2aee0e70d66, 0.000000/3.000000 CPU_group_e87c1f6b3ff4cc7826fc6010b0e89e5c, 3.000000/3.000000 CPU_group_1c1623d47dc52ab4d03bbf01a69f155d, 1.000000/1.000000 CPU_group_0_21bf9cf993811a2a2174aadea9724842, 1.000000/1.000000 node:10.10.1.17, 1.000000/1.000000 CPU_group_2_1c1623d47dc52ab4d03bbf01a69f155d, 0.000000/1.000000 CPU_group_0_1edf78cda26535435567346a5e7e9012, 0.000000/1.000000 CPU_group_0_007247c1028f776129066838f2c30dda, 3.000000/3.000000 CPU_group_21bf9cf993811a2a2174aadea9724842, 1.000000/1.000000 CPU_group_2_1edf78cda26535435567346a5e7e9012, 0.000000/1.000000 CPU_group_0_38cda0202e59dc6af2faf2aee0e70d66, 1.000000/1.000000 CPU_group_2_007247c1028f776129066838f2c30dda, 1.000000/1.000000 CPU_group_1_21bf9cf993811a2a2174aadea9724842, 1.000000/1.000000 CPU_group_1_1c1623d47dc52ab4d03bbf01a69f155d, 1.000000/1.000000 CPU_group_1_e87c1f6b3ff4cc7826fc6010b0e89e5c, 1.000000/1.000000 CPU_group_2_21bf9cf993811a2a2174aadea9724842, 0.000000/3.000000 CPU_group_007247c1028f776129066838f2c30dda}\n",
      ". In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "\u001b[2m\u001b[36m(pid=39756, ip=10.10.1.17)\u001b[0m 2021-07-29 08:48:23,081\tINFO trainer.py:696 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=39756, ip=10.10.1.17)\u001b[0m 2021-07-29 08:48:34,250\tINFO trainable.py:104 -- Trainable.setup took 11.172 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7ff9097f5f10>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Stop cluster and release allocation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "ray.util.disconnect()\n",
    "exp.stop(cluster)\n",
    "if alloc:\n",
    "    slurm.release_allocation(alloc)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "08:56:41 osprey.us.cray.com SmartSim[17173] INFO Stopping model workers with job name workers-CD5NVSH5WO1S\n",
      "08:56:41 osprey.us.cray.com SmartSim[17173] INFO Stopping model head with job name head-CD5NVR06QEKH\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('smartsim': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "b738ecfe013e3ceede67431676fa5746fa1d95dad4240bd1c29430e75f30557e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}